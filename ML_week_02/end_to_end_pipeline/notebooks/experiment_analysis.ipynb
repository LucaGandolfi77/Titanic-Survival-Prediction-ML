{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64d3a9d3",
   "metadata": {},
   "source": [
    "# Experiment Analysis — Titanic MLOps Pipeline\n",
    "\n",
    "This notebook explores MLflow experiment results:\n",
    "- Compare model performance across runs\n",
    "- Visualise Optuna hyperparameter search\n",
    "- Identify the best model for promotion to Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8118d870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure project root is importable\n",
    "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "\n",
    "from src.utils.config_loader import load_config\n",
    "\n",
    "cfg = load_config(PROJECT_ROOT / \"config\" / \"config.yaml\")\n",
    "mlflow.set_tracking_uri(cfg[\"mlflow\"][\"tracking_uri\"])\n",
    "print(f\"MLflow tracking: {cfg['mlflow']['tracking_uri']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876e5de4",
   "metadata": {},
   "source": [
    "## 1. Load All Experiment Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a28433e",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = mlflow.get_experiment_by_name(cfg[\"mlflow\"][\"experiment_name\"])\n",
    "\n",
    "if experiment is None:\n",
    "    print(\"No experiment found — run `python train.py` first!\")\n",
    "else:\n",
    "    runs = mlflow.search_runs(\n",
    "        experiment_ids=[experiment.experiment_id],\n",
    "        order_by=[\"metrics.accuracy DESC\"],\n",
    "    )\n",
    "    print(f\"Total runs: {len(runs)}\")\n",
    "    runs.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4d40d6",
   "metadata": {},
   "source": [
    "## 2. Compare Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a92c291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter baseline runs (non-HPO)\n",
    "if experiment is not None:\n",
    "    baseline = runs[runs[\"tags.stage\"] == \"baseline\"].copy()\n",
    "    \n",
    "    metrics_cols = [\"metrics.accuracy\", \"metrics.precision\", \"metrics.recall\",\n",
    "                    \"metrics.f1\", \"metrics.roc_auc\"]\n",
    "    comparison = baseline[[\"params.model_type\"] + metrics_cols].set_index(\"params.model_type\")\n",
    "    comparison.columns = [c.replace(\"metrics.\", \"\") for c in comparison.columns]\n",
    "    \n",
    "    display(comparison.style.highlight_max(axis=0, color=\"lightgreen\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f38f192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart comparison\n",
    "if experiment is not None and len(baseline) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    comparison.plot(kind=\"bar\", ax=ax, rot=0)\n",
    "    ax.set_title(\"Baseline Model Comparison\")\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3830df",
   "metadata": {},
   "source": [
    "## 3. Optuna HPO Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873f25c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter HPO trials\n",
    "if experiment is not None:\n",
    "    hpo_runs = runs[runs[\"tags.stage\"] == \"hpo\"]\n",
    "    trial_runs = runs[runs[\"tags.optuna_trial\"].notna()].copy()\n",
    "    \n",
    "    if len(trial_runs) > 0:\n",
    "        trial_runs[\"trial\"] = trial_runs[\"tags.optuna_trial\"].astype(int)\n",
    "        trial_runs = trial_runs.sort_values(\"trial\")\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 5))\n",
    "        ax.plot(trial_runs[\"trial\"], trial_runs[\"metrics.cv_accuracy\"], \"b-o\", markersize=3)\n",
    "        ax.axhline(trial_runs[\"metrics.cv_accuracy\"].max(), color=\"r\", linestyle=\"--\",\n",
    "                   label=f\"Best: {trial_runs['metrics.cv_accuracy'].max():.4f}\")\n",
    "        ax.set_xlabel(\"Trial\")\n",
    "        ax.set_ylabel(\"CV Accuracy\")\n",
    "        ax.set_title(\"Optuna Optimisation History\")\n",
    "        ax.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No HPO trials found — run `python optimize.py` first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cb07a0",
   "metadata": {},
   "source": [
    "## 4. Best Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b07948",
   "metadata": {},
   "outputs": [],
   "source": [
    "if experiment is not None and len(runs) > 0:\n",
    "    best_run = runs.iloc[0]  # Already sorted by accuracy DESC\n",
    "    print(f\"Best run ID:  {best_run['run_id']}\")\n",
    "    print(f\"Model type:   {best_run.get('params.model_type', 'N/A')}\")\n",
    "    print(f\"Accuracy:     {best_run.get('metrics.accuracy', 'N/A'):.4f}\")\n",
    "    print(f\"F1 Score:     {best_run.get('metrics.f1', 'N/A'):.4f}\")\n",
    "    print(f\"ROC AUC:      {best_run.get('metrics.roc_auc', 'N/A'):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64aac97e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Promote to Production** — Use `mlflow_utils.transition_model_stage()` to move the best model to the Production stage\n",
    "2. **Serve** — Run `python serve.py` to start the prediction API\n",
    "3. **Monitor** — Check `/health` and `/model/info` endpoints for model status"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
