{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ⚽ FIFA World Cup & International Football — ML Analysis\n",
    "\n",
    "**Datasets**: international_matches.csv (17,769 matches, 1872-2022), world_cup_matches.csv (900 WC matches),\n",
    "world_cups.csv (22 tournaments), 2022_world_cup_squads.csv (831 players)\n",
    "\n",
    "## ML Tasks\n",
    "| # | Task | Type | Target |\n",
    "|---|------|------|--------|\n",
    "| 1 | Match Outcome Prediction | Multi-class Classification | Home Win / Draw / Away Win |\n",
    "| 2 | Total Goals Regression | Regression | Total goals per match |\n",
    "| 3 | Player Position Classification | Multi-class Classification | GK / DEF / MID / FWD |\n",
    "| 4 | Country Performance Clustering | Unsupervised | K-Means / DBSCAN |"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import warnings, os, base64, io, pathlib\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from jinja2 import Template\n",
    "\n",
    "from sklearn.model_selection import (train_test_split, cross_val_score,\n",
    "                                     GridSearchCV, RandomizedSearchCV,\n",
    "                                     learning_curve, StratifiedKFold)\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import (RandomForestClassifier, RandomForestRegressor,\n",
    "                              GradientBoostingClassifier, GradientBoostingRegressor,\n",
    "                              AdaBoostClassifier, VotingClassifier, StackingClassifier)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, confusion_matrix, classification_report,\n",
    "                             mean_absolute_error, mean_squared_error, r2_score)\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams.update({'figure.max_open_warning': 0, 'figure.dpi': 120})\n",
    "\n",
    "PLOT_DIR = pathlib.Path('outputs/plots')\n",
    "PLOT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "all_results = {}   # collector for HTML report\n",
    "saved_plots = []   # list of (title, path)\n",
    "\n",
    "def save(fig, name, title=None):\n",
    "    p = PLOT_DIR / name\n",
    "    fig.savefig(p, bbox_inches='tight', facecolor='white')\n",
    "    plt.close(fig)\n",
    "    saved_plots.append((title or name.replace('.png','').replace('_',' ').title(), str(p)))\n",
    "    print(f'  ✓ {name}')\n",
    "\n",
    "import sklearn\n",
    "print(f'All imports successful')\n",
    "print(f'  scikit-learn {sklearn.__version__}')\n",
    "print(f'  pandas {pd.__version__}, numpy {np.__version__}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 · Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "DATA = pathlib.Path('World+Cup')\n",
    "\n",
    "intl = pd.read_csv(DATA / 'international_matches.csv')\n",
    "wc_matches = pd.read_csv(DATA / 'world_cup_matches.csv')\n",
    "wc_summary = pd.read_csv(DATA / 'world_cups.csv')\n",
    "squads = pd.read_csv(DATA / '2022_world_cup_squads.csv', encoding='latin1')\n",
    "groups = pd.read_csv(DATA / '2022_world_cup_groups.csv')\n",
    "\n",
    "print(f'international_matches: {intl.shape}')\n",
    "print(f'world_cup_matches:     {wc_matches.shape}')\n",
    "print(f'world_cups:            {wc_summary.shape}')\n",
    "print(f'squads:                {squads.shape}')\n",
    "print(f'groups:                {groups.shape}')\n",
    "print()\n",
    "print('=== international_matches columns ===')\n",
    "print(intl.dtypes.to_string())\n",
    "print()\n",
    "intl.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── Parse dates & basic features ──\n",
    "intl['Date'] = pd.to_datetime(intl['Date'])\n",
    "intl['year'] = intl['Date'].dt.year\n",
    "intl['month'] = intl['Date'].dt.month\n",
    "intl['decade'] = (intl['year'] // 10) * 10\n",
    "\n",
    "intl['total_goals'] = intl['Home Goals'] + intl['Away Goals']\n",
    "intl['goal_diff'] = intl['Home Goals'] - intl['Away Goals']\n",
    "\n",
    "# Match result (target for classification)\n",
    "intl['result'] = intl['goal_diff'].apply(\n",
    "    lambda x: 'Home Win' if x > 0 else ('Draw' if x == 0 else 'Away Win'))\n",
    "\n",
    "# Tournament importance encoding\n",
    "tourn_map = {\n",
    "    'FIFA World Cup': 5, 'FIFA World Cup qualification': 4,\n",
    "    'Confederations Cup': 4,\n",
    "    'Copa America': 3, 'UEFA Euro': 3, 'African Cup of Nations': 3,\n",
    "    'Gold Cup': 3, 'AFC Asian Cup': 3,\n",
    "    'UEFA Euro qualification': 2, 'African Cup of Nations qualification': 2,\n",
    "    'Copa America qualification': 2, 'AFC Asian Cup qualification': 2,\n",
    "    'British Championship': 2, 'Friendly': 1\n",
    "}\n",
    "intl['tournament_importance'] = intl['Tournament'].map(tourn_map).fillna(2).astype(int)\n",
    "\n",
    "# Is World Cup match\n",
    "intl['is_world_cup'] = (intl['Tournament'] == 'FIFA World Cup').astype(int)\n",
    "\n",
    "print(f'Processed: {intl.shape}')\n",
    "print(f'\\nResult distribution:')\n",
    "print(intl['result'].value_counts())\n",
    "print(f'\\nYear range: {intl[\"year\"].min()} – {intl[\"year\"].max()}')\n",
    "print(f'Home Stadium: {intl[\"Home Stadium\"].value_counts().to_dict()}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 · Feature Engineering — Team Strength"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── Build historical team stats (rolling) ──\n",
    "# For each match, compute team's historical win rate, avg goals, etc.\n",
    "# using all prior matches (expanding window)\n",
    "\n",
    "# Create a long-format record: each row = team + match\n",
    "records = []\n",
    "for _, row in intl.iterrows():\n",
    "    records.append({\n",
    "        'date': row['Date'], 'team': row['Home Team'],\n",
    "        'goals_for': row['Home Goals'], 'goals_against': row['Away Goals'],\n",
    "        'is_home': 1,\n",
    "        'result': 1 if row['goal_diff'] > 0 else (0.5 if row['goal_diff'] == 0 else 0)\n",
    "    })\n",
    "    records.append({\n",
    "        'date': row['Date'], 'team': row['Away Team'],\n",
    "        'goals_for': row['Away Goals'], 'goals_against': row['Home Goals'],\n",
    "        'is_home': 0,\n",
    "        'result': 1 if row['goal_diff'] < 0 else (0.5 if row['goal_diff'] == 0 else 0)\n",
    "    })\n",
    "\n",
    "team_log = pd.DataFrame(records).sort_values('date').reset_index(drop=True)\n",
    "\n",
    "# Compute expanding stats per team\n",
    "team_stats = {}\n",
    "for team in team_log['team'].unique():\n",
    "    t = team_log[team_log['team'] == team].copy()\n",
    "    t['cum_wins'] = (t['result'] == 1).cumsum().shift(1)\n",
    "    t['cum_matches'] = range(len(t))\n",
    "    t['cum_matches'] = t['cum_matches'].replace(0, np.nan)\n",
    "    t['win_rate'] = t['cum_wins'] / t['cum_matches']\n",
    "    t['avg_gf'] = t['goals_for'].expanding().mean().shift(1)\n",
    "    t['avg_ga'] = t['goals_against'].expanding().mean().shift(1)\n",
    "    team_stats[team] = t[['date', 'win_rate', 'avg_gf', 'avg_ga']].set_index('date')\n",
    "\n",
    "print(f'Built historical stats for {len(team_stats)} teams')\n",
    "\n",
    "# Merge back into intl\n",
    "def get_team_stat(team, date, stat):\n",
    "    if team not in team_stats:\n",
    "        return np.nan\n",
    "    ts = team_stats[team]\n",
    "    mask = ts.index <= date\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    return ts.loc[mask, stat].iloc[-1]\n",
    "\n",
    "# Vectorised: use last known stats at match date\n",
    "home_wr, away_wr = [], []\n",
    "home_gf, away_gf = [], []\n",
    "home_ga, away_ga = [], []\n",
    "\n",
    "for _, row in intl.iterrows():\n",
    "    home_wr.append(get_team_stat(row['Home Team'], row['Date'], 'win_rate'))\n",
    "    away_wr.append(get_team_stat(row['Away Team'], row['Date'], 'win_rate'))\n",
    "    home_gf.append(get_team_stat(row['Home Team'], row['Date'], 'avg_gf'))\n",
    "    away_gf.append(get_team_stat(row['Away Team'], row['Date'], 'avg_gf'))\n",
    "    home_ga.append(get_team_stat(row['Home Team'], row['Date'], 'avg_ga'))\n",
    "    away_ga.append(get_team_stat(row['Away Team'], row['Date'], 'avg_ga'))\n",
    "\n",
    "intl['home_win_rate'] = home_wr\n",
    "intl['away_win_rate'] = away_wr\n",
    "intl['home_avg_gf'] = home_gf\n",
    "intl['away_avg_gf'] = away_gf\n",
    "intl['home_avg_ga'] = home_ga\n",
    "intl['away_avg_ga'] = away_ga\n",
    "\n",
    "# Derived features\n",
    "intl['win_rate_diff'] = intl['home_win_rate'] - intl['away_win_rate']\n",
    "intl['attack_diff'] = intl['home_avg_gf'] - intl['away_avg_gf']\n",
    "\n",
    "# Drop rows with NaN team stats (first few matches per team)\n",
    "df = intl.dropna(subset=['home_win_rate', 'away_win_rate',\n",
    "                          'home_avg_gf', 'away_avg_gf']).copy()\n",
    "print(f'After dropping NaN team stats: {len(df)} matches')\n",
    "print(f'\\nFeature columns added: home_win_rate, away_win_rate, home_avg_gf, away_avg_gf, '\n",
    "      f'home_avg_ga, away_avg_ga, win_rate_diff, attack_diff')\n",
    "print(f'\\nSample:')\n",
    "df[['Home Team', 'Away Team', 'year', 'result', 'home_win_rate', 'away_win_rate',\n",
    "    'win_rate_diff', 'total_goals']].head(5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 · Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── Plot 1: Goals per match over decades ──\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "decade_goals = df.groupby('decade')['total_goals'].mean()\n",
    "ax.bar(decade_goals.index.astype(str), decade_goals.values, color='#2E86AB', edgecolor='white')\n",
    "ax.set_xlabel('Decade'); ax.set_ylabel('Avg Goals per Match')\n",
    "ax.set_title('Average Goals per Match by Decade')\n",
    "for i, v in enumerate(decade_goals.values):\n",
    "    ax.text(i, v + 0.05, f'{v:.2f}', ha='center', fontsize=8)\n",
    "save(fig, 'goals_per_decade.png', 'Goals per Match by Decade')\n",
    "\n",
    "# ── Plot 2: Match outcome distribution ──\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "colors = ['#2E86AB', '#A23B72', '#F18F01']\n",
    "df['result'].value_counts().plot.pie(ax=axes[0], colors=colors, autopct='%1.1f%%',\n",
    "                                      startangle=90, textprops={'fontsize': 10})\n",
    "axes[0].set_ylabel(''); axes[0].set_title('Match Outcome Distribution')\n",
    "\n",
    "# By tournament importance\n",
    "ct = pd.crosstab(df['tournament_importance'], df['result'], normalize='index') * 100\n",
    "ct[['Home Win', 'Draw', 'Away Win']].plot.bar(ax=axes[1], color=colors, edgecolor='white')\n",
    "axes[1].set_xlabel('Tournament Importance'); axes[1].set_ylabel('Percentage')\n",
    "axes[1].set_title('Outcome by Tournament Importance')\n",
    "axes[1].legend(fontsize=8); axes[1].tick_params(axis='x', rotation=0)\n",
    "fig.tight_layout()\n",
    "save(fig, 'outcome_distribution.png', 'Match Outcome Distribution')\n",
    "\n",
    "# ── Plot 3: Top 20 teams by win rate (min 50 matches) ──\n",
    "all_teams = pd.concat([\n",
    "    df[['Home Team', 'result']].rename(columns={'Home Team': 'team'}).assign(\n",
    "        win=lambda x: (x['result'] == 'Home Win').astype(int)),\n",
    "    df[['Away Team', 'result']].rename(columns={'Away Team': 'team'}).assign(\n",
    "        win=lambda x: (x['result'] == 'Away Win').astype(int))\n",
    "])\n",
    "team_agg = all_teams.groupby('team')['win'].agg(['sum', 'count'])\n",
    "team_agg.columns = ['wins', 'matches']\n",
    "team_agg['win_rate'] = team_agg['wins'] / team_agg['matches']\n",
    "team_top = team_agg[team_agg['matches'] >= 50].nlargest(20, 'win_rate')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars = ax.barh(range(len(team_top)), team_top['win_rate'], color='#2E86AB', edgecolor='white')\n",
    "ax.set_yticks(range(len(team_top)))\n",
    "ax.set_yticklabels(team_top.index, fontsize=9)\n",
    "ax.set_xlabel('Win Rate'); ax.set_title('Top 20 Teams by Win Rate (min 50 matches)')\n",
    "ax.invert_yaxis()\n",
    "for i, (wr, m) in enumerate(zip(team_top['win_rate'], team_top['matches'])):\n",
    "    ax.text(wr + 0.005, i, f'{wr:.2f} ({m})', va='center', fontsize=8)\n",
    "fig.tight_layout()\n",
    "save(fig, 'top20_teams.png', 'Top 20 Teams by Win Rate')\n",
    "\n",
    "# ── Plot 4: World Cup goals trend ──\n",
    "wc_s = wc_summary.dropna(subset=['Goals Scored']).copy()\n",
    "wc_s['goals_per_match'] = wc_s['Goals Scored'] / wc_s['Matches Played']\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "ax1.bar(wc_s['Year'].astype(str), wc_s['Goals Scored'], color='#2E86AB', alpha=0.7, label='Total Goals')\n",
    "ax1.set_xlabel('Year'); ax1.set_ylabel('Total Goals', color='#2E86AB')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(wc_s['Year'].astype(str), wc_s['goals_per_match'], 'o-', color='#F18F01', linewidth=2, label='Goals/Match')\n",
    "ax2.set_ylabel('Goals per Match', color='#F18F01')\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "ax1.set_title('FIFA World Cup — Goals Over the Years')\n",
    "fig.tight_layout()\n",
    "save(fig, 'wc_goals_trend.png', 'World Cup Goals Trend')\n",
    "\n",
    "# ── Plot 5: Home advantage over time ──\n",
    "ha = df.groupby('decade').apply(\n",
    "    lambda x: (x['result'] == 'Home Win').mean() * 100).reset_index()\n",
    "ha.columns = ['decade', 'home_win_pct']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(ha['decade'].astype(str), ha['home_win_pct'], 'o-', color='#2E86AB', linewidth=2, markersize=8)\n",
    "ax.axhline(y=50, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.fill_between(range(len(ha)), 50, ha['home_win_pct'], alpha=0.2, color='#2E86AB')\n",
    "ax.set_xticks(range(len(ha))); ax.set_xticklabels(ha['decade'].astype(str))\n",
    "ax.set_xlabel('Decade'); ax.set_ylabel('Home Win %')\n",
    "ax.set_title('Home Advantage Over Time')\n",
    "fig.tight_layout()\n",
    "save(fig, 'home_advantage.png', 'Home Advantage Over Time')\n",
    "\n",
    "# ── Plot 6: Correlation heatmap ──\n",
    "num_cols = ['Home Goals', 'Away Goals', 'total_goals', 'goal_diff',\n",
    "            'home_win_rate', 'away_win_rate', 'home_avg_gf', 'away_avg_gf',\n",
    "            'home_avg_ga', 'away_avg_ga', 'win_rate_diff', 'attack_diff',\n",
    "            'tournament_importance', 'Home Stadium', 'year']\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "corr = df[num_cols].corr()\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "sns.heatmap(corr, mask=mask, annot=True, fmt='.2f', cmap='RdBu_r',\n",
    "            center=0, square=True, linewidths=0.5, ax=ax, annot_kws={'size': 7})\n",
    "ax.set_title('Feature Correlation Heatmap')\n",
    "fig.tight_layout()\n",
    "save(fig, 'correlation_heatmap.png', 'Feature Correlation Heatmap')\n",
    "\n",
    "print('All EDA plots saved!')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 · Task 1 — Total Goals Regression\n",
    "Predict the total number of goals scored in a match using team strength features, tournament info and home advantage."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── Prepare regression data ──\n",
    "reg_features = ['home_win_rate', 'away_win_rate', 'home_avg_gf', 'away_avg_gf',\n",
    "                'home_avg_ga', 'away_avg_ga', 'win_rate_diff', 'attack_diff',\n",
    "                'tournament_importance', 'Home Stadium', 'year', 'is_world_cup']\n",
    "\n",
    "reg_df = df.dropna(subset=reg_features + ['total_goals']).copy()\n",
    "X_reg = reg_df[reg_features].astype(float)\n",
    "y_reg = reg_df['total_goals']\n",
    "\n",
    "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=SEED)\n",
    "\n",
    "scaler_r = StandardScaler()\n",
    "X_train_rs = scaler_r.fit_transform(X_train_r)\n",
    "X_test_rs = scaler_r.transform(X_test_r)\n",
    "\n",
    "print(f'Regression dataset: {len(reg_df)} rows')\n",
    "print(f'Train: {len(X_train_r)}  |  Test: {len(X_test_r)}')\n",
    "\n",
    "regressors = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(alpha=1.0),\n",
    "    'Lasso Regression': Lasso(alpha=0.01),\n",
    "    'Decision Tree': DecisionTreeRegressor(max_depth=10, random_state=SEED),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=10, random_state=SEED, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=200, max_depth=5, random_state=SEED),\n",
    "}\n",
    "\n",
    "reg_results = {}\n",
    "print(f'\\nTotal Goals Regression Results:')\n",
    "print('=' * 70)\n",
    "for name, model in regressors.items():\n",
    "    use_scaled = name in ['Ridge Regression', 'Lasso Regression']\n",
    "    Xtr = X_train_rs if use_scaled else X_train_r\n",
    "    Xte = X_test_rs if use_scaled else X_test_r\n",
    "    model.fit(Xtr, y_train_r)\n",
    "    pred = model.predict(Xte)\n",
    "    r2 = r2_score(y_test_r, pred)\n",
    "    mae = mean_absolute_error(y_test_r, pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_r, pred))\n",
    "    reg_results[name] = {'R²': r2, 'MAE': mae, 'RMSE': rmse, 'model': model}\n",
    "    print(f'  {name:30s} R²={r2:.4f}  MAE={mae:.2f}  RMSE={rmse:.2f}')\n",
    "\n",
    "# ── Plot regression comparison ──\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "names = list(reg_results.keys())\n",
    "r2s = [reg_results[n]['R²'] for n in names]\n",
    "maes = [reg_results[n]['MAE'] for n in names]\n",
    "\n",
    "axes[0].barh(names, r2s, color='#2E86AB', edgecolor='white')\n",
    "axes[0].set_xlabel('R² Score'); axes[0].set_title('Regression — R² Comparison')\n",
    "for i, v in enumerate(r2s):\n",
    "    axes[0].text(v + 0.002, i, f'{v:.4f}', va='center', fontsize=8)\n",
    "\n",
    "axes[1].barh(names, maes, color='#F18F01', edgecolor='white')\n",
    "axes[1].set_xlabel('MAE'); axes[1].set_title('Regression — MAE Comparison')\n",
    "for i, v in enumerate(maes):\n",
    "    axes[1].text(v + 0.02, i, f'{v:.2f}', va='center', fontsize=8)\n",
    "fig.tight_layout()\n",
    "save(fig, 'regression_results.png', 'Total Goals Regression Results')\n",
    "\n",
    "# ── Feature importance (best tree model) ──\n",
    "best_tree_name = max(['Decision Tree', 'Random Forest', 'Gradient Boosting'],\n",
    "                      key=lambda n: reg_results[n]['R²'])\n",
    "best_tree = reg_results[best_tree_name]['model']\n",
    "imp = pd.Series(best_tree.feature_importances_, index=reg_features).sort_values()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "imp.plot.barh(ax=ax, color='#2E86AB', edgecolor='white')\n",
    "ax.set_xlabel('Feature Importance'); ax.set_title(f'Regression Feature Importance ({best_tree_name})')\n",
    "fig.tight_layout()\n",
    "save(fig, 'regression_feature_importance.png', 'Regression Feature Importance')\n",
    "\n",
    "all_results['regression'] = {k: {m: v for m, v in v.items() if m != 'model'}\n",
    "                              for k, v in reg_results.items()}\n",
    "\n",
    "best_reg = max(reg_results, key=lambda n: reg_results[n]['R²'])\n",
    "print(f'\\nBest: {best_reg} (R² = {reg_results[best_reg][\"R²\"]:.4f})')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 · Task 2 — Match Outcome Classification\n",
    "Predict whether a match ends in **Home Win**, **Draw**, or **Away Win** using pre-match features."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── Classification setup ──\n",
    "clf_features = ['home_win_rate', 'away_win_rate', 'home_avg_gf', 'away_avg_gf',\n",
    "                'home_avg_ga', 'away_avg_ga', 'win_rate_diff', 'attack_diff',\n",
    "                'tournament_importance', 'Home Stadium', 'year', 'is_world_cup']\n",
    "\n",
    "clf_df = df.dropna(subset=clf_features).copy()\n",
    "le_result = LabelEncoder()\n",
    "clf_df['result_enc'] = le_result.fit_transform(clf_df['result'])\n",
    "class_names = list(le_result.classes_)\n",
    "\n",
    "X_clf = clf_df[clf_features].astype(float)\n",
    "y_clf = clf_df['result_enc']\n",
    "\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n",
    "    X_clf, y_clf, test_size=0.2, random_state=SEED, stratify=y_clf)\n",
    "\n",
    "scaler_c = StandardScaler()\n",
    "X_train_cs = scaler_c.fit_transform(X_train_c)\n",
    "X_test_cs = scaler_c.transform(X_test_c)\n",
    "\n",
    "print(f'Classification dataset: {len(clf_df)} rows')\n",
    "print(f'Classes: {class_names}')\n",
    "print(clf_df['result'].value_counts())\n",
    "print(f'\\nTrain: {len(X_train_c)}  |  Test: {len(X_test_c)}')\n",
    "\n",
    "def eval_clf(name, model, Xtr, ytr, Xte, yte, needs_scale=False):\n",
    "    \"\"\"Train, predict, return metrics dict.\"\"\"\n",
    "    Xtr_ = scaler_c.transform(Xtr) if needs_scale else Xtr\n",
    "    Xte_ = scaler_c.transform(Xte) if needs_scale else Xte\n",
    "    model.fit(Xtr_, ytr)\n",
    "    pred = model.predict(Xte_)\n",
    "    acc = accuracy_score(yte, pred)\n",
    "    f1 = f1_score(yte, pred, average='weighted')\n",
    "    prec = precision_score(yte, pred, average='weighted')\n",
    "    rec = recall_score(yte, pred, average='weighted')\n",
    "    return {'Accuracy': acc, 'Precision': prec, 'Recall': rec, 'F1 Score': f1,\n",
    "            'model': model, 'predictions': pred}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "clf_results = {}\n",
    "\n",
    "classifiers = [\n",
    "    ('Logistic Regression', LogisticRegression(max_iter=2000, random_state=SEED), True),\n",
    "    ('Decision Tree', DecisionTreeClassifier(max_depth=10, random_state=SEED), False),\n",
    "    ('Random Forest', RandomForestClassifier(n_estimators=200, max_depth=15, random_state=SEED, n_jobs=-1), False),\n",
    "    ('Gradient Boosting', GradientBoostingClassifier(n_estimators=200, max_depth=5, random_state=SEED), False),\n",
    "    ('AdaBoost', AdaBoostClassifier(n_estimators=100, random_state=SEED), False),\n",
    "    ('SVM (linear)', SVC(kernel='linear', random_state=SEED), True),\n",
    "    ('SVM (rbf)', SVC(kernel='rbf', random_state=SEED), True),\n",
    "    ('Naive Bayes', GaussianNB(), True),\n",
    "    ('MLP Neural Network', MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=500,\n",
    "                                          random_state=SEED, early_stopping=True), True),\n",
    "]\n",
    "\n",
    "print('Match Outcome Classification')\n",
    "print('=' * 65)\n",
    "\n",
    "for name, model, needs_scale in classifiers:\n",
    "    r = eval_clf(name, model, X_train_c, y_train_c, X_test_c, y_test_c, needs_scale)\n",
    "    clf_results[name] = r\n",
    "    print(f'  {name:30s} Acc={r[\"Accuracy\"]:.4f}  F1={r[\"F1 Score\"]:.4f}')\n",
    "\n",
    "# KNN with k search\n",
    "k_scores = {}\n",
    "for k in [3, 5, 7, 9, 11]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, n_jobs=-1)\n",
    "    knn.fit(X_train_cs, y_train_c)\n",
    "    k_scores[k] = accuracy_score(y_test_c, knn.predict(X_test_cs))\n",
    "best_k = max(k_scores, key=k_scores.get)\n",
    "print(f'  KNN by k: {k_scores} -> best k={best_k}')\n",
    "\n",
    "r = eval_clf(f'KNN (k={best_k})', KNeighborsClassifier(n_neighbors=best_k, n_jobs=-1),\n",
    "             X_train_c, y_train_c, X_test_c, y_test_c, needs_scale=True)\n",
    "clf_results[f'KNN (k={best_k})'] = r\n",
    "print(f'  {\"KNN (k=\"+str(best_k)+\")\":30s} Acc={r[\"Accuracy\"]:.4f}  F1={r[\"F1 Score\"]:.4f}')\n",
    "\n",
    "print(f'\\nAll {len(clf_results)} classifiers trained!')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 · Task 3 — Player Position Classification\n",
    "Predict a player's **position** (Goalkeeper, Defender, Midfielder, Forward) from age, caps, goals, and World Cup goals."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── Player position classification using squad data ──\n",
    "pos_df = squads[['Position', 'Age', 'Caps', 'Goals', 'WC Goals']].dropna().copy()\n",
    "le_pos = LabelEncoder()\n",
    "pos_df['pos_enc'] = le_pos.fit_transform(pos_df['Position'])\n",
    "pos_classes = list(le_pos.classes_)\n",
    "\n",
    "X_pos = pos_df[['Age', 'Caps', 'Goals', 'WC Goals']].astype(float)\n",
    "y_pos = pos_df['pos_enc']\n",
    "\n",
    "X_train_p, X_test_p, y_train_p, y_test_p = train_test_split(\n",
    "    X_pos, y_pos, test_size=0.2, random_state=SEED, stratify=y_pos)\n",
    "\n",
    "scaler_p = StandardScaler()\n",
    "X_train_ps = scaler_p.fit_transform(X_train_p)\n",
    "X_test_ps = scaler_p.transform(X_test_p)\n",
    "\n",
    "print(f'Player position dataset: {len(pos_df)} rows')\n",
    "print(f'Classes: {pos_classes}')\n",
    "print(pos_df['Position'].value_counts())\n",
    "print(f'Train: {len(X_train_p)}  |  Test: {len(X_test_p)}')\n",
    "\n",
    "pos_results = {}\n",
    "pos_classifiers = [\n",
    "    ('Logistic Regression', LogisticRegression(max_iter=2000, random_state=SEED), True),\n",
    "    ('Decision Tree', DecisionTreeClassifier(max_depth=8, random_state=SEED), False),\n",
    "    ('Random Forest', RandomForestClassifier(n_estimators=200, random_state=SEED, n_jobs=-1), False),\n",
    "    ('Gradient Boosting', GradientBoostingClassifier(n_estimators=200, random_state=SEED), False),\n",
    "    ('SVM (rbf)', SVC(kernel='rbf', random_state=SEED), True),\n",
    "    ('KNN (k=5)', KNeighborsClassifier(n_neighbors=5, n_jobs=-1), True),\n",
    "    ('MLP', MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=500,\n",
    "                           random_state=SEED, early_stopping=True), True),\n",
    "]\n",
    "\n",
    "print(f'\\nPlayer Position Classification')\n",
    "print('=' * 65)\n",
    "\n",
    "for name, model, needs_scale in pos_classifiers:\n",
    "    Xtr_ = X_train_ps if needs_scale else X_train_p\n",
    "    Xte_ = X_test_ps if needs_scale else X_test_p\n",
    "    model.fit(Xtr_, y_train_p)\n",
    "    pred = model.predict(Xte_)\n",
    "    acc = accuracy_score(y_test_p, pred)\n",
    "    f1 = f1_score(y_test_p, pred, average='weighted')\n",
    "    prec = precision_score(y_test_p, pred, average='weighted')\n",
    "    rec = recall_score(y_test_p, pred, average='weighted')\n",
    "    pos_results[name] = {'Accuracy': acc, 'Precision': prec, 'Recall': rec,\n",
    "                          'F1 Score': f1, 'model': model, 'predictions': pred}\n",
    "    print(f'  {name:30s} Acc={acc:.4f}  F1={f1:.4f}')\n",
    "\n",
    "# ── Position classification plots ──\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart\n",
    "names_p = list(pos_results.keys())\n",
    "f1s_p = [pos_results[n]['F1 Score'] for n in names_p]\n",
    "colors_p = plt.cm.Set2(np.linspace(0, 1, len(names_p)))\n",
    "axes[0].barh(names_p, f1s_p, color=colors_p, edgecolor='white')\n",
    "axes[0].set_xlabel('F1 Score'); axes[0].set_title('Player Position — Model F1 Scores')\n",
    "for i, v in enumerate(f1s_p):\n",
    "    axes[0].text(v + 0.005, i, f'{v:.4f}', va='center', fontsize=8)\n",
    "\n",
    "# Best model confusion matrix\n",
    "best_pos_name = max(pos_results, key=lambda n: pos_results[n]['F1 Score'])\n",
    "cm = confusion_matrix(y_test_p, pos_results[best_pos_name]['predictions'])\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=pos_classes,\n",
    "            yticklabels=pos_classes, ax=axes[1])\n",
    "axes[1].set_xlabel('Predicted'); axes[1].set_ylabel('Actual')\n",
    "axes[1].set_title(f'Confusion Matrix — {best_pos_name}')\n",
    "fig.tight_layout()\n",
    "save(fig, 'position_classification.png', 'Player Position Classification')\n",
    "\n",
    "all_results['position_clf'] = {k: {m: v for m, v in v.items() if m not in ('model', 'predictions')}\n",
    "                                for k, v in pos_results.items()}\n",
    "print(f'\\nBest: {best_pos_name} (F1 = {pos_results[best_pos_name][\"F1 Score\"]:.4f})')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 · Task 4 — Country Performance Clustering\n",
    "Cluster countries by their historical performance metrics: win rate, average goals, total matches."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── Build country profiles ──\n",
    "home_stats = df.groupby('Home Team').agg(\n",
    "    home_matches=('result', 'count'),\n",
    "    home_wins=('result', lambda x: (x == 'Home Win').sum()),\n",
    "    home_gf=('Home Goals', 'mean'),\n",
    "    home_ga=('Away Goals', 'mean'),\n",
    ").rename_axis('team')\n",
    "\n",
    "away_stats = df.groupby('Away Team').agg(\n",
    "    away_matches=('result', 'count'),\n",
    "    away_wins=('result', lambda x: (x == 'Away Win').sum()),\n",
    "    away_gf=('Away Goals', 'mean'),\n",
    "    away_ga=('Home Goals', 'mean'),\n",
    ").rename_axis('team')\n",
    "\n",
    "country_profiles = home_stats.join(away_stats, how='outer').fillna(0)\n",
    "country_profiles['total_matches'] = country_profiles['home_matches'] + country_profiles['away_matches']\n",
    "country_profiles['total_wins'] = country_profiles['home_wins'] + country_profiles['away_wins']\n",
    "country_profiles['win_rate'] = country_profiles['total_wins'] / country_profiles['total_matches']\n",
    "country_profiles['avg_gf'] = (country_profiles['home_gf'] * country_profiles['home_matches'] +\n",
    "                               country_profiles['away_gf'] * country_profiles['away_matches']) / country_profiles['total_matches']\n",
    "country_profiles['avg_ga'] = (country_profiles['home_ga'] * country_profiles['home_matches'] +\n",
    "                               country_profiles['away_ga'] * country_profiles['away_matches']) / country_profiles['total_matches']\n",
    "\n",
    "# Filter: at least 20 matches\n",
    "cp = country_profiles[country_profiles['total_matches'] >= 20].copy()\n",
    "cluster_features = ['win_rate', 'avg_gf', 'avg_ga', 'total_matches']\n",
    "\n",
    "X_clust = cp[cluster_features].values\n",
    "scaler_cl = StandardScaler()\n",
    "X_clust_s = scaler_cl.fit_transform(X_clust)\n",
    "\n",
    "print(f'Clustering: {len(cp)} countries (min 20 matches)')\n",
    "\n",
    "# ── Elbow + Silhouette ──\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "K_range = range(2, 11)\n",
    "inertias, sils = [], []\n",
    "for k in K_range:\n",
    "    km = KMeans(n_clusters=k, random_state=SEED, n_init=10)\n",
    "    km.fit(X_clust_s)\n",
    "    inertias.append(km.inertia_)\n",
    "    sils.append(silhouette_score(X_clust_s, km.labels_))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "axes[0].plot(list(K_range), inertias, 'o-', color='#2E86AB', linewidth=2)\n",
    "axes[0].set_xlabel('k'); axes[0].set_ylabel('Inertia')\n",
    "axes[0].set_title('Elbow Method')\n",
    "axes[1].plot(list(K_range), sils, 'o-', color='#F18F01', linewidth=2)\n",
    "axes[1].set_xlabel('k'); axes[1].set_ylabel('Silhouette Score')\n",
    "axes[1].set_title('Silhouette Scores')\n",
    "fig.tight_layout()\n",
    "save(fig, 'elbow_silhouette.png', 'Elbow & Silhouette Analysis')\n",
    "\n",
    "best_k_cl = list(K_range)[np.argmax(sils)]\n",
    "print(f'Best k = {best_k_cl} (silhouette = {max(sils):.4f})')\n",
    "\n",
    "# ── Final clustering ──\n",
    "km_final = KMeans(n_clusters=best_k_cl, random_state=SEED, n_init=10)\n",
    "cp['cluster'] = km_final.fit_predict(X_clust_s)\n",
    "\n",
    "# PCA for visualisation\n",
    "pca = PCA(n_components=2, random_state=SEED)\n",
    "X_pca = pca.fit_transform(X_clust_s)\n",
    "\n",
    "# DBSCAN\n",
    "db = DBSCAN(eps=1.5, min_samples=3)\n",
    "cp['dbscan_cluster'] = db.fit_predict(X_clust_s)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "colors_cl = plt.cm.Set1(np.linspace(0, 1, best_k_cl))\n",
    "for cl in range(best_k_cl):\n",
    "    mask = cp['cluster'] == cl\n",
    "    axes[0].scatter(X_pca[mask, 0], X_pca[mask, 1], label=f'Cluster {cl}',\n",
    "                     alpha=0.7, s=60, edgecolors='white', linewidths=0.5)\n",
    "# Annotate top teams\n",
    "for i, (team, row) in enumerate(cp.iterrows()):\n",
    "    if row['total_matches'] > 200:\n",
    "        axes[0].annotate(team, (X_pca[i, 0], X_pca[i, 1]), fontsize=7, alpha=0.8)\n",
    "axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n",
    "axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n",
    "axes[0].set_title(f'K-Means Clustering (k={best_k_cl})')\n",
    "axes[0].legend(fontsize=8)\n",
    "\n",
    "# DBSCAN\n",
    "unique_db = sorted(cp['dbscan_cluster'].unique())\n",
    "for cl in unique_db:\n",
    "    mask = cp['dbscan_cluster'] == cl\n",
    "    label = f'Cluster {cl}' if cl >= 0 else 'Noise'\n",
    "    axes[1].scatter(X_pca[mask, 0], X_pca[mask, 1], label=label, alpha=0.7, s=60,\n",
    "                     edgecolors='white', linewidths=0.5)\n",
    "axes[1].set_xlabel(f'PC1'); axes[1].set_ylabel(f'PC2')\n",
    "axes[1].set_title('DBSCAN Clustering')\n",
    "axes[1].legend(fontsize=8)\n",
    "fig.tight_layout()\n",
    "save(fig, 'clustering_results.png', 'Country Clustering Results')\n",
    "\n",
    "# Cluster profiles\n",
    "print(f'\\nCluster Profiles:')\n",
    "print(cp.groupby('cluster')[cluster_features].agg(['mean', 'count']).round(2).to_string())\n",
    "\n",
    "all_results['clustering'] = {\n",
    "    'k': best_k_cl, 'silhouette': max(sils),\n",
    "    'profiles': cp.groupby('cluster')[cluster_features].mean().round(3).to_dict()\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 · Hyperparameter Tuning\n",
    "GridSearchCV and RandomizedSearchCV on the best classifiers for match outcome prediction."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── GridSearchCV — Random Forest ──\n",
    "print('GridSearchCV: Random Forest ...')\n",
    "rf_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 15, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "}\n",
    "gs_rf = GridSearchCV(RandomForestClassifier(random_state=SEED, n_jobs=-1),\n",
    "                      rf_grid, cv=3, scoring='f1_weighted', n_jobs=-1, verbose=0)\n",
    "gs_rf.fit(X_train_c, y_train_c)\n",
    "pred_gs = gs_rf.predict(X_test_c)\n",
    "acc_gs = accuracy_score(y_test_c, pred_gs)\n",
    "f1_gs = f1_score(y_test_c, pred_gs, average='weighted')\n",
    "print(f'  Best params: {gs_rf.best_params_}')\n",
    "print(f'  Best CV F1:  {gs_rf.best_score_:.4f}')\n",
    "print(f'  Test Acc:    {acc_gs:.4f}  F1: {f1_gs:.4f}')\n",
    "\n",
    "clf_results['RF (Tuned)'] = {\n",
    "    'Accuracy': acc_gs, 'F1 Score': f1_gs,\n",
    "    'Precision': precision_score(y_test_c, pred_gs, average='weighted'),\n",
    "    'Recall': recall_score(y_test_c, pred_gs, average='weighted'),\n",
    "    'model': gs_rf.best_estimator_, 'predictions': pred_gs}\n",
    "\n",
    "# ── RandomizedSearchCV — Gradient Boosting ──\n",
    "print(f'\\nRandomizedSearchCV: Gradient Boosting ...')\n",
    "gb_dist = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "}\n",
    "rs_gb = RandomizedSearchCV(GradientBoostingClassifier(random_state=SEED),\n",
    "                            gb_dist, n_iter=20, cv=3, scoring='f1_weighted',\n",
    "                            random_state=SEED, n_jobs=-1, verbose=0)\n",
    "rs_gb.fit(X_train_c, y_train_c)\n",
    "pred_rs = rs_gb.predict(X_test_c)\n",
    "acc_rs = accuracy_score(y_test_c, pred_rs)\n",
    "f1_rs = f1_score(y_test_c, pred_rs, average='weighted')\n",
    "print(f'  Best params: {rs_gb.best_params_}')\n",
    "print(f'  Test Acc:    {acc_rs:.4f}  F1: {f1_rs:.4f}')\n",
    "\n",
    "clf_results['GB (Tuned)'] = {\n",
    "    'Accuracy': acc_rs, 'F1 Score': f1_rs,\n",
    "    'Precision': precision_score(y_test_c, pred_rs, average='weighted'),\n",
    "    'Recall': recall_score(y_test_c, pred_rs, average='weighted'),\n",
    "    'model': rs_gb.best_estimator_, 'predictions': pred_rs}\n",
    "\n",
    "print(f'\\nHyperparameter tuning complete!')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 · Cross-Validation, Confusion Matrices & Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── 5-Fold Cross-Validation ──\n",
    "cv_models = {\n",
    "    'Logistic Regression': (LogisticRegression(max_iter=2000, random_state=SEED), True),\n",
    "    'Random Forest': (RandomForestClassifier(n_estimators=200, max_depth=15, random_state=SEED, n_jobs=-1), False),\n",
    "    'Gradient Boosting': (GradientBoostingClassifier(n_estimators=200, max_depth=5, random_state=SEED), False),\n",
    "    'MLP': (MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=500, random_state=SEED, early_stopping=True), True),\n",
    "}\n",
    "\n",
    "cv_scores = {}\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "for name, (model, needs_scale) in cv_models.items():\n",
    "    X_cv = X_train_cs if needs_scale else X_train_c\n",
    "    scores = cross_val_score(model, X_cv, y_train_c, cv=skf, scoring='f1_weighted', n_jobs=-1)\n",
    "    cv_scores[name] = scores\n",
    "    print(f'{name:30s} CV F1: {scores.mean():.4f} ± {scores.std():.4f}')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.boxplot(cv_scores.values(), labels=cv_scores.keys(), patch_artist=True,\n",
    "           boxprops=dict(facecolor='#2E86AB', alpha=0.7))\n",
    "ax.set_ylabel('F1 Score'); ax.set_title('5-Fold Cross-Validation — F1 Scores')\n",
    "ax.tick_params(axis='x', rotation=15)\n",
    "fig.tight_layout()\n",
    "save(fig, 'cv_comparison.png', 'Cross-Validation Comparison')\n",
    "\n",
    "# ── Feature Importance — RF ──\n",
    "rf_model = clf_results.get('RF (Tuned)', clf_results.get('Random Forest'))['model']\n",
    "if hasattr(rf_model, 'feature_importances_'):\n",
    "    imp_clf = pd.Series(rf_model.feature_importances_, index=clf_features).sort_values()\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    imp_clf.plot.barh(ax=ax, color='#2E86AB', edgecolor='white')\n",
    "    ax.set_xlabel('Feature Importance')\n",
    "    ax.set_title('Match Outcome — Feature Importance (Random Forest)')\n",
    "    fig.tight_layout()\n",
    "    save(fig, 'feature_importance.png', 'Match Outcome Feature Importance')\n",
    "\n",
    "# ── Confusion Matrices (top 4 classifiers) ──\n",
    "top4 = sorted(clf_results, key=lambda n: clf_results[n]['F1 Score'], reverse=True)[:4]\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "for ax, name in zip(axes.ravel(), top4):\n",
    "    cm = confusion_matrix(y_test_c, clf_results[name]['predictions'])\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names, ax=ax)\n",
    "    ax.set_title(f'{name}\\n(F1={clf_results[name][\"F1 Score\"]:.4f})')\n",
    "    ax.set_xlabel('Predicted'); ax.set_ylabel('Actual')\n",
    "fig.suptitle('Confusion Matrices — Top 4 Classifiers', fontsize=14, y=1.02)\n",
    "fig.tight_layout()\n",
    "save(fig, 'confusion_matrices.png', 'Confusion Matrices — Top 4')\n",
    "\n",
    "# ── Learning Curves (best model) ──\n",
    "best_clf_name = top4[0]\n",
    "best_needs_scale = best_clf_name in ['Logistic Regression', 'SVM (linear)', 'SVM (rbf)',\n",
    "                                      'MLP Neural Network', 'MLP', 'Naive Bayes'] or 'KNN' in best_clf_name\n",
    "X_lc = X_train_cs if best_needs_scale else X_train_c\n",
    "\n",
    "# Re-create model for learning curve\n",
    "if 'Random Forest' in best_clf_name or 'RF' in best_clf_name:\n",
    "    lc_model = RandomForestClassifier(n_estimators=200, max_depth=15, random_state=SEED, n_jobs=-1)\n",
    "elif 'Gradient Boosting' in best_clf_name or 'GB' in best_clf_name:\n",
    "    lc_model = GradientBoostingClassifier(n_estimators=200, max_depth=5, random_state=SEED)\n",
    "elif 'MLP' in best_clf_name:\n",
    "    lc_model = MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=500, random_state=SEED, early_stopping=True)\n",
    "else:\n",
    "    lc_model = LogisticRegression(max_iter=2000, random_state=SEED)\n",
    "\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    lc_model, X_lc, y_train_c, cv=5, scoring='f1_weighted',\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10), n_jobs=-1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.plot(train_sizes, train_scores.mean(axis=1), 'o-', color='#2E86AB', label='Train')\n",
    "ax.fill_between(train_sizes, train_scores.mean(axis=1) - train_scores.std(axis=1),\n",
    "                train_scores.mean(axis=1) + train_scores.std(axis=1), alpha=0.1, color='#2E86AB')\n",
    "ax.plot(train_sizes, val_scores.mean(axis=1), 'o-', color='#F18F01', label='Validation')\n",
    "ax.fill_between(train_sizes, val_scores.mean(axis=1) - val_scores.std(axis=1),\n",
    "                val_scores.mean(axis=1) + val_scores.std(axis=1), alpha=0.1, color='#F18F01')\n",
    "ax.set_xlabel('Training Size'); ax.set_ylabel('F1 Score')\n",
    "ax.set_title(f'Learning Curve — {best_clf_name}')\n",
    "ax.legend(); ax.grid(True, alpha=0.3)\n",
    "fig.tight_layout()\n",
    "save(fig, 'learning_curves.png', f'Learning Curve — {best_clf_name}')\n",
    "\n",
    "print('CV, confusion matrices, and learning curves complete!')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 · Ensemble Methods & Final Comparison"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── Voting Classifier ──\n",
    "print('Training Voting Classifier ...')\n",
    "voting = VotingClassifier(estimators=[\n",
    "    ('rf', RandomForestClassifier(n_estimators=200, max_depth=15, random_state=SEED, n_jobs=-1)),\n",
    "    ('gb', GradientBoostingClassifier(n_estimators=200, max_depth=5, random_state=SEED)),\n",
    "    ('lr', LogisticRegression(max_iter=2000, random_state=SEED)),\n",
    "], voting='hard', n_jobs=-1)\n",
    "# Use scaled data for LR inside pipeline — but VotingClassifier with hard voting\n",
    "# We'll use unscaled; LR will converge anyway on this data\n",
    "voting.fit(X_train_c, y_train_c)\n",
    "pred_v = voting.predict(X_test_c)\n",
    "clf_results['Voting Ensemble'] = {\n",
    "    'Accuracy': accuracy_score(y_test_c, pred_v),\n",
    "    'F1 Score': f1_score(y_test_c, pred_v, average='weighted'),\n",
    "    'Precision': precision_score(y_test_c, pred_v, average='weighted'),\n",
    "    'Recall': recall_score(y_test_c, pred_v, average='weighted'),\n",
    "    'model': voting, 'predictions': pred_v}\n",
    "print(f'  Voting Ensemble               Acc={clf_results[\"Voting Ensemble\"][\"Accuracy\"]:.4f}  '\n",
    "      f'F1={clf_results[\"Voting Ensemble\"][\"F1 Score\"]:.4f}')\n",
    "\n",
    "# ── Stacking Classifier ──\n",
    "print('Training Stacking Classifier ...')\n",
    "stacking = StackingClassifier(estimators=[\n",
    "    ('rf', RandomForestClassifier(n_estimators=200, max_depth=15, random_state=SEED, n_jobs=-1)),\n",
    "    ('gb', GradientBoostingClassifier(n_estimators=200, max_depth=5, random_state=SEED)),\n",
    "    ('dt', DecisionTreeClassifier(max_depth=10, random_state=SEED)),\n",
    "], final_estimator=LogisticRegression(max_iter=2000, random_state=SEED),\n",
    "    cv=3, n_jobs=-1)\n",
    "stacking.fit(X_train_c, y_train_c)\n",
    "pred_s = stacking.predict(X_test_c)\n",
    "clf_results['Stacking Ensemble'] = {\n",
    "    'Accuracy': accuracy_score(y_test_c, pred_s),\n",
    "    'F1 Score': f1_score(y_test_c, pred_s, average='weighted'),\n",
    "    'Precision': precision_score(y_test_c, pred_s, average='weighted'),\n",
    "    'Recall': recall_score(y_test_c, pred_s, average='weighted'),\n",
    "    'model': stacking, 'predictions': pred_s}\n",
    "print(f'  Stacking Ensemble             Acc={clf_results[\"Stacking Ensemble\"][\"Accuracy\"]:.4f}  '\n",
    "      f'F1={clf_results[\"Stacking Ensemble\"][\"F1 Score\"]:.4f}')\n",
    "\n",
    "# ── Final Comparison Table ──\n",
    "comp = pd.DataFrame({\n",
    "    name: {k: v for k, v in metrics.items() if k not in ('model', 'predictions')}\n",
    "    for name, metrics in clf_results.items()\n",
    "}).T.sort_values('F1 Score', ascending=False)\n",
    "comp.index.name = 'Model'\n",
    "\n",
    "print(f'\\nFinal Model Comparison:')\n",
    "print(comp.to_string(float_format=lambda x: f'{x:.6f}'))\n",
    "\n",
    "# ── Comparison Plot ──\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "colors_bar = plt.cm.viridis(np.linspace(0.2, 0.9, len(comp)))\n",
    "bars = ax.barh(range(len(comp)), comp['F1 Score'], color=colors_bar, edgecolor='white')\n",
    "ax.set_yticks(range(len(comp)))\n",
    "ax.set_yticklabels(comp.index, fontsize=9)\n",
    "ax.set_xlabel('F1 Score (weighted)')\n",
    "ax.set_title('Match Outcome Classification — All Models')\n",
    "for i, v in enumerate(comp['F1 Score']):\n",
    "    ax.text(v + 0.002, i, f'{v:.4f}', va='center', fontsize=8)\n",
    "ax.invert_yaxis()\n",
    "fig.tight_layout()\n",
    "save(fig, 'model_comparison.png', 'Model Comparison — Match Outcome')\n",
    "\n",
    "all_results['match_clf'] = comp.to_dict('index')\n",
    "\n",
    "best_overall = comp.index[0]\n",
    "print(f'\\nBest model: {best_overall} (F1 = {comp.loc[best_overall, \"F1 Score\"]:.4f})')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11 · HTML Report Generation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ── Generate HTML Report ──\n",
    "def img_to_b64(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return base64.b64encode(f.read()).decode()\n",
    "\n",
    "HTML_TEMPLATE = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "<meta charset=\"UTF-8\"><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "<title>Football ML Analysis Report</title>\n",
    "<style>\n",
    "  :root { --primary: #1B4332; --accent: #2D6A4F; --light: #D8F3DC; --bg: #F8FAF9; }\n",
    "  * { margin: 0; padding: 0; box-sizing: border-box; }\n",
    "  body { font-family: 'Segoe UI', system-ui, sans-serif; background: var(--bg); color: #333; line-height: 1.6; }\n",
    "  .header { background: linear-gradient(135deg, var(--primary), var(--accent)); color: white;\n",
    "             padding: 3rem 2rem; text-align: center; }\n",
    "  .header h1 { font-size: 2.2rem; margin-bottom: 0.5rem; }\n",
    "  .header p { opacity: 0.9; font-size: 1.1rem; }\n",
    "  .container { max-width: 1200px; margin: 0 auto; padding: 2rem; }\n",
    "  .section { background: white; border-radius: 12px; padding: 2rem; margin-bottom: 2rem;\n",
    "             box-shadow: 0 2px 8px rgba(0,0,0,0.08); }\n",
    "  .section h2 { color: var(--primary); border-bottom: 3px solid var(--accent);\n",
    "                 padding-bottom: 0.5rem; margin-bottom: 1.5rem; font-size: 1.5rem; }\n",
    "  .section h3 { color: var(--accent); margin: 1rem 0 0.5rem; }\n",
    "  table { width: 100%%; border-collapse: collapse; margin: 1rem 0; font-size: 0.9rem; }\n",
    "  th { background: var(--accent); color: white; padding: 10px 12px; text-align: left; }\n",
    "  td { padding: 8px 12px; border-bottom: 1px solid #e0e0e0; }\n",
    "  tr:nth-child(even) { background: var(--light); }\n",
    "  tr:hover { background: #B7E4C7; }\n",
    "  .best { background: #95D5B2 !important; font-weight: bold; }\n",
    "  img { max-width: 100%%; border-radius: 8px; margin: 1rem 0; }\n",
    "  .grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(400px, 1fr)); gap: 1.5rem; }\n",
    "  .metric-card { background: var(--light); border-radius: 8px; padding: 1.5rem; text-align: center; }\n",
    "  .metric-card .value { font-size: 2rem; font-weight: bold; color: var(--primary); }\n",
    "  .metric-card .label { color: #666; font-size: 0.9rem; }\n",
    "  .footer { text-align: center; padding: 2rem; color: #888; font-size: 0.85rem; }\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "<div class=\"header\">\n",
    "  <h1>⚽ FIFA World Cup & International Football — ML Report</h1>\n",
    "  <p>Comprehensive Machine Learning Analysis • {{ n_matches }} matches • {{ n_teams }} teams • {{ year_range }}</p>\n",
    "</div>\n",
    "<div class=\"container\">\n",
    "\n",
    "  <!-- Key metrics -->\n",
    "  <div class=\"grid\" style=\"margin-bottom:2rem;\">\n",
    "    <div class=\"metric-card\"><div class=\"value\">{{ n_matches }}</div><div class=\"label\">International Matches</div></div>\n",
    "    <div class=\"metric-card\"><div class=\"value\">{{ n_teams }}</div><div class=\"label\">Unique Teams</div></div>\n",
    "    <div class=\"metric-card\"><div class=\"value\">{{ n_wc }}</div><div class=\"label\">World Cup Editions</div></div>\n",
    "    <div class=\"metric-card\"><div class=\"value\">{{ n_players }}</div><div class=\"label\">Squad Players (2022)</div></div>\n",
    "  </div>\n",
    "\n",
    "  <!-- EDA -->\n",
    "  <div class=\"section\">\n",
    "    <h2>1 · Exploratory Data Analysis</h2>\n",
    "    <div class=\"grid\">\n",
    "    {% for title, b64 in eda_plots %}\n",
    "      <div><h3>{{ title }}</h3><img src=\"data:image/png;base64,{{ b64 }}\" alt=\"{{ title }}\"></div>\n",
    "    {% endfor %}\n",
    "    </div>\n",
    "  </div>\n",
    "\n",
    "  <!-- Regression -->\n",
    "  <div class=\"section\">\n",
    "    <h2>2 · Total Goals Regression</h2>\n",
    "    <table>\n",
    "      <tr><th>Model</th><th>R²</th><th>MAE</th><th>RMSE</th></tr>\n",
    "      {% for name, m in reg_results.items() %}\n",
    "      <tr{% if loop.index == 1 %} class=\"best\"{% endif %}>\n",
    "        <td>{{ name }}</td><td>{{ \"%.4f\"|format(m['R²']) }}</td>\n",
    "        <td>{{ \"%.2f\"|format(m['MAE']) }}</td><td>{{ \"%.2f\"|format(m['RMSE']) }}</td></tr>\n",
    "      {% endfor %}\n",
    "    </table>\n",
    "    <div class=\"grid\">\n",
    "    {% for title, b64 in reg_plots %}\n",
    "      <div><h3>{{ title }}</h3><img src=\"data:image/png;base64,{{ b64 }}\" alt=\"{{ title }}\"></div>\n",
    "    {% endfor %}\n",
    "    </div>\n",
    "  </div>\n",
    "\n",
    "  <!-- Classification -->\n",
    "  <div class=\"section\">\n",
    "    <h2>3 · Match Outcome Classification</h2>\n",
    "    <table>\n",
    "      <tr><th>Model</th><th>Accuracy</th><th>Precision</th><th>Recall</th><th>F1 Score</th></tr>\n",
    "      {% for name, m in clf_table.items() %}\n",
    "      <tr{% if loop.index == 1 %} class=\"best\"{% endif %}>\n",
    "        <td>{{ name }}</td><td>{{ \"%.4f\"|format(m['Accuracy']) }}</td>\n",
    "        <td>{{ \"%.4f\"|format(m['Precision']) }}</td><td>{{ \"%.4f\"|format(m['Recall']) }}</td>\n",
    "        <td>{{ \"%.4f\"|format(m['F1 Score']) }}</td></tr>\n",
    "      {% endfor %}\n",
    "    </table>\n",
    "    <div class=\"grid\">\n",
    "    {% for title, b64 in clf_plots %}\n",
    "      <div><h3>{{ title }}</h3><img src=\"data:image/png;base64,{{ b64 }}\" alt=\"{{ title }}\"></div>\n",
    "    {% endfor %}\n",
    "    </div>\n",
    "  </div>\n",
    "\n",
    "  <!-- Position Classification -->\n",
    "  <div class=\"section\">\n",
    "    <h2>4 · Player Position Classification</h2>\n",
    "    <table>\n",
    "      <tr><th>Model</th><th>Accuracy</th><th>Precision</th><th>Recall</th><th>F1 Score</th></tr>\n",
    "      {% for name, m in pos_table.items() %}\n",
    "      <tr{% if loop.index == 1 %} class=\"best\"{% endif %}>\n",
    "        <td>{{ name }}</td><td>{{ \"%.4f\"|format(m['Accuracy']) }}</td>\n",
    "        <td>{{ \"%.4f\"|format(m['Precision']) }}</td><td>{{ \"%.4f\"|format(m['Recall']) }}</td>\n",
    "        <td>{{ \"%.4f\"|format(m['F1 Score']) }}</td></tr>\n",
    "      {% endfor %}\n",
    "    </table>\n",
    "    <div class=\"grid\">\n",
    "    {% for title, b64 in pos_plots %}\n",
    "      <div><h3>{{ title }}</h3><img src=\"data:image/png;base64,{{ b64 }}\" alt=\"{{ title }}\"></div>\n",
    "    {% endfor %}\n",
    "    </div>\n",
    "  </div>\n",
    "\n",
    "  <!-- Clustering -->\n",
    "  <div class=\"section\">\n",
    "    <h2>5 · Country Performance Clustering</h2>\n",
    "    <div class=\"grid\" style=\"margin-bottom:1rem;\">\n",
    "      <div class=\"metric-card\"><div class=\"value\">{{ cluster_k }}</div><div class=\"label\">Optimal Clusters</div></div>\n",
    "      <div class=\"metric-card\"><div class=\"value\">{{ \"%.4f\"|format(cluster_sil) }}</div><div class=\"label\">Silhouette Score</div></div>\n",
    "    </div>\n",
    "    <div class=\"grid\">\n",
    "    {% for title, b64 in cluster_plots %}\n",
    "      <div><h3>{{ title }}</h3><img src=\"data:image/png;base64,{{ b64 }}\" alt=\"{{ title }}\"></div>\n",
    "    {% endfor %}\n",
    "    </div>\n",
    "  </div>\n",
    "\n",
    "  <!-- Analysis -->\n",
    "  <div class=\"section\">\n",
    "    <h2>6 · Cross-Validation & Analysis</h2>\n",
    "    <div class=\"grid\">\n",
    "    {% for title, b64 in analysis_plots %}\n",
    "      <div><h3>{{ title }}</h3><img src=\"data:image/png;base64,{{ b64 }}\" alt=\"{{ title }}\"></div>\n",
    "    {% endfor %}\n",
    "    </div>\n",
    "  </div>\n",
    "\n",
    "  <!-- Final comparison -->\n",
    "  <div class=\"section\">\n",
    "    <h2>7 · Final Model Comparison</h2>\n",
    "    {% for title, b64 in final_plots %}\n",
    "      <h3>{{ title }}</h3><img src=\"data:image/png;base64,{{ b64 }}\" alt=\"{{ title }}\">\n",
    "    {% endfor %}\n",
    "  </div>\n",
    "\n",
    "</div>\n",
    "<div class=\"footer\">\n",
    "  Generated automatically · Football ML Analysis · scikit-learn {{ sklearn_ver }}\n",
    "</div>\n",
    "</body></html>\n",
    "\"\"\"\n",
    "\n",
    "# Categorise saved plots\n",
    "plot_categories = {\n",
    "    'eda': ['goals_per_decade', 'outcome_distribution', 'top20_teams', 'wc_goals_trend',\n",
    "            'home_advantage', 'correlation_heatmap'],\n",
    "    'reg': ['regression_results', 'regression_feature_importance'],\n",
    "    'clf': ['confusion_matrices', 'feature_importance'],\n",
    "    'pos': ['position_classification'],\n",
    "    'cluster': ['elbow_silhouette', 'clustering_results'],\n",
    "    'analysis': ['cv_comparison', 'learning_curves'],\n",
    "    'final': ['model_comparison'],\n",
    "}\n",
    "\n",
    "def get_plots(category):\n",
    "    keys = plot_categories[category]\n",
    "    result = []\n",
    "    for title, path in saved_plots:\n",
    "        fname = pathlib.Path(path).stem\n",
    "        if fname in keys:\n",
    "            result.append((title, img_to_b64(path)))\n",
    "    return result\n",
    "\n",
    "# Sort regression results by R²\n",
    "reg_sorted = dict(sorted(all_results['regression'].items(),\n",
    "                          key=lambda x: x[1]['R²'], reverse=True))\n",
    "\n",
    "# Sort classification results by F1\n",
    "clf_sorted = dict(sorted(\n",
    "    {k: {m: v for m, v in met.items() if m not in ('model', 'predictions')}\n",
    "     for k, met in clf_results.items()}.items(),\n",
    "    key=lambda x: x[1]['F1 Score'], reverse=True))\n",
    "\n",
    "# Sort position results by F1\n",
    "pos_sorted = dict(sorted(all_results['position_clf'].items(),\n",
    "                          key=lambda x: x[1]['F1 Score'], reverse=True))\n",
    "\n",
    "import sklearn\n",
    "tmpl = Template(HTML_TEMPLATE)\n",
    "html = tmpl.render(\n",
    "    n_matches=f'{len(intl):,}',\n",
    "    n_teams=pd.concat([intl['Home Team'], intl['Away Team']]).nunique(),\n",
    "    year_range=f'{intl[\"year\"].min()}–{intl[\"year\"].max()}',\n",
    "    n_wc=len(wc_summary),\n",
    "    n_players=len(squads),\n",
    "    eda_plots=get_plots('eda'),\n",
    "    reg_results=reg_sorted,\n",
    "    reg_plots=get_plots('reg'),\n",
    "    clf_table=clf_sorted,\n",
    "    clf_plots=get_plots('clf'),\n",
    "    pos_table=pos_sorted,\n",
    "    pos_plots=get_plots('pos'),\n",
    "    cluster_k=all_results['clustering']['k'],\n",
    "    cluster_sil=all_results['clustering']['silhouette'],\n",
    "    cluster_plots=get_plots('cluster'),\n",
    "    analysis_plots=get_plots('analysis'),\n",
    "    final_plots=get_plots('final'),\n",
    "    sklearn_ver=sklearn.__version__,\n",
    ")\n",
    "\n",
    "report_path = pathlib.Path('outputs/football_ml_report.html')\n",
    "report_path.write_text(html)\n",
    "print(f'✅ HTML Report generated: {report_path}')\n",
    "print(f'   File size: {report_path.stat().st_size / 1024:.1f} KB')\n",
    "print(f'   Embedded images: {html.count(\"data:image/png;base64,\")}')\n",
    "print(f'\\n🎉 Analysis complete! Open the HTML file to view the full report.')"
   ],
   "outputs": [],
   "execution_count": null
  }
 ]
}