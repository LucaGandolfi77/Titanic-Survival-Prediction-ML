{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8568ec5a",
   "metadata": {},
   "source": [
    "# Stage 2 — DCGAN Analysis\n",
    "\n",
    "This notebook analyzes the **Deep Convolutional GAN** (Radford et al., 2016)\n",
    "and compares it against the Vanilla GAN baseline.\n",
    "\n",
    "Key architectural improvements:\n",
    "- Strided convolutions replace pooling\n",
    "- BatchNorm stabilizes training\n",
    "- No fully connected layers in the backbone\n",
    "- Spectral Normalization in the Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd907d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "if PROJECT_ROOT.name == \"notebooks\":\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from src.utils.config_loader import load_config, get_device\n",
    "from src.utils.checkpointing import load_checkpoint, find_latest_checkpoint\n",
    "from src.models.dcgan import build_dcgan\n",
    "from src.data.dataloaders import get_dataloader_from_config\n",
    "from src.evaluation.visualization import GANVisualizer\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49302c6a",
   "metadata": {},
   "source": [
    "## 1. Load Config & DCGAN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1aa3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(PROJECT_ROOT / \"config\" / \"dcgan.yaml\")\n",
    "device = get_device(config)\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "generator, discriminator = build_dcgan(config)\n",
    "generator = generator.to(device)\n",
    "discriminator = discriminator.to(device)\n",
    "\n",
    "print(f\"Generator params:     {sum(p.numel() for p in generator.parameters()):>10,}\")\n",
    "print(f\"Discriminator params: {sum(p.numel() for p in discriminator.parameters()):>10,}\")\n",
    "\n",
    "# Load checkpoint\n",
    "ckpt_path = find_latest_checkpoint(config['paths']['models_dir'])\n",
    "if ckpt_path:\n",
    "    info = load_checkpoint(ckpt_path, generator, discriminator, device=device)\n",
    "    print(f\"\\nLoaded: epoch {info['epoch']}, step {info['global_step']}\")\n",
    "else:\n",
    "    print(\"\\nNo checkpoint found — using untrained model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d797f6a9",
   "metadata": {},
   "source": [
    "## 2. Architecture Visualization\n",
    "\n",
    "**Generator** (64×64 output):\n",
    "```\n",
    "z [100, 1, 1]\n",
    "  → ConvT 4×4, s1  → [512, 4, 4]  + BN + ReLU\n",
    "  → ConvT 4×4, s2  → [256, 8, 8]  + BN + ReLU\n",
    "  → ConvT 4×4, s2  → [128, 16, 16] + BN + ReLU\n",
    "  → ConvT 4×4, s2  → [64, 32, 32]  + BN + ReLU\n",
    "  → ConvT 4×4, s2  → [1, 64, 64]   + Tanh\n",
    "```\n",
    "\n",
    "**Discriminator** (64×64 input):\n",
    "```\n",
    "[1, 64, 64]\n",
    "  → Conv 4×4, s2  → [64, 32, 32]  + LeakyReLU\n",
    "  → Conv 4×4, s2  → [128, 16, 16] + LeakyReLU\n",
    "  → Conv 4×4, s2  → [256, 8, 8]   + LeakyReLU\n",
    "  → Conv 4×4, s2  → [512, 4, 4]   + LeakyReLU\n",
    "  → Conv 4×4, s1  → [1, 1, 1]     (logit)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62be7bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Generator ===\")\n",
    "print(generator)\n",
    "print(f\"\\n=== Discriminator ===\")\n",
    "print(discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e768cf5",
   "metadata": {},
   "source": [
    "## 3. Generate & Visualize Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369c99c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.eval()\n",
    "latent_dim = config['model']['latent_dim']\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(64, latent_dim, device=device)\n",
    "    fake_images = generator(z)\n",
    "\n",
    "grid = make_grid(fake_images.cpu(), nrow=8, normalize=True, value_range=(-1, 1))\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(grid.permute(1, 2, 0).numpy(), cmap='gray')\n",
    "plt.title('DCGAN — Generated Samples (64×64)', fontsize=14)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2900a198",
   "metadata": {},
   "source": [
    "## 4. Real vs DCGAN Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17e09c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = get_dataloader_from_config(config)\n",
    "real_batch, _ = next(iter(dataloader))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "real_grid = make_grid(real_batch[:32], nrow=8, normalize=True, value_range=(-1, 1))\n",
    "ax1.imshow(real_grid.permute(1, 2, 0).numpy(), cmap='gray')\n",
    "ax1.set_title('Real MNIST (resized to 64×64)', fontsize=14)\n",
    "ax1.axis('off')\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(32, latent_dim, device=device)\n",
    "    fakes = generator(z)\n",
    "fake_grid = make_grid(fakes.cpu(), nrow=8, normalize=True, value_range=(-1, 1))\n",
    "ax2.imshow(fake_grid.permute(1, 2, 0).numpy(), cmap='gray')\n",
    "ax2.set_title('DCGAN Generated', fontsize=14)\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33300bdc",
   "metadata": {},
   "source": [
    "## 5. Latent Space Interpolation (Slerp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b535cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = GANVisualizer(output_dir=str(PROJECT_ROOT / \"outputs\"))\n",
    "\n",
    "n_pairs = 6\n",
    "n_steps = 12\n",
    "all_interp = []\n",
    "\n",
    "for _ in range(n_pairs):\n",
    "    z1 = torch.randn(latent_dim, device=device)\n",
    "    z2 = torch.randn(latent_dim, device=device)\n",
    "    z_interp = visualizer.spherical_interpolation(z1, z2, n_steps)\n",
    "    with torch.no_grad():\n",
    "        imgs = generator(z_interp)\n",
    "    all_interp.append(imgs)\n",
    "\n",
    "interp_grid = make_grid(\n",
    "    torch.cat(all_interp).cpu(), nrow=n_steps,\n",
    "    normalize=True, value_range=(-1, 1)\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(18, 9))\n",
    "plt.imshow(interp_grid.permute(1, 2, 0).numpy(), cmap='gray')\n",
    "plt.title('DCGAN — Spherical Interpolation in Latent Space', fontsize=14)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99eb68dc",
   "metadata": {},
   "source": [
    "## 6. Feature Map Visualization\n",
    "\n",
    "Inspect internal feature maps of the discriminator to see what it learns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8f8541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hook into discriminator first conv layer\n",
    "activations = {}\n",
    "\n",
    "def hook_fn(name):\n",
    "    def hook(module, input, output):\n",
    "        activations[name] = output.detach().cpu()\n",
    "    return hook\n",
    "\n",
    "# Register hooks on first few conv layers\n",
    "hooks = []\n",
    "for i, layer in enumerate(discriminator.net):\n",
    "    if isinstance(layer, torch.nn.Conv2d) and i < 6:\n",
    "        h = layer.register_forward_hook(hook_fn(f\"conv_{i}\"))\n",
    "        hooks.append(h)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    sample = real_batch[:1].to(device)\n",
    "    _ = discriminator(sample)\n",
    "\n",
    "# Plot feature maps\n",
    "for name, feat in activations.items():\n",
    "    n_maps = min(16, feat.shape[1])\n",
    "    fig, axes = plt.subplots(2, 8, figsize=(16, 4))\n",
    "    fig.suptitle(f'Feature maps: {name} ({feat.shape[1]} channels)', fontsize=12)\n",
    "    for idx, ax in enumerate(axes.flat):\n",
    "        if idx < n_maps:\n",
    "            ax.imshow(feat[0, idx].numpy(), cmap='viridis')\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Remove hooks\n",
    "for h in hooks:\n",
    "    h.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ad0488",
   "metadata": {},
   "source": [
    "## 7. Quantitative Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eaedde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation.fid_score import FIDCalculator\n",
    "from src.evaluation.inception_score import InceptionScoreCalculator\n",
    "\n",
    "N_EVAL = 1000\n",
    "\n",
    "# Real images\n",
    "real_list = []\n",
    "for imgs, _ in dataloader:\n",
    "    real_list.append(imgs)\n",
    "    if sum(x.size(0) for x in real_list) >= N_EVAL:\n",
    "        break\n",
    "real_imgs = torch.cat(real_list)[:N_EVAL]\n",
    "\n",
    "# Fake images  \n",
    "fake_list = []\n",
    "remaining = N_EVAL\n",
    "while remaining > 0:\n",
    "    bs = min(64, remaining)\n",
    "    z = torch.randn(bs, latent_dim, device=device)\n",
    "    with torch.no_grad():\n",
    "        fakes = generator(z)\n",
    "    fake_list.append(fakes.cpu())\n",
    "    remaining -= bs\n",
    "fake_imgs = torch.cat(fake_list)[:N_EVAL]\n",
    "\n",
    "print(\"Computing FID...\")\n",
    "fid_calc = FIDCalculator(device=str(device), batch_size=32)\n",
    "fid = fid_calc.compute_fid(real_imgs, fake_imgs)\n",
    "print(f\"DCGAN FID: {fid:.2f}\")\n",
    "\n",
    "print(\"\\nComputing IS...\")\n",
    "is_calc = InceptionScoreCalculator(device=str(device), batch_size=32)\n",
    "is_mean, is_std = is_calc.compute_inception_score(fake_imgs, splits=5)\n",
    "print(f\"DCGAN IS: {is_mean:.2f} ± {is_std:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
