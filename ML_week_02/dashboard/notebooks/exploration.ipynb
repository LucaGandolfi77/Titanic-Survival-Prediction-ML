{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d0c6248",
   "metadata": {},
   "source": [
    "# Explainable AI Dashboard – Exploration Notebook\n",
    "\n",
    "This notebook demonstrates the core XAI components independently,\n",
    "outside the Streamlit dashboard, for experimentation and debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f320b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a66aff6",
   "metadata": {},
   "source": [
    "## 1. Load Data & Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b28bc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/credit_risk_sample.csv')\n",
    "print(f'Shape: {df.shape}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab34e6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'default'\n",
    "protected = ['age', 'gender']\n",
    "feature_cols = [c for c in df.columns if c != target and c not in protected]\n",
    "\n",
    "X = df[feature_cols].copy()\n",
    "y = df[target].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "print(f'Accuracy: {model.score(X_test, y_test):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a08ed6",
   "metadata": {},
   "source": [
    "## 2. SHAP – Global Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f423aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.explainability.shap_explainer import SHAPExplainer\n",
    "from src.visualization.shap_plots import plot_global_importance, plot_beeswarm\n",
    "\n",
    "shap_exp = SHAPExplainer(model, X_train.sample(100, random_state=42))\n",
    "shap_vals = shap_exp.compute_shap_values(X_test)\n",
    "\n",
    "importance = shap_exp.global_importance(X_test)\n",
    "importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be49dece",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_global_importance(shap_vals, list(X_test.columns))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ccc189",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_beeswarm(shap_vals, X_test, top_k=10)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d02ab0d",
   "metadata": {},
   "source": [
    "## 3. LIME – Local Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5478438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.explainability.lime_explainer import LIMEExplainer\n",
    "from src.visualization.lime_plots import plot_lime_explanation\n",
    "\n",
    "lime_exp = LIMEExplainer(X_train, feature_names=list(X_train.columns))\n",
    "result = lime_exp.explain_instance(model, X_test.iloc[0].values, num_features=10)\n",
    "\n",
    "fig = plot_lime_explanation(result.feature_weights, result.prediction, result.prediction_proba)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4fca38",
   "metadata": {},
   "source": [
    "## 4. Fairness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3555e0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.fairness.metrics import compute_all_metrics\n",
    "from src.fairness.bias_detector import detect_bias, bias_summary_text\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "sensitive = df.loc[X_test.index, 'gender'].values\n",
    "\n",
    "bias_df = detect_bias(y_test.values, y_pred, sensitive, 'gender')\n",
    "print(bias_summary_text(bias_df))\n",
    "bias_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e82951c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visualization.fairness_plots import plot_bias_heatmap\n",
    "\n",
    "fig = plot_bias_heatmap(bias_df)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877ac580",
   "metadata": {},
   "source": [
    "## 5. Report Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832d341b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.metadata import extract_model_info, compute_performance\n",
    "from src.reporting.summary_generator import generate_executive_summary, generate_technical_report, to_plain_text\n",
    "\n",
    "info = extract_model_info(model, feature_names=list(X_train.columns))\n",
    "perf = compute_performance(y_test, y_pred, model, X_test)\n",
    "\n",
    "summary = generate_executive_summary(info, perf, bias_df, list(importance['feature'].head(5)))\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44989d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = generate_technical_report(info, perf, fairness_results=bias_df)\n",
    "print(to_plain_text(report))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
