# ðŸš¢ Titanic Survival Prediction

End-to-end binary classification on the classic
[Kaggle Titanic dataset](https://www.kaggle.com/c/titanic) using pure
**scikit-learn**.

---

## Project structure

```
titanic/
â”œâ”€â”€ titanic_eda.ipynb   # Full pipeline notebook (8 sections)
â”œâ”€â”€ train.csv           # Kaggle training set (891 rows)
â”œâ”€â”€ test.csv            # Kaggle test set (418 rows)
â”œâ”€â”€ requirements.txt    # Pinned dependencies
â””â”€â”€ README.md           # â† you are here
```

## Quick start

```bash
# 1. Create a virtual environment
python3 -m venv .venv && source .venv/bin/activate

# 2. Install dependencies
pip install -r requirements.txt

# 3. Open in VSCode and "Run All Cells"
code titanic_eda.ipynb
```

## Pipeline overview

| Section | What happens |
|---------|-------------|
| 0 | Setup & imports |
| 1 | Load data & first look (nulls heatmap) |
| 2 | EDA â€” 8 charts with bullet insights |
| 3 | Feature engineering (impute, encode, create, scale) |
| 4 | Model comparison (LogReg, RF, GBC, SVC) â€” 5-fold CV |
| 5 | Hyperparameter tuning (GridSearchCV) |
| 6 | Hold-out evaluation (confusion matrix, ROC, feature importance) |
| 7 | Predict on test set â†’ `submission.csv` |
| 8 | Key takeaways |

## Key results

| Model | CV Accuracy (mean Â± std) |
|-------|--------------------------|
| Logistic Regression | ~80 % |
| Random Forest | ~82 % |
| **Gradient Boosting** | **~83 %** |
| SVC | ~82 % |

> Results may vary slightly due to random splits.

## License

MIT â€” for educational purposes.
