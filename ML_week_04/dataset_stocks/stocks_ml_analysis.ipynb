{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“ˆ S&P 500 Stock Prices (2014â€“2017) â€“ ML Analysis\n",
    "**Dataset**: 497,472 daily records Â· 505 stocks Â· 2014-01-02 to 2017-12-29\n",
    "\n",
    "| # | Task | Type | Target |\n",
    "|---|------|------|--------|\n",
    "| 1 | Price Direction Classification | Binary Classification | Next-day up / down |\n",
    "| 2 | Daily Return Regression | Regression | Next-day return % |\n",
    "| 3 | Volatility Classification | Multi-class Classification | Low / Medium / High volatility |\n",
    "| 4 | Stock Clustering | Unsupervised | K-Means on stock-level features |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Â· Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Imports OK\n"
     ]
    }
   ],
   "source": [
    "import warnings, os, pathlib\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import (train_test_split, GridSearchCV,\n",
    "                                     RandomizedSearchCV, cross_val_score,\n",
    "                                     learning_curve, StratifiedKFold)\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (accuracy_score, f1_score, classification_report,\n",
    "                             confusion_matrix, mean_squared_error, r2_score,\n",
    "                             mean_absolute_error, silhouette_score)\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import (RandomForestClassifier, RandomForestRegressor,\n",
    "                              GradientBoostingClassifier, GradientBoostingRegressor,\n",
    "                              AdaBoostClassifier, VotingClassifier, StackingClassifier)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.cluster import KMeans\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import jinja2, base64\n",
    "from io import BytesIO\n",
    "\n",
    "SEED = 42\n",
    "PLOT_DIR = pathlib.Path(\"outputs/plots\")\n",
    "PLOT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"viridis\")\n",
    "print(\"\\u2705 Imports OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Â· Load Data & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw shape: (497472, 7)\n",
      "Missing values:\n",
      "symbol     0\n",
      "date       0\n",
      "open      11\n",
      "high       8\n",
      "low        8\n",
      "close      0\n",
      "volume     0\n",
      "dtype: int64\n",
      "Unique symbols: 505\n"
     ]
    }
   ],
   "source": [
    "df_raw = pd.read_csv(\"S&P 500 Stock Prices 2014-2017.csv\")\n",
    "print(f\"Raw shape: {df_raw.shape}\")\n",
    "print(f\"Missing values:\\n{df_raw.isnull().sum()}\")\n",
    "print(f\"Unique symbols: {df_raw['symbol'].nunique()}\")\n",
    "\n",
    "df = df_raw.copy()\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df = df.sort_values([\"symbol\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "# Fill small number of missing open/high/low with close\n",
    "df[\"open\"]  = df[\"open\"].fillna(df[\"close\"])\n",
    "df[\"high\"]  = df[\"high\"].fillna(df[\"close\"])\n",
    "df[\"low\"]   = df[\"low\"].fillna(df[\"close\"])\n",
    "\n",
    "# Technical indicators per stock\n",
    "df[\"daily_return\"]    = df.groupby(\"symbol\")[\"close\"].pct_change()\n",
    "df[\"daily_range\"]     = (df[\"high\"] - df[\"low\"]) / df[\"close\"]\n",
    "df[\"open_close_pct\"]  = (df[\"close\"] - df[\"open\"]) / df[\"open\"]\n",
    "df[\"gap\"]             = df.groupby(\"symbol\").apply(\n",
    "    lambda g: g[\"open\"] / g[\"close\"].shift(1) - 1, include_groups=False\n",
    ").reset_index(level=0, drop=True)\n",
    "\n",
    "# Moving averages & momentum\n",
    "for w in [5, 10, 20]:\n",
    "    df[f\"sma_{w}\"]  = df.groupby(\"symbol\")[\"close\"].transform(lambda x: x.rolling(w).mean())\n",
    "    df[f\"vol_{w}\"]  = df.groupby(\"symbol\")[\"daily_return\"].transform(lambda x: x.rolling(w).std())\n",
    "df[\"sma_ratio_5_20\"] = df[\"sma_5\"] / df[\"sma_20\"]\n",
    "\n",
    "# Volume features\n",
    "df[\"vol_sma_5\"]  = df.groupby(\"symbol\")[\"volume\"].transform(lambda x: x.rolling(5).mean())\n",
    "df[\"vol_ratio\"]  = df[\"volume\"] / df[\"vol_sma_5\"]\n",
    "\n",
    "# RSI (14-day)\n",
    "delta = df.groupby(\"symbol\")[\"close\"].diff()\n",
    "gain  = delta.clip(lower=0)\n",
    "loss  = (-delta.clip(upper=0))\n",
    "avg_gain = df.groupby(\"symbol\")[\"close\"].transform(\n",
    "    lambda x: gain.loc[x.index].rolling(14).mean()\n",
    ")\n",
    "avg_loss = df.groupby(\"symbol\")[\"close\"].transform(\n",
    "    lambda x: loss.loc[x.index].rolling(14).mean()\n",
    ")\n",
    "rs = avg_gain / (avg_loss + 1e-9)\n",
    "df[\"rsi_14\"] = 100 - (100 / (1 + rs))\n",
    "\n",
    "# Calendar features\n",
    "df[\"dow\"]    = df[\"date\"].dt.dayofweek\n",
    "df[\"month\"]  = df[\"date\"].dt.month\n",
    "df[\"year\"]   = df[\"date\"].dt.year\n",
    "\n",
    "# Target: next-day direction (1=up, 0=down/flat)\n",
    "df[\"next_return\"] = df.groupby(\"symbol\")[\"daily_return\"].shift(-1)\n",
    "df[\"direction\"]   = (df[\"next_return\"] > 0).astype(int)\n",
    "\n",
    "# Drop warm-up rows (first 20 per symbol + last row per symbol)\n",
    "row_num = df.groupby(\"symbol\").cumcount()\n",
    "group_size = df.groupby(\"symbol\")[\"close\"].transform(\"count\")\n",
    "mask = (row_num >= 20) & (row_num < group_size - 1)\n",
    "df = df[mask].reset_index(drop=True)\n",
    "df = df.dropna()\n",
    "\n",
    "print(f\"\\nEngineered shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(f\"Direction balance: {df['direction'].value_counts().to_dict()}\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Â· Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'symbol'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m axes[\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m].set_ylabel(\u001b[33m\"\u001b[39m\u001b[33mPrice (USD)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# 3 - Top 15 stocks by average volume\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m top_vol = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msymbol\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mvolume\u001b[39m\u001b[33m\"\u001b[39m].mean().sort_values(ascending=\u001b[38;5;28;01mFalse\u001b[39;00m).head(\u001b[32m15\u001b[39m)\n\u001b[32m     18\u001b[39m top_vol.plot.barh(ax=axes[\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m], color=sns.color_palette(\u001b[33m\"\u001b[39m\u001b[33mrocket\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m15\u001b[39m))\n\u001b[32m     19\u001b[39m axes[\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m].set_title(\u001b[33m\"\u001b[39m\u001b[33mTop 15 Stocks by Avg Volume\u001b[39m\u001b[33m\"\u001b[39m); axes[\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m].set_xlabel(\u001b[33m\"\u001b[39m\u001b[33mAvg Volume\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Gandalf/Github/Titanic-Survival-Prediction-ML/.venv/lib/python3.14/site-packages/pandas/util/_decorators.py:336\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    331\u001b[39m     warnings.warn(\n\u001b[32m    332\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    333\u001b[39m         klass,\n\u001b[32m    334\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    335\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Gandalf/Github/Titanic-Survival-Prediction-ML/.venv/lib/python3.14/site-packages/pandas/core/frame.py:10817\u001b[39m, in \u001b[36mDataFrame.groupby\u001b[39m\u001b[34m(self, by, level, as_index, sort, group_keys, observed, dropna)\u001b[39m\n\u001b[32m  10814\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m  10815\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mYou have to supply one of \u001b[39m\u001b[33m'\u001b[39m\u001b[33mby\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlevel\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m> \u001b[39m\u001b[32m10817\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m  10818\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m  10819\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10820\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10821\u001b[39m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10822\u001b[39m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10823\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10824\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10825\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m  10826\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Gandalf/Github/Titanic-Survival-Prediction-ML/.venv/lib/python3.14/site-packages/pandas/core/groupby/groupby.py:1095\u001b[39m, in \u001b[36mGroupBy.__init__\u001b[39m\u001b[34m(self, obj, keys, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[39m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28mself\u001b[39m.dropna = dropna\n\u001b[32m   1094\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1095\u001b[39m     grouper, exclusions, obj = \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1096\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1097\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1098\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1099\u001b[39m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1100\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1101\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1102\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1104\u001b[39m \u001b[38;5;28mself\u001b[39m.observed = observed\n\u001b[32m   1105\u001b[39m \u001b[38;5;28mself\u001b[39m.obj = obj\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Gandalf/Github/Titanic-Survival-Prediction-ML/.venv/lib/python3.14/site-packages/pandas/core/groupby/grouper.py:901\u001b[39m, in \u001b[36mget_grouper\u001b[39m\u001b[34m(obj, key, level, sort, observed, validate, dropna)\u001b[39m\n\u001b[32m    899\u001b[39m         in_axis, level, gpr = \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    900\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m901\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[32m    902\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr.key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    903\u001b[39m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[32m    904\u001b[39m     exclusions.add(gpr.key)\n",
      "\u001b[31mKeyError\u001b[39m: 'symbol'"
     ]
    }
   ],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "# 1 - Distribution of daily returns\n",
    "df[\"daily_return\"].clip(-0.1, 0.1).hist(bins=80, ax=axes[0, 0], color=\"steelblue\",\n",
    "                                         edgecolor=\"white\", alpha=0.8)\n",
    "axes[0, 0].axvline(0, color=\"red\", linestyle=\"--\", linewidth=1.5)\n",
    "axes[0, 0].set_title(\"Daily Return Distribution\")\n",
    "axes[0, 0].set_xlabel(\"Return\"); axes[0, 0].set_ylabel(\"Count\")\n",
    "\n",
    "# 2 - Average close price per year\n",
    "yearly = df.groupby(\"year\")[\"close\"].mean()\n",
    "axes[0, 1].bar(yearly.index.astype(str), yearly.values, color=\"teal\")\n",
    "axes[0, 1].set_title(\"Average Close Price by Year\")\n",
    "axes[0, 1].set_ylabel(\"Price (USD)\")\n",
    "\n",
    "# 3 - Top 15 stocks by average volume\n",
    "top_vol = df.groupby(\"symbol\")[\"volume\"].mean().sort_values(ascending=False).head(15)\n",
    "top_vol.plot.barh(ax=axes[0, 2], color=sns.color_palette(\"rocket\", 15))\n",
    "axes[0, 2].set_title(\"Top 15 Stocks by Avg Volume\"); axes[0, 2].set_xlabel(\"Avg Volume\")\n",
    "axes[0, 2].invert_yaxis()\n",
    "\n",
    "# 4 - RSI distribution\n",
    "df[\"rsi_14\"].clip(0, 100).hist(bins=50, ax=axes[1, 0], color=\"orange\", edgecolor=\"white\")\n",
    "axes[1, 0].set_title(\"RSI-14 Distribution\"); axes[1, 0].set_xlabel(\"RSI\")\n",
    "\n",
    "# 5 - Intraday range distribution\n",
    "df[\"daily_range\"].clip(0, 0.1).hist(bins=50, ax=axes[1, 1], color=\"green\", edgecolor=\"white\")\n",
    "axes[1, 1].set_title(\"Daily Range (High-Low)/Close\"); axes[1, 1].set_xlabel(\"Range %\")\n",
    "\n",
    "# 6 - Direction balance\n",
    "df[\"direction\"].value_counts().plot.bar(ax=axes[1, 2], color=[\"#e74c3c\", \"#2ecc71\"])\n",
    "axes[1, 2].set_title(\"Next-Day Direction\")\n",
    "axes[1, 2].set_xticklabels([\"Down (0)\", \"Up (1)\"], rotation=0)\n",
    "axes[1, 2].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOT_DIR / \"eda_overview.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap\n",
    "feat_cols_plot = [\"daily_return\", \"daily_range\", \"open_close_pct\", \"gap\",\n",
    "                  \"sma_ratio_5_20\", \"vol_ratio\", \"rsi_14\", \"vol_5\", \"vol_10\", \"vol_20\",\n",
    "                  \"dow\", \"month\", \"next_return\", \"direction\"]\n",
    "fig, ax = plt.subplots(figsize=(12, 9))\n",
    "sns.heatmap(df[feat_cols_plot].corr(), annot=True, fmt=\".2f\", cmap=\"coolwarm\",\n",
    "            ax=ax, linewidths=0.5, vmin=-1, vmax=1)\n",
    "ax.set_title(\"Feature Correlation Heatmap\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOT_DIR / \"correlation_heatmap.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Top 10 gainers & losers\n",
    "total_ret = df.groupby(\"symbol\").apply(\n",
    "    lambda g: (g[\"close\"].iloc[-1] / g[\"close\"].iloc[0] - 1) * 100, include_groups=False\n",
    ").sort_values()\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "total_ret.tail(15).plot.barh(ax=ax1, color=\"green\"); ax1.set_title(\"Top 15 Gainers (2014-2017)\")\n",
    "ax1.set_xlabel(\"Total Return %\")\n",
    "total_ret.head(15).plot.barh(ax=ax2, color=\"red\"); ax2.set_title(\"Top 15 Losers (2014-2017)\")\n",
    "ax2.set_xlabel(\"Total Return %\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOT_DIR / \"gainers_losers.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"\\u2705 All EDA plots saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Â· Prepare Features for Direction Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\n",
    "    \"daily_return\", \"daily_range\", \"open_close_pct\", \"gap\",\n",
    "    \"sma_ratio_5_20\", \"vol_ratio\", \"rsi_14\",\n",
    "    \"vol_5\", \"vol_10\", \"vol_20\",\n",
    "    \"dow\", \"month\"\n",
    "]\n",
    "\n",
    "X_dir = df[feature_cols].values\n",
    "y_dir = df[\"direction\"].values\n",
    "print(f\"Direction classification: {X_dir.shape}\")\n",
    "print(f\"Up: {y_dir.sum()}, Down: {len(y_dir) - y_dir.sum()}\")\n",
    "print(f\"Up rate: {y_dir.mean():.4f}\")\n",
    "\n",
    "# Time-based split: last 20% of dates for test\n",
    "dates_sorted = df[\"date\"].sort_values().unique()\n",
    "cutoff = dates_sorted[int(len(dates_sorted) * 0.8)]\n",
    "train_mask = df[\"date\"] < cutoff\n",
    "test_mask  = df[\"date\"] >= cutoff\n",
    "print(f\"Train cutoff: {cutoff}\")\n",
    "print(f\"Train: {train_mask.sum()}, Test: {test_mask.sum()}\")\n",
    "\n",
    "X_train_d = df.loc[train_mask, feature_cols].values\n",
    "X_test_d  = df.loc[test_mask, feature_cols].values\n",
    "y_train_d = df.loc[train_mask, \"direction\"].values\n",
    "y_test_d  = df.loc[test_mask, \"direction\"].values\n",
    "\n",
    "scaler_d = StandardScaler()\n",
    "X_train_ds = scaler_d.fit_transform(X_train_d)\n",
    "X_test_ds  = scaler_d.transform(X_test_d)\n",
    "print(f\"Train scaled: {X_train_ds.shape}, Test scaled: {X_test_ds.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Â· Price Direction Classification (10 Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=SEED),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=7),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(max_depth=10, random_state=SEED),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=200, max_depth=15, random_state=SEED, n_jobs=-1),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=200, max_depth=5, random_state=SEED),\n",
    "    \"AdaBoost\": AdaBoostClassifier(n_estimators=150, random_state=SEED),\n",
    "    \"XGBoost\": XGBClassifier(n_estimators=200, max_depth=5, eval_metric=\"logloss\",\n",
    "                              random_state=SEED, n_jobs=-1),\n",
    "    \"SVM\": SVC(kernel=\"rbf\", probability=True, random_state=SEED),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=400, random_state=SEED),\n",
    "}\n",
    "\n",
    "clf_results = {}\n",
    "for name, model in classifiers.items():\n",
    "    print(f\"Training {name}...\", end=\" \")\n",
    "    model.fit(X_train_ds, y_train_d)\n",
    "    y_pred = model.predict(X_test_ds)\n",
    "    acc = accuracy_score(y_test_d, y_pred)\n",
    "    f1 = f1_score(y_test_d, y_pred, average=\"weighted\")\n",
    "    clf_results[name] = {\"accuracy\": acc, \"f1\": f1, \"model\": model, \"y_pred\": y_pred}\n",
    "    print(f\"Acc={acc:.4f}  F1={f1:.4f}\")\n",
    "\n",
    "best_clf_name = max(clf_results, key=lambda k: clf_results[k][\"f1\"])\n",
    "print(f\"\\n\\U0001f3c6 Best classifier: {best_clf_name} (F1={clf_results[best_clf_name]['f1']:.4f})\")\n",
    "\n",
    "# Bar chart\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "names = list(clf_results.keys())\n",
    "accs = [clf_results[n][\"accuracy\"] for n in names]\n",
    "f1s  = [clf_results[n][\"f1\"] for n in names]\n",
    "x = np.arange(len(names))\n",
    "ax.bar(x - 0.2, accs, 0.4, label=\"Accuracy\", color=\"steelblue\")\n",
    "ax.bar(x + 0.2, f1s,  0.4, label=\"F1 (weighted)\", color=\"coral\")\n",
    "ax.set_xticks(x); ax.set_xticklabels(names, rotation=45, ha=\"right\")\n",
    "ax.set_ylim(0, 1); ax.set_title(\"Price Direction - Model Comparison\")\n",
    "ax.legend(); plt.tight_layout()\n",
    "plt.savefig(PLOT_DIR / \"model_comparison.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"\\u2705 Model comparison saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Â· Daily Return Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_reg = df[\"next_return\"].values\n",
    "\n",
    "X_train_r = df.loc[train_mask, feature_cols].values\n",
    "X_test_r  = df.loc[test_mask, feature_cols].values\n",
    "y_train_r = df.loc[train_mask, \"next_return\"].values\n",
    "y_test_r  = df.loc[test_mask, \"next_return\"].values\n",
    "\n",
    "scaler_r = StandardScaler()\n",
    "X_train_rs = scaler_r.fit_transform(X_train_r)\n",
    "X_test_rs  = scaler_r.transform(X_test_r)\n",
    "\n",
    "regressors = {\n",
    "    \"Ridge\": Ridge(alpha=1.0),\n",
    "    \"Lasso\": Lasso(alpha=0.001, max_iter=2000),\n",
    "    \"ElasticNet\": ElasticNet(alpha=0.001, l1_ratio=0.5, max_iter=2000),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(max_depth=10, random_state=SEED),\n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=200, max_depth=10, random_state=SEED, n_jobs=-1),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(n_estimators=200, max_depth=5, random_state=SEED),\n",
    "}\n",
    "\n",
    "reg_results = {}\n",
    "for name, model in regressors.items():\n",
    "    print(f\"Training {name}...\", end=\" \")\n",
    "    model.fit(X_train_rs, y_train_r)\n",
    "    y_pred = model.predict(X_test_rs)\n",
    "    r2   = r2_score(y_test_r, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_r, y_pred))\n",
    "    mae  = mean_absolute_error(y_test_r, y_pred)\n",
    "    reg_results[name] = {\"r2\": r2, \"rmse\": rmse, \"mae\": mae, \"model\": model, \"y_pred\": y_pred}\n",
    "    print(f\"R2={r2:.6f}  RMSE={rmse:.6f}  MAE={mae:.6f}\")\n",
    "\n",
    "best_reg_name = max(reg_results, key=lambda k: reg_results[k][\"r2\"])\n",
    "print(f\"\\n\\U0001f3c6 Best regressor: {best_reg_name} (R2={reg_results[best_reg_name]['r2']:.6f})\")\n",
    "\n",
    "# Actual vs Predicted\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "for ax, (name, res) in zip(axes.flat, reg_results.items()):\n",
    "    ax.scatter(y_test_r, res[\"y_pred\"], alpha=0.05, s=2, c=\"teal\")\n",
    "    lims = [-0.08, 0.08]\n",
    "    ax.plot(lims, lims, \"r--\", linewidth=1.5)\n",
    "    ax.set_title(f\"{name}\\nR2={res['r2']:.6f}\")\n",
    "    ax.set_xlabel(\"Actual Return\"); ax.set_ylabel(\"Predicted Return\")\n",
    "    ax.set_xlim(lims); ax.set_ylim(lims)\n",
    "plt.suptitle(\"Daily Return Regression - Actual vs Predicted\", fontsize=14, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOT_DIR / \"return_regression_results.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Feature importance\n",
    "best_reg = reg_results[best_reg_name][\"model\"]\n",
    "if hasattr(best_reg, \"feature_importances_\"):\n",
    "    imp = best_reg.feature_importances_\n",
    "    idx = np.argsort(imp)\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    ax.barh(np.array(feature_cols)[idx], imp[idx], color=\"darkcyan\")\n",
    "    ax.set_title(f\"Return Regression - Feature Importance ({best_reg_name})\")\n",
    "    ax.set_xlabel(\"Importance\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PLOT_DIR / \"return_feature_importance.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "print(\"\\u2705 Return regression plots saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Â· Volatility Classification (Low / Medium / High)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build stock-level features for volatility classification\n",
    "stock_feats = df.groupby(\"symbol\").agg(\n",
    "    mean_return=(\"daily_return\", \"mean\"),\n",
    "    std_return=(\"daily_return\", \"std\"),\n",
    "    mean_range=(\"daily_range\", \"mean\"),\n",
    "    mean_volume=(\"volume\", \"mean\"),\n",
    "    mean_close=(\"close\", \"mean\"),\n",
    "    mean_rsi=(\"rsi_14\", \"mean\"),\n",
    "    total_return=(\"daily_return\", \"sum\"),\n",
    "    mean_gap=(\"gap\", \"mean\"),\n",
    "    mean_vol_ratio=(\"vol_ratio\", \"mean\"),\n",
    "    count=(\"close\", \"count\"),\n",
    ").reset_index()\n",
    "\n",
    "# Classify volatility into 3 classes by std_return terciles\n",
    "stock_feats[\"vol_class\"] = pd.qcut(stock_feats[\"std_return\"], q=3,\n",
    "                                    labels=[\"Low\", \"Medium\", \"High\"])\n",
    "print(f\"Volatility class distribution:\\n{stock_feats['vol_class'].value_counts().to_string()}\")\n",
    "\n",
    "vol_features = [\"mean_return\", \"mean_range\", \"mean_volume\", \"mean_close\",\n",
    "                \"mean_rsi\", \"total_return\", \"mean_gap\", \"mean_vol_ratio\"]\n",
    "\n",
    "X_vol = stock_feats[vol_features].values\n",
    "le_vol = LabelEncoder()\n",
    "y_vol = le_vol.fit_transform(stock_feats[\"vol_class\"])\n",
    "print(f\"\\nClasses: {le_vol.classes_}\")\n",
    "print(f\"Shape: {X_vol.shape}\")\n",
    "\n",
    "X_train_v, X_test_v, y_train_v, y_test_v = train_test_split(\n",
    "    X_vol, y_vol, test_size=0.2, random_state=SEED, stratify=y_vol\n",
    ")\n",
    "scaler_v = StandardScaler()\n",
    "X_train_vs = scaler_v.fit_transform(X_train_v)\n",
    "X_test_vs  = scaler_v.transform(X_test_v)\n",
    "\n",
    "vol_models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=SEED),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=200, random_state=SEED, n_jobs=-1),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=200, random_state=SEED),\n",
    "    \"XGBoost\": XGBClassifier(n_estimators=200, eval_metric=\"mlogloss\", random_state=SEED, n_jobs=-1),\n",
    "    \"SVM\": SVC(kernel=\"rbf\", probability=True, random_state=SEED),\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=400, random_state=SEED),\n",
    "}\n",
    "\n",
    "vol_results = {}\n",
    "for name, model in vol_models.items():\n",
    "    print(f\"Training {name}...\", end=\" \")\n",
    "    model.fit(X_train_vs, y_train_v)\n",
    "    y_pred = model.predict(X_test_vs)\n",
    "    acc = accuracy_score(y_test_v, y_pred)\n",
    "    f1  = f1_score(y_test_v, y_pred, average=\"weighted\")\n",
    "    vol_results[name] = {\"accuracy\": acc, \"f1\": f1, \"model\": model, \"y_pred\": y_pred}\n",
    "    print(f\"Acc={acc:.4f}  F1={f1:.4f}\")\n",
    "\n",
    "best_vol_name = max(vol_results, key=lambda k: vol_results[k][\"f1\"])\n",
    "print(f\"\\n\\U0001f3c6 Best volatility classifier: {best_vol_name} (F1={vol_results[best_vol_name]['f1']:.4f})\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "names_v = list(vol_results.keys())\n",
    "f1s_v   = [vol_results[n][\"f1\"] for n in names_v]\n",
    "ax.barh(names_v, f1s_v, color=sns.color_palette(\"rocket\", len(names_v)))\n",
    "ax.set_title(\"Volatility Classification - F1 Scores\"); ax.set_xlabel(\"F1 (weighted)\")\n",
    "ax.set_xlim(0, 1)\n",
    "for i, v in enumerate(f1s_v):\n",
    "    ax.text(v + 0.01, i, f\"{v:.4f}\", va=\"center\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOT_DIR / \"volatility_comparison.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"\\u2705 Volatility classification plot saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 Â· Stock Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_features = [\"mean_return\", \"std_return\", \"mean_range\", \"mean_volume\",\n",
    "                  \"mean_close\", \"mean_rsi\", \"total_return\", \"mean_gap\", \"mean_vol_ratio\"]\n",
    "X_clust = stock_feats[clust_features].values\n",
    "scaler_cl = StandardScaler()\n",
    "X_clust_s = scaler_cl.fit_transform(X_clust)\n",
    "\n",
    "K_range = range(2, 11)\n",
    "inertias, sils = [], []\n",
    "for k in K_range:\n",
    "    km = KMeans(n_clusters=k, random_state=SEED, n_init=10)\n",
    "    labels = km.fit_predict(X_clust_s)\n",
    "    inertias.append(km.inertia_)\n",
    "    sils.append(silhouette_score(X_clust_s, labels))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "ax1.plot(list(K_range), inertias, \"bo-\"); ax1.set_title(\"Elbow Method\")\n",
    "ax1.set_xlabel(\"k\"); ax1.set_ylabel(\"Inertia\")\n",
    "ax2.plot(list(K_range), sils, \"rs-\"); ax2.set_title(\"Silhouette Score\")\n",
    "ax2.set_xlabel(\"k\"); ax2.set_ylabel(\"Score\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOT_DIR / \"elbow_silhouette.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "best_k = list(K_range)[np.argmax(sils)]\n",
    "print(f\"Best k={best_k}, silhouette={max(sils):.4f}\")\n",
    "\n",
    "km_final = KMeans(n_clusters=best_k, random_state=SEED, n_init=10)\n",
    "stock_feats[\"cluster\"] = km_final.fit_predict(X_clust_s)\n",
    "\n",
    "# Cluster profiles\n",
    "cluster_profile = stock_feats.groupby(\"cluster\")[clust_features].mean()\n",
    "print(\"\\nCluster profiles:\")\n",
    "print(cluster_profile.round(4).to_string())\n",
    "\n",
    "# Cluster sizes\n",
    "print(f\"\\nCluster sizes: {stock_feats['cluster'].value_counts().sort_index().to_dict()}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "cp_norm = (cluster_profile - cluster_profile.min()) / (cluster_profile.max() - cluster_profile.min() + 1e-9)\n",
    "cp_norm.T.plot(kind=\"bar\", ax=ax, colormap=\"viridis\")\n",
    "ax.set_title(f\"Stock Cluster Profiles (k={best_k})\")\n",
    "ax.set_ylabel(\"Normalized value\"); ax.set_xlabel(\"Feature\")\n",
    "ax.legend(title=\"Cluster\", bbox_to_anchor=(1.05, 1)); ax.tick_params(axis=\"x\", rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOT_DIR / \"clustering_results.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Show sample stocks per cluster\n",
    "for c in sorted(stock_feats[\"cluster\"].unique()):\n",
    "    symbols = stock_feats[stock_feats[\"cluster\"] == c][\"symbol\"].tolist()\n",
    "    print(f\"Cluster {c} ({len(symbols)} stocks): {symbols[:8]}{'...' if len(symbols) > 8 else ''}\")\n",
    "print(\"\\u2705 Clustering plots saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 Â· Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a subsample for tuning (direction classification is large)\n",
    "np.random.seed(SEED)\n",
    "sub_idx = np.random.choice(len(X_train_ds), size=min(50000, len(X_train_ds)), replace=False)\n",
    "X_sub = X_train_ds[sub_idx]\n",
    "y_sub = y_train_d[sub_idx]\n",
    "\n",
    "# GridSearchCV - Random Forest\n",
    "print(\"GridSearchCV on Random Forest (subsample)...\")\n",
    "rf_grid = {\n",
    "    \"n_estimators\": [100, 200],\n",
    "    \"max_depth\": [5, 10, 15],\n",
    "    \"min_samples_split\": [5, 10],\n",
    "}\n",
    "gs_rf = GridSearchCV(RandomForestClassifier(random_state=SEED, n_jobs=-1),\n",
    "                     rf_grid, cv=3, scoring=\"f1_weighted\", n_jobs=-1)\n",
    "gs_rf.fit(X_sub, y_sub)\n",
    "print(f\"  Best params: {gs_rf.best_params_}\")\n",
    "print(f\"  Best CV F1:  {gs_rf.best_score_:.4f}\")\n",
    "\n",
    "# RandomizedSearchCV - Gradient Boosting\n",
    "print(\"\\nRandomizedSearchCV on Gradient Boosting (subsample)...\")\n",
    "gb_dist = {\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"max_depth\": [3, 5, 7],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"subsample\": [0.7, 0.8, 1.0],\n",
    "}\n",
    "rs_gb = RandomizedSearchCV(GradientBoostingClassifier(random_state=SEED),\n",
    "                           gb_dist, n_iter=15, cv=3, scoring=\"f1_weighted\",\n",
    "                           random_state=SEED, n_jobs=-1)\n",
    "rs_gb.fit(X_sub, y_sub)\n",
    "print(f\"  Best params: {rs_gb.best_params_}\")\n",
    "print(f\"  Best CV F1:  {rs_gb.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate tuned models on full test set\n",
    "for label, model in [(\"Tuned RF (Grid)\", gs_rf.best_estimator_),\n",
    "                     (\"Tuned GB (Random)\", rs_gb.best_estimator_)]:\n",
    "    model.fit(X_train_ds, y_train_d)\n",
    "    y_pred = model.predict(X_test_ds)\n",
    "    acc = accuracy_score(y_test_d, y_pred)\n",
    "    f1  = f1_score(y_test_d, y_pred, average=\"weighted\")\n",
    "    clf_results[label] = {\"accuracy\": acc, \"f1\": f1, \"model\": model, \"y_pred\": y_pred}\n",
    "    print(f\"  {label}: Acc={acc:.4f}  F1={f1:.4f}\")\n",
    "\n",
    "print(\"\\n\\u2705 Hyperparameter tuning complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 Â· Cross-Validation, Feature Importance, Confusion Matrices & Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold CV (on subsample for speed)\n",
    "cv_models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=SEED),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, max_depth=10, random_state=SEED, n_jobs=-1),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(n_estimators=100, max_depth=5, random_state=SEED),\n",
    "    \"XGBoost\": XGBClassifier(n_estimators=100, max_depth=5, eval_metric=\"logloss\",\n",
    "                              random_state=SEED, n_jobs=-1),\n",
    "}\n",
    "\n",
    "# Subsample for CV\n",
    "np.random.seed(SEED)\n",
    "cv_idx = np.random.choice(len(X_dir), size=min(60000, len(X_dir)), replace=False)\n",
    "X_cv_sub = scaler_d.transform(X_dir[cv_idx])\n",
    "y_cv_sub = y_dir[cv_idx]\n",
    "\n",
    "cv_scores = {}\n",
    "for name, model in cv_models.items():\n",
    "    scores = cross_val_score(model, X_cv_sub, y_cv_sub, cv=5, scoring=\"f1_weighted\", n_jobs=-1)\n",
    "    cv_scores[name] = scores\n",
    "    print(f\"{name}: mean F1={scores.mean():.4f} +/- {scores.std():.4f}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.boxplot(cv_scores.values(), labels=cv_scores.keys())\n",
    "ax.set_title(\"5-Fold Cross-Validation F1 Scores (Direction)\")\n",
    "ax.set_ylabel(\"F1 (weighted)\"); ax.tick_params(axis=\"x\", rotation=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOT_DIR / \"cv_comparison.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Feature importance\n",
    "best_clf_model = clf_results[best_clf_name][\"model\"]\n",
    "if hasattr(best_clf_model, \"feature_importances_\"):\n",
    "    imp = best_clf_model.feature_importances_\n",
    "    idx = np.argsort(imp)\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    ax.barh(np.array(feature_cols)[idx], imp[idx], color=\"coral\")\n",
    "    ax.set_title(f\"Feature Importance - {best_clf_name}\")\n",
    "    ax.set_xlabel(\"Importance\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PLOT_DIR / \"feature_importance.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "# Confusion matrices (top 4)\n",
    "top4 = sorted(clf_results, key=lambda k: clf_results[k][\"f1\"], reverse=True)[:4]\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "for ax, name in zip(axes, top4):\n",
    "    cm = confusion_matrix(y_test_d, clf_results[name][\"y_pred\"])\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax,\n",
    "                xticklabels=[\"Down\", \"Up\"], yticklabels=[\"Down\", \"Up\"])\n",
    "    ax.set_title(f\"{name}\\nF1={clf_results[name]['f1']:.4f}\")\n",
    "    ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"Actual\")\n",
    "plt.suptitle(\"Confusion Matrices - Top 4 Models\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOT_DIR / \"confusion_matrices.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Learning curves (on subsample)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "for ax, (name, model) in zip(axes, [\n",
    "    (\"Random Forest\", RandomForestClassifier(n_estimators=50, max_depth=10, random_state=SEED, n_jobs=-1)),\n",
    "    (\"Gradient Boosting\", GradientBoostingClassifier(n_estimators=50, max_depth=5, random_state=SEED)),\n",
    "]):\n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        model, X_cv_sub, y_cv_sub, cv=3, n_jobs=-1,\n",
    "        train_sizes=np.linspace(0.1, 1.0, 6), scoring=\"f1_weighted\"\n",
    "    )\n",
    "    ax.plot(train_sizes, train_scores.mean(axis=1), \"o-\", label=\"Train\")\n",
    "    ax.plot(train_sizes, val_scores.mean(axis=1), \"s-\", label=\"Validation\")\n",
    "    ax.set_title(f\"Learning Curve - {name}\")\n",
    "    ax.set_xlabel(\"Training Size\"); ax.set_ylabel(\"F1 (weighted)\")\n",
    "    ax.legend(); ax.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOT_DIR / \"learning_curves.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"\\u2705 CV, feature importance, confusion matrices & learning curves saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11 Â· Voting & Stacking Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voting Classifier\n",
    "print(\"Training Voting Classifier...\")\n",
    "voting = VotingClassifier(\n",
    "    estimators=[\n",
    "        (\"rf\", RandomForestClassifier(n_estimators=200, max_depth=15, random_state=SEED, n_jobs=-1)),\n",
    "        (\"gb\", GradientBoostingClassifier(n_estimators=200, max_depth=5, random_state=SEED)),\n",
    "        (\"xgb\", XGBClassifier(n_estimators=200, max_depth=5, eval_metric=\"logloss\",\n",
    "                               random_state=SEED, n_jobs=-1)),\n",
    "    ],\n",
    "    voting=\"soft\"\n",
    ")\n",
    "voting.fit(X_train_ds, y_train_d)\n",
    "y_pred_v = voting.predict(X_test_ds)\n",
    "acc_v = accuracy_score(y_test_d, y_pred_v)\n",
    "f1_v  = f1_score(y_test_d, y_pred_v, average=\"weighted\")\n",
    "clf_results[\"Voting Ensemble\"] = {\"accuracy\": acc_v, \"f1\": f1_v, \"model\": voting, \"y_pred\": y_pred_v}\n",
    "print(f\"  Voting: Acc={acc_v:.4f}  F1={f1_v:.4f}\")\n",
    "\n",
    "# Stacking Classifier\n",
    "print(\"Training Stacking Classifier...\")\n",
    "stacking = StackingClassifier(\n",
    "    estimators=[\n",
    "        (\"rf\", RandomForestClassifier(n_estimators=100, max_depth=10, random_state=SEED, n_jobs=-1)),\n",
    "        (\"gb\", GradientBoostingClassifier(n_estimators=100, max_depth=5, random_state=SEED)),\n",
    "        (\"xgb\", XGBClassifier(n_estimators=100, max_depth=5, eval_metric=\"logloss\",\n",
    "                               random_state=SEED, n_jobs=-1)),\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(max_iter=1000, random_state=SEED),\n",
    "    cv=3, n_jobs=-1\n",
    ")\n",
    "stacking.fit(X_train_ds, y_train_d)\n",
    "y_pred_s = stacking.predict(X_test_ds)\n",
    "acc_s = accuracy_score(y_test_d, y_pred_s)\n",
    "f1_s  = f1_score(y_test_d, y_pred_s, average=\"weighted\")\n",
    "clf_results[\"Stacking Ensemble\"] = {\"accuracy\": acc_s, \"f1\": f1_s, \"model\": stacking, \"y_pred\": y_pred_s}\n",
    "print(f\"  Stacking: Acc={acc_s:.4f}  F1={f1_s:.4f}\")\n",
    "\n",
    "# Final ranking\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL MODEL RANKING - Price Direction Classification\")\n",
    "print(\"=\"*60)\n",
    "ranking = sorted(clf_results.items(), key=lambda x: x[1][\"f1\"], reverse=True)\n",
    "for i, (name, res) in enumerate(ranking, 1):\n",
    "    print(f\"  {i:>2}. {name:<25s} Acc={res['accuracy']:.4f}  F1={res['f1']:.4f}\")\n",
    "best_overall = ranking[0][0]\n",
    "print(f\"\\n\\U0001f3c6 Best overall: {best_overall} (F1={clf_results[best_overall]['f1']:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VOLATILITY CLASSIFICATION RANKING\")\n",
    "print(\"=\"*60)\n",
    "v_ranking = sorted(vol_results.items(), key=lambda x: x[1][\"f1\"], reverse=True)\n",
    "for i, (name, res) in enumerate(v_ranking, 1):\n",
    "    print(f\"  {i:>2}. {name:<25s} Acc={res['accuracy']:.4f}  F1={res['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12 Â· Generate HTML Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_base64(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return base64.b64encode(f.read()).decode()\n",
    "\n",
    "images = {}\n",
    "for p in sorted(PLOT_DIR.glob(\"*.png\")):\n",
    "    images[p.stem] = img_to_base64(p)\n",
    "\n",
    "TEMPLATE = \"\"\"<!DOCTYPE html>\n",
    "<html lang=\"en\"><head><meta charset=\"UTF-8\">\n",
    "<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\">\n",
    "<title>S&P 500 Stocks - ML Report</title>\n",
    "<style>\n",
    ":root{--bg:#0f172a;--card:#1e293b;--accent:#10b981;--text:#e2e8f0;--muted:#94a3b8}\n",
    "*{margin:0;padding:0;box-sizing:border-box}\n",
    "body{background:var(--bg);color:var(--text);font-family:'Segoe UI',system-ui,sans-serif;padding:2rem}\n",
    "h1{text-align:center;font-size:2.2rem;margin-bottom:.4rem;color:var(--accent)}\n",
    ".subtitle{text-align:center;color:var(--muted);margin-bottom:2rem}\n",
    ".card{background:var(--card);border-radius:12px;padding:1.5rem;margin-bottom:1.5rem;box-shadow:0 4px 24px #0004}\n",
    ".card h2{color:var(--accent);margin-bottom:1rem;font-size:1.3rem}\n",
    "table{width:100%;border-collapse:collapse;margin:1rem 0}\n",
    "th,td{padding:.55rem .8rem;text-align:left;border-bottom:1px solid #334155}\n",
    "th{color:var(--accent);font-size:.85rem;text-transform:uppercase}\n",
    "tr:hover{background:#ffffff08}\n",
    ".best{background:#10b98115;font-weight:700}\n",
    "img{width:100%;border-radius:8px;margin:.8rem 0}\n",
    ".grid2{display:grid;grid-template-columns:1fr 1fr;gap:1.2rem}\n",
    "@media(max-width:800px){.grid2{grid-template-columns:1fr}}\n",
    "</style></head><body>\n",
    "<h1>S&P 500 Stock Prices - ML Report</h1>\n",
    "<p class=\"subtitle\">497,472 records - 505 stocks - 2014 to 2017</p>\n",
    "\n",
    "<div class=\"card\"><h2>Exploratory Data Analysis</h2>\n",
    "<img src=\"data:image/png;base64,{{images.eda_overview}}\" alt=\"EDA Overview\">\n",
    "<div class=\"grid2\">\n",
    "<img src=\"data:image/png;base64,{{images.correlation_heatmap}}\" alt=\"Correlation Heatmap\">\n",
    "<img src=\"data:image/png;base64,{{images.gainers_losers}}\" alt=\"Gainers and Losers\">\n",
    "</div></div>\n",
    "\n",
    "<div class=\"card\"><h2>Task 1 - Price Direction Classification</h2>\n",
    "<table><tr><th>#</th><th>Model</th><th>Accuracy</th><th>F1 (weighted)</th></tr>\n",
    "{% for name, res in clf_ranking %}\n",
    "<tr{% if loop.first %} class=\"best\"{% endif %}>\n",
    "<td>{{loop.index}}</td><td>{{name}}</td>\n",
    "<td>{{\"{:.4f}\".format(res.accuracy)}}</td><td>{{\"{:.4f}\".format(res.f1)}}</td></tr>\n",
    "{% endfor %}</table>\n",
    "<img src=\"data:image/png;base64,{{images.model_comparison}}\" alt=\"Model Comparison\">\n",
    "</div>\n",
    "\n",
    "<div class=\"card\"><h2>Task 2 - Daily Return Regression</h2>\n",
    "<table><tr><th>#</th><th>Model</th><th>R2</th><th>RMSE</th><th>MAE</th></tr>\n",
    "{% for name, res in reg_ranking %}\n",
    "<tr{% if loop.first %} class=\"best\"{% endif %}>\n",
    "<td>{{loop.index}}</td><td>{{name}}</td>\n",
    "<td>{{\"{:.6f}\".format(res.r2)}}</td><td>{{\"{:.6f}\".format(res.rmse)}}</td>\n",
    "<td>{{\"{:.6f}\".format(res.mae)}}</td></tr>\n",
    "{% endfor %}</table>\n",
    "<div class=\"grid2\">\n",
    "{% if images.return_feature_importance %}\n",
    "<img src=\"data:image/png;base64,{{images.return_feature_importance}}\" alt=\"Return Feature Importance\">\n",
    "{% endif %}\n",
    "<img src=\"data:image/png;base64,{{images.return_regression_results}}\" alt=\"Return Regression\">\n",
    "</div></div>\n",
    "\n",
    "<div class=\"card\"><h2>Task 3 - Volatility Classification (Low / Medium / High)</h2>\n",
    "<table><tr><th>#</th><th>Model</th><th>Accuracy</th><th>F1 (weighted)</th></tr>\n",
    "{% for name, res in vol_ranking %}\n",
    "<tr{% if loop.first %} class=\"best\"{% endif %}>\n",
    "<td>{{loop.index}}</td><td>{{name}}</td>\n",
    "<td>{{\"{:.4f}\".format(res.accuracy)}}</td><td>{{\"{:.4f}\".format(res.f1)}}</td></tr>\n",
    "{% endfor %}</table>\n",
    "<img src=\"data:image/png;base64,{{images.volatility_comparison}}\" alt=\"Volatility Comparison\">\n",
    "</div>\n",
    "\n",
    "<div class=\"card\"><h2>Task 4 - Stock Clustering</h2>\n",
    "<p>Best k={{best_k}}, Silhouette={{\"{:.4f}\".format(best_sil)}}</p>\n",
    "<div class=\"grid2\">\n",
    "<img src=\"data:image/png;base64,{{images.elbow_silhouette}}\" alt=\"Elbow and Silhouette\">\n",
    "<img src=\"data:image/png;base64,{{images.clustering_results}}\" alt=\"Clustering Results\">\n",
    "</div></div>\n",
    "\n",
    "<div class=\"card\"><h2>Hyperparameter Tuning & Cross-Validation</h2>\n",
    "<div class=\"grid2\">\n",
    "<img src=\"data:image/png;base64,{{images.cv_comparison}}\" alt=\"CV Comparison\">\n",
    "{% if images.feature_importance %}\n",
    "<img src=\"data:image/png;base64,{{images.feature_importance}}\" alt=\"Feature Importance\">\n",
    "{% endif %}\n",
    "</div>\n",
    "<img src=\"data:image/png;base64,{{images.confusion_matrices}}\" alt=\"Confusion Matrices\">\n",
    "<img src=\"data:image/png;base64,{{images.learning_curves}}\" alt=\"Learning Curves\">\n",
    "</div>\n",
    "\n",
    "</body></html>\"\"\"\n",
    "\n",
    "from types import SimpleNamespace\n",
    "clf_ranking = [(n, SimpleNamespace(**{k: v for k, v in r.items() if k not in (\"model\", \"y_pred\")}))\n",
    "               for n, r in sorted(clf_results.items(), key=lambda x: x[1][\"f1\"], reverse=True)]\n",
    "reg_ranking = [(n, SimpleNamespace(**{k: v for k, v in r.items() if k not in (\"model\", \"y_pred\")}))\n",
    "               for n, r in sorted(reg_results.items(), key=lambda x: x[1][\"r2\"], reverse=True)]\n",
    "vol_ranking = [(n, SimpleNamespace(**{k: v for k, v in r.items() if k not in (\"model\", \"y_pred\")}))\n",
    "               for n, r in sorted(vol_results.items(), key=lambda x: x[1][\"f1\"], reverse=True)]\n",
    "\n",
    "html = jinja2.Template(TEMPLATE).render(\n",
    "    images=images,\n",
    "    clf_ranking=clf_ranking,\n",
    "    reg_ranking=reg_ranking,\n",
    "    vol_ranking=vol_ranking,\n",
    "    best_k=best_k, best_sil=max(sils),\n",
    ")\n",
    "\n",
    "out_path = pathlib.Path(\"outputs/stocks_ml_report.html\")\n",
    "out_path.write_text(html)\n",
    "print(f\"Report generated: {out_path}\")\n",
    "print(f\"   File size: {out_path.stat().st_size / 1024:.1f} KB\")\n",
    "print(f\"   Embedded images: {len(images)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
