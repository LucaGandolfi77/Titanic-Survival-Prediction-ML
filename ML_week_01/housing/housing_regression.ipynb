{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "903f3245",
   "metadata": {},
   "source": [
    "# ðŸ  Iowa Housing â€” Regression Pipeline\n",
    "\n",
    "**Goal:** Predict the sale price of houses in Ames, Iowa using 80+ features.  \n",
    "**Dataset:** [Kaggle House Prices](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) â€” 1460 training samples.  \n",
    "**Approach:** Full ML pipeline â†’ EDA â†’ Feature Engineering â†’ Model Comparison â†’ Tuning â†’ Interpretation â†’ Submission."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d288c27",
   "metadata": {},
   "source": [
    "---\n",
    "## 0. Configuration & Imports\n",
    "\n",
    "Centralising every constant at the top makes the notebook reproducible and easy to tweak.  \n",
    "Creating output directories up front prevents file-not-found errors when saving plots later.  \n",
    "Importing everything here keeps the rest of the notebook clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10e41e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Constants â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "SEED       = 42\n",
    "TEST_SIZE  = 0.2\n",
    "CV_FOLDS   = 5\n",
    "TARGET_COL = \"SalePrice\"\n",
    "\n",
    "DATA_DIR    = Path(\"data\")\n",
    "OUTPUT_DIR  = Path(\"outputs\")\n",
    "FIGURES_DIR = OUTPUT_DIR / \"figures\"\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# â”€â”€ Libraries â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, cross_val_score, KFold, GridSearchCV,\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# â”€â”€ Reproducibility & style â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "np.random.seed(SEED)\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "plt.rcParams.update({\"figure.figsize\": (10, 6), \"axes.titlesize\": 14, \"axes.labelsize\": 12})\n",
    "\n",
    "print(\"Setup complete âœ…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b701fb5e",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Load Data & First Inspection\n",
    "\n",
    "Loading the raw data and immediately checking shape, types, and summary statistics gives us a mental model of the dataset.  \n",
    "Separating numeric vs categorical columns up front is essential because they require completely different processing pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff9a429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Load datasets â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df_train = pd.read_csv(DATA_DIR / \"train.csv\")\n",
    "df_test  = pd.read_csv(DATA_DIR / \"test.csv\")\n",
    "\n",
    "# Preserve test IDs for submission\n",
    "test_ids = df_test[\"Id\"].copy()\n",
    "\n",
    "print(f\"Train: {df_train.shape}  |  Test: {df_test.shape}\")\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3152db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Data types overview â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(df_train.dtypes.value_counts())\n",
    "print(\"\\n\")\n",
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203f9a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Separate feature types â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# MSSubClass is numeric in dtype but categorical in meaning\n",
    "df_train[\"MSSubClass\"] = df_train[\"MSSubClass\"].astype(str)\n",
    "df_test[\"MSSubClass\"]  = df_test[\"MSSubClass\"].astype(str)\n",
    "\n",
    "NUMERIC_COLS     = df_train.select_dtypes(include=np.number).columns.drop([\"Id\", TARGET_COL]).tolist()\n",
    "CATEGORICAL_COLS = df_train.select_dtypes(include=\"object\").columns.tolist()\n",
    "\n",
    "print(f\"Numeric features:     {len(NUMERIC_COLS)}\")\n",
    "print(f\"Categorical features: {len(CATEGORICAL_COLS)}\")\n",
    "print(f\"Target range: ${df_train[TARGET_COL].min():,.0f} â€“ ${df_train[TARGET_COL].max():,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b09e2b",
   "metadata": {},
   "source": [
    "### ðŸ“Œ Summary\n",
    "- **36 numeric** and **44 categorical** features (after casting `MSSubClass` to string).\n",
    "- Target `SalePrice` ranges from ~\\$35 k to ~\\$755 k."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba314bf",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Target Variable Analysis\n",
    "\n",
    "Many regression metrics (including RMSE on Kaggle) are evaluated on the **log** of the target.  \n",
    "A log-transform makes the right-skewed SalePrice distribution closer to Normal, which satisfies the assumptions of linear models and stabilises variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a7d502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Plot 1: Raw SalePrice distribution â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# 1a. Raw histogram + KDE\n",
    "sns.histplot(df_train[TARGET_COL], kde=True, ax=axes[0], color=\"steelblue\")\n",
    "axes[0].set_title(\"SalePrice Distribution (Raw)\")\n",
    "axes[0].set_xlabel(\"SalePrice ($)\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "axes[0].axvline(df_train[TARGET_COL].mean(), color=\"red\", ls=\"--\", label=f\"Mean: ${df_train[TARGET_COL].mean():,.0f}\")\n",
    "axes[0].legend()\n",
    "\n",
    "# 1b. Log-transformed histogram + KDE\n",
    "log_price = np.log1p(df_train[TARGET_COL])\n",
    "sns.histplot(log_price, kde=True, ax=axes[1], color=\"#2ecc71\")\n",
    "axes[1].set_title(\"log(1 + SalePrice) Distribution\")\n",
    "axes[1].set_xlabel(\"log(1 + SalePrice)\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "\n",
    "# 1c. QQ-plot\n",
    "stats.probplot(log_price, dist=\"norm\", plot=axes[2])\n",
    "axes[2].set_title(\"QQ-Plot: log(SalePrice) vs Normal\")\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / \"target_distribution.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Skewness (raw):  {df_train[TARGET_COL].skew():.3f}\")\n",
    "print(f\"Skewness (log):  {log_price.skew():.3f}\")\n",
    "print(f\"Kurtosis (log):  {log_price.kurtosis():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56ed7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Apply log1p transform to the target â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "y_train_log = np.log1p(df_train[TARGET_COL])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecf84c4",
   "metadata": {},
   "source": [
    "- Raw `SalePrice` has **positive skew â‰ˆ 1.88** â€” the right tail of expensive houses pulls the mean above the median.\n",
    "- After `log1p`, skewness drops to **â‰ˆ 0.12** and the QQ-plot is nearly linear â†’ safe for linear models.\n",
    "- We will predict `log(1+SalePrice)` during training and back-transform with `expm1` at submission."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492a431c",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Missing Data Analysis & Strategy\n",
    "\n",
    "The Iowa Housing dataset uses `NaN` for both *truly missing* data and *absence of a feature* (e.g., \"no pool\").  \n",
    "Domain-aware imputation is critical: filling `PoolQC = NaN` with \"None\" (no pool) is semantically correct, whereas using the mode would be wrong.  \n",
    "We must handle both sets identically to avoid trainâ€“test mismatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305dd60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Missing-value summary for both sets â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def missing_summary(df: pd.DataFrame, name: str) -> pd.DataFrame:\n",
    "    \"\"\"Return a DataFrame of columns with missing values and their percentage.\"\"\"\n",
    "    total = df.isnull().sum()\n",
    "    pct   = (total / len(df) * 100).round(1)\n",
    "    out   = pd.DataFrame({\"count\": total, \"pct\": pct}).query(\"count > 0\").sort_values(\"pct\", ascending=False)\n",
    "    out.columns = [f\"{name}_count\", f\"{name}_pct\"]\n",
    "    return out\n",
    "\n",
    "miss_train = missing_summary(df_train, \"train\")\n",
    "miss_test  = missing_summary(df_test,  \"test\")\n",
    "miss_all   = miss_train.join(miss_test, how=\"outer\").fillna(0)\n",
    "print(f\"Columns with missing values â€” train: {len(miss_train)}, test: {len(miss_test)}\")\n",
    "miss_all.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5e69f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Plot: Top 30 columns by missing % (train) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "top30 = miss_train.head(30)\n",
    "ax.barh(top30.index, top30[\"train_pct\"], color=\"salmon\")\n",
    "ax.set_xlabel(\"Missing (%)\")\n",
    "ax.set_ylabel(\"Feature\")\n",
    "ax.set_title(\"Top 30 Features by Missing Percentage (Train)\")\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / \"missing_values.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115dba24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Domain-aware imputation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# These columns use NaN to mean \"feature absent\", not \"unknown\"\n",
    "NONE_FILL_COLS = [\n",
    "    \"PoolQC\", \"MiscFeature\", \"Alley\", \"Fence\", \"FireplaceQu\",\n",
    "    \"GarageType\", \"GarageFinish\", \"GarageQual\", \"GarageCond\",\n",
    "    \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\",\n",
    "    \"MasVnrType\",\n",
    "]\n",
    "\n",
    "# Numeric cols where NaN means zero\n",
    "ZERO_FILL_COLS = [\n",
    "    \"GarageYrBlt\", \"GarageArea\", \"GarageCars\",\n",
    "    \"BsmtFinSF1\", \"BsmtFinSF2\", \"BsmtUnfSF\", \"TotalBsmtSF\",\n",
    "    \"BsmtFullBath\", \"BsmtHalfBath\", \"MasVnrArea\",\n",
    "]\n",
    "\n",
    "# Columns to drop (>40 % missing)\n",
    "HIGH_MISSING = [c for c in miss_train.index if miss_train.loc[c, \"train_pct\"] > 40]\n",
    "print(f\"Dropping (>40 % missing): {HIGH_MISSING}\")\n",
    "\n",
    "\n",
    "def impute_missing(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Apply domain-aware imputation to all missing columns.\"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Drop high-missing columns\n",
    "    df.drop(columns=[c for c in HIGH_MISSING if c in df.columns], inplace=True, errors=\"ignore\")\n",
    "\n",
    "    # Fill \"absence\" categoricals with \"None\"\n",
    "    for col in NONE_FILL_COLS:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna(\"None\")\n",
    "\n",
    "    # Fill \"absence\" numerics with 0\n",
    "    for col in ZERO_FILL_COLS:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna(0)\n",
    "\n",
    "    # Fix GarageYrBlt typo (2207 â†’ 2007)\n",
    "    if \"GarageYrBlt\" in df.columns:\n",
    "        df[\"GarageYrBlt\"] = df[\"GarageYrBlt\"].clip(upper=2010)\n",
    "\n",
    "    # Remaining categoricals â†’ mode\n",
    "    for col in df.select_dtypes(include=\"object\").columns:\n",
    "        if df[col].isnull().any():\n",
    "            df[col] = df[col].fillna(df[col].mode()[0])\n",
    "\n",
    "    # Remaining numerics â†’ median\n",
    "    for col in df.select_dtypes(include=np.number).columns:\n",
    "        if df[col].isnull().any():\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df_train = impute_missing(df_train)\n",
    "df_test  = impute_missing(df_test)\n",
    "\n",
    "# â”€â”€ Assert no nulls remain â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "assert df_train.isnull().sum().sum() == 0, \"Train still has nulls!\"\n",
    "assert df_test.isnull().sum().sum()  == 0, \"Test still has nulls!\"\n",
    "print(f\"âœ… Zero nulls remaining â€” train: {df_train.shape}, test: {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c70edae",
   "metadata": {},
   "source": [
    "### ðŸ“Œ Before / After\n",
    "\n",
    "| Bucket | Columns | Strategy |\n",
    "|--------|---------|----------|\n",
    "| **HIGH (>40 %)** | PoolQC, MiscFeature, Alley, Fence | **Dropped** |\n",
    "| **Semantic NaN** | FireplaceQu, Garage*, Bsmt*, MasVnrType | Filled with **\"None\"** / **0** |\n",
    "| **Low missing** | LotFrontage, Electrical, etc. | Median / Mode |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb73a24",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Exploratory Data Analysis â€” 10 Charts\n",
    "\n",
    "EDA reveals which features carry the most signal for predicting SalePrice.  \n",
    "We identify outliers, confirm expected domain relationships (quality â†’ price), and spot high-cardinality categoricals.  \n",
    "Every chart is saved to `outputs/figures/` for the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bfe3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Chart 1: Top 10 numeric correlations with SalePrice â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "numeric_for_corr = [c for c in NUMERIC_COLS if c in df_train.columns] + [TARGET_COL]\n",
    "corr = df_train[numeric_for_corr].corr()\n",
    "top10 = corr[TARGET_COL].drop(TARGET_COL).abs().nlargest(10).index.tolist() + [TARGET_COL]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(df_train[top10].corr(), annot=True, fmt=\".2f\", cmap=\"RdBu_r\",\n",
    "            center=0, square=True, linewidths=0.5, ax=ax)\n",
    "ax.set_title(\"Top 10 Features Correlated with SalePrice\")\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / \"correlation_top10.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2866913d",
   "metadata": {},
   "source": [
    "- **OverallQual** (0.79) and **GrLivArea** (0.71) have the strongest positive correlations.\n",
    "- **GarageCars / GarageArea** are highly correlated with each other (~0.88) â€” we might drop one to avoid multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abdd40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Chart 2: SalePrice vs GrLivArea scatter â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.scatter(df_train[\"GrLivArea\"], df_train[TARGET_COL],\n",
    "           alpha=0.4, c=\"steelblue\", edgecolors=\"k\", linewidths=0.3, s=30)\n",
    "\n",
    "# Mark outliers: GrLivArea > 4000\n",
    "outliers = df_train[df_train[\"GrLivArea\"] > 4000]\n",
    "ax.scatter(outliers[\"GrLivArea\"], outliers[TARGET_COL],\n",
    "           c=\"red\", s=80, edgecolors=\"k\", label=f\"Outliers ({len(outliers)})\")\n",
    "ax.set_title(\"SalePrice vs GrLivArea\")\n",
    "ax.set_xlabel(\"Above-Ground Living Area (sq ft)\")\n",
    "ax.set_ylabel(\"SalePrice ($)\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / \"scatter_grliv.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a4ad48",
   "metadata": {},
   "source": [
    "- Two houses with **GrLivArea > 4500** but low SalePrice are clear outliers (likely partial sales or data errors).\n",
    "- We will **remove** these before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13b1fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Remove GrLivArea outliers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "before = len(df_train)\n",
    "df_train = df_train[df_train[\"GrLivArea\"] <= 4500].reset_index(drop=True)\n",
    "y_train_log = np.log1p(df_train[TARGET_COL])\n",
    "print(f\"Removed {before - len(df_train)} outliers â†’ {len(df_train)} rows remain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef837ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Chart 3: SalePrice vs OverallQual (violin) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "sns.violinplot(x=\"OverallQual\", y=TARGET_COL, data=df_train, ax=ax, palette=\"YlOrRd\", inner=\"quartile\")\n",
    "ax.set_title(\"SalePrice Distribution by Overall Quality\")\n",
    "ax.set_xlabel(\"Overall Quality (1â€“10)\")\n",
    "ax.set_ylabel(\"SalePrice ($)\")\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / \"violin_quality.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330d0fa9",
   "metadata": {},
   "source": [
    "- Price increases roughly **exponentially** with quality: quality 10 houses sell for 3â€“5Ã— more than quality 5.\n",
    "- Strong separation â†’ OverallQual will be a top predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33131f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Chart 4: SalePrice by Neighborhood (boxplot, sorted) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "nbh_order = df_train.groupby(\"Neighborhood\")[TARGET_COL].median().sort_values().index\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "sns.boxplot(x=\"Neighborhood\", y=TARGET_COL, data=df_train,\n",
    "            order=nbh_order, ax=ax, palette=\"viridis\")\n",
    "ax.set_title(\"SalePrice by Neighborhood (sorted by median)\")\n",
    "ax.set_xlabel(\"Neighborhood\")\n",
    "ax.set_ylabel(\"SalePrice ($)\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / \"boxplot_neighborhood.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002215f5",
   "metadata": {},
   "source": [
    "- **NoRidge, NridgHt, StoneBr** are the most expensive neighbourhoods (median > $250 k).\n",
    "- **MeadowV, IDOTRR, BrDale** are the cheapest (median < $100 k)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a872a14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Chart 5: SalePrice by decade built â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df_train[\"Decade\"] = (df_train[\"YearBuilt\"] // 10) * 10\n",
    "decade_price = df_train.groupby(\"Decade\")[TARGET_COL].mean()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(decade_price.index, decade_price.values, \"o-\", color=\"steelblue\", lw=2)\n",
    "ax.set_title(\"Mean SalePrice by Decade Built\")\n",
    "ax.set_xlabel(\"Decade\")\n",
    "ax.set_ylabel(\"Mean SalePrice ($)\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / \"line_decade.png\", dpi=150)\n",
    "plt.show()\n",
    "df_train.drop(columns=[\"Decade\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2cefbb",
   "metadata": {},
   "source": [
    "- Houses built after 2000 command significantly higher prices (newer construction, modern amenities).\n",
    "- Pre-1920 homes retain some value â€” likely historic/character appeal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3900ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Charts 6 & 7: SalePrice vs TotalBsmtSF and GarageArea â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for ax, col, color in zip(axes, [\"TotalBsmtSF\", \"GarageArea\"], [\"#3498db\", \"#e67e22\"]):\n",
    "    ax.scatter(df_train[col], df_train[TARGET_COL], alpha=0.4, c=color, edgecolors=\"k\", linewidths=0.3, s=25)\n",
    "    ax.set_title(f\"SalePrice vs {col}\")\n",
    "    ax.set_xlabel(col)\n",
    "    ax.set_ylabel(\"SalePrice ($)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / \"scatter_bsmt_garage.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7da169",
   "metadata": {},
   "source": [
    "- Both features show a **positive linear trend** with SalePrice.\n",
    "- `TotalBsmtSF = 0` cluster represents houses with no basement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f057a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Chart 8: Distribution of OverallQual â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "df_train[\"OverallQual\"].value_counts().sort_index().plot.bar(ax=ax, color=\"steelblue\", edgecolor=\"k\")\n",
    "ax.set_title(\"Distribution of Overall Quality\")\n",
    "ax.set_xlabel(\"OverallQual\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / \"hist_overallqual.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b581ebc8",
   "metadata": {},
   "source": [
    "- Most houses cluster around quality **5â€“7** (average to above-average).\n",
    "- Only a handful are rated 1â€“2 or 10 â€” extreme tails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec33c9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Chart 9: SalePrice by MSZoning (hue = low/high price) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "median_price = df_train[TARGET_COL].median()\n",
    "df_train[\"PriceBin\"] = np.where(df_train[TARGET_COL] >= median_price, \"High\", \"Low\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "sns.countplot(x=\"MSZoning\", hue=\"PriceBin\", data=df_train, ax=ax,\n",
    "              palette={\"Low\": \"#e74c3c\", \"High\": \"#2ecc71\"}, hue_order=[\"Low\", \"High\"])\n",
    "ax.set_title(\"MSZoning Count (Low vs High Price)\")\n",
    "ax.set_xlabel(\"MSZoning\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / \"bar_mszoning.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "df_train.drop(columns=[\"PriceBin\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180e26ea",
   "metadata": {},
   "source": [
    "- **RL (Residential Low Density)** dominates the dataset and has balanced high/low sales.\n",
    "- **RM (Residential Medium)** skews towards lower prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb69c796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Chart 10: Pairplot (top 5 features + SalePrice, sampled) â”€â”€â”€â”€â”€â”€â”€\n",
    "top5_feats = [\"OverallQual\", \"GrLivArea\", \"TotalBsmtSF\", \"GarageArea\", \"YearBuilt\", TARGET_COL]\n",
    "sample = df_train[top5_feats].sample(300, random_state=SEED)\n",
    "\n",
    "g = sns.pairplot(sample, corner=True, plot_kws={\"alpha\": 0.4, \"s\": 15})\n",
    "g.figure.suptitle(\"Pairplot â€” Top 5 Features + SalePrice\", y=1.01)\n",
    "g.figure.savefig(FIGURES_DIR / \"pairplot_top5.png\", dpi=100, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf7c3d6",
   "metadata": {},
   "source": [
    "- **GrLivArea** and **TotalBsmtSF** show clear linear relationships with SalePrice.\n",
    "- **YearBuilt** vs SalePrice shows a non-linear (accelerating) trend for newer homes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb46e26",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Feature Engineering\n",
    "\n",
    "Deriving composite features like `TotalSF` captures information spread across multiple columns.  \n",
    "Ordinal encoding preserves the natural order in quality scales; one-hot handles unordered categories.  \n",
    "Correcting skewness in numeric features prevents a few extreme values from dominating tree splits and linear weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f395bafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Ordinal quality mapping â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "QUAL_MAP = {\"None\": 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5}\n",
    "QUAL_COLS = [\n",
    "    \"ExterQual\", \"ExterCond\", \"BsmtQual\", \"BsmtCond\", \"HeatingQC\",\n",
    "    \"KitchenQual\", \"FireplaceQu\", \"GarageQual\", \"GarageCond\",\n",
    "]\n",
    "\n",
    "\n",
    "def engineer_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create new features, encode categoricals, and fix skewness.\"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # â”€â”€ Derived numeric features â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    df[\"TotalSF\"]        = df[\"TotalBsmtSF\"] + df[\"1stFlrSF\"] + df[\"2ndFlrSF\"]\n",
    "    df[\"TotalBathrooms\"] = (df[\"FullBath\"] + 0.5 * df[\"HalfBath\"]\n",
    "                            + df[\"BsmtFullBath\"] + 0.5 * df[\"BsmtHalfBath\"])\n",
    "    df[\"TotalPorchSF\"]   = (df[\"OpenPorchSF\"] + df[\"EnclosedPorch\"]\n",
    "                            + df[\"3SsnPorch\"] + df[\"ScreenPorch\"])\n",
    "    df[\"HouseAge\"]       = df[\"YrSold\"] - df[\"YearBuilt\"]\n",
    "    df[\"RemodelAge\"]     = df[\"YrSold\"] - df[\"YearRemodAdd\"]\n",
    "    df[\"IsRemodeled\"]    = (df[\"YearRemodAdd\"] != df[\"YearBuilt\"]).astype(int)\n",
    "    df[\"GarageAge\"]      = (df[\"YrSold\"] - df[\"GarageYrBlt\"]).clip(lower=0).fillna(0)\n",
    "    df[\"HasPool\"]        = (df.get(\"PoolArea\", pd.Series(0, index=df.index)) > 0).astype(int)\n",
    "    df[\"Has2ndFloor\"]    = (df[\"2ndFlrSF\"] > 0).astype(int)\n",
    "    df[\"HasGarage\"]      = (df[\"GarageArea\"] > 0).astype(int)\n",
    "    df[\"HasBsmt\"]        = (df[\"TotalBsmtSF\"] > 0).astype(int)\n",
    "    df[\"HasFireplace\"]   = (df[\"Fireplaces\"] > 0).astype(int)\n",
    "\n",
    "    # â”€â”€ Ordinal encode quality columns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    for col in QUAL_COLS:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].map(QUAL_MAP).fillna(0).astype(int)\n",
    "\n",
    "    # â”€â”€ Drop Id and raw columns we've replaced â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    df.drop(columns=[\"Id\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "    # â”€â”€ One-hot encode remaining categoricals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    df = pd.get_dummies(df, drop_first=True, dtype=int)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "print(f\"Before engineering â€” train: {df_train.shape}, test: {df_test.shape}\")\n",
    "\n",
    "# Keep target separate\n",
    "train_eng = engineer_features(df_train.drop(columns=[TARGET_COL]))\n",
    "test_eng  = engineer_features(df_test)\n",
    "\n",
    "# Align columns (add missing one-hot cols as 0, drop extras)\n",
    "train_eng, test_eng = train_eng.align(test_eng, join=\"left\", axis=1, fill_value=0)\n",
    "\n",
    "print(f\"After engineering  â€” train: {train_eng.shape}, test: {test_eng.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7666a49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Skewness correction (log1p for features with skew > 0.75) â”€â”€â”€â”€â”€â”€\n",
    "numeric_feats = train_eng.select_dtypes(include=np.number).columns\n",
    "skew_before = train_eng[numeric_feats].apply(lambda x: x.skew()).sort_values(ascending=False)\n",
    "skewed = skew_before[skew_before.abs() > 0.75]\n",
    "print(f\"Features with |skew| > 0.75: {len(skewed)}\")\n",
    "\n",
    "for col in skewed.index:\n",
    "    if (train_eng[col] >= 0).all() and (test_eng[col] >= 0).all():\n",
    "        train_eng[col] = np.log1p(train_eng[col])\n",
    "        test_eng[col]  = np.log1p(test_eng[col])\n",
    "\n",
    "skew_after = train_eng[skewed.index].apply(lambda x: x.skew())\n",
    "skew_compare = pd.DataFrame({\"before\": skew_before[skewed.index], \"after\": skew_after}).head(10)\n",
    "print(\"\\nSkewness before / after (top 10):\")\n",
    "skew_compare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb9aa7a",
   "metadata": {},
   "source": [
    "### ðŸ“Œ Encoding rationale\n",
    "- **Ordinal** for quality columns because `Po < Fa < TA < Gd < Ex` is a meaningful order.\n",
    "- **One-hot** for unordered categoricals (Neighborhood, RoofStyle, etc.) â€” no implicit ranking.\n",
    "- **Log1p** on skewed numerics reduces the influence of extreme values and improves linearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a918417",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Model Training & Cross-Validation Comparison\n",
    "\n",
    "Comparing multiple model families tells us whether the problem is best served by linear methods or tree-based ensembles.  \n",
    "Cross-validation gives a robust estimate of generalisation error, while the hold-out split provides a single sanity check.  \n",
    "Timing each model helps us balance accuracy vs compute cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0226c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Train / validation split â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "X = train_eng.copy()\n",
    "y = y_train_log.copy()\n",
    "\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, random_state=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6610ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Define models â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Ridge\":             Ridge(alpha=1.0, random_state=SEED),\n",
    "    \"Lasso\":             Lasso(alpha=0.001, random_state=SEED, max_iter=10_000),\n",
    "    \"Random Forest\":     RandomForestRegressor(n_estimators=200, random_state=SEED, n_jobs=-1),\n",
    "    \"XGBoost\":           XGBRegressor(n_estimators=300, learning_rate=0.05,\n",
    "                                      random_state=SEED, n_jobs=-1, verbosity=0),\n",
    "}\n",
    "\n",
    "kf = KFold(n_splits=CV_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "# â”€â”€ Cross-validate each model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "records = []\n",
    "for name, model in models.items():\n",
    "    t0 = time.time()\n",
    "    cv_scores = cross_val_score(model, X, y, cv=kf,\n",
    "                                scoring=\"neg_root_mean_squared_error\")\n",
    "    elapsed = time.time() - t0\n",
    "\n",
    "    cv_rmse = -cv_scores\n",
    "\n",
    "    # Fit once on split for Val metrics\n",
    "    model.fit(X_tr, y_tr)\n",
    "    val_pred = model.predict(X_val)\n",
    "    val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "    val_r2   = r2_score(y_val, val_pred)\n",
    "\n",
    "    records.append({\n",
    "        \"Model\":         name,\n",
    "        \"CV RMSE (mean)\": cv_rmse.mean(),\n",
    "        \"CV RMSE (std)\":  cv_rmse.std(),\n",
    "        \"Val RMSE\":      val_rmse,\n",
    "        \"Val RÂ²\":        val_r2,\n",
    "        \"Train time (s)\": round(elapsed, 2),\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(records).sort_values(\"CV RMSE (mean)\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33944a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Bar chart: CV RMSE comparison â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.barh(results_df[\"Model\"], results_df[\"CV RMSE (mean)\"],\n",
    "        xerr=results_df[\"CV RMSE (std)\"], color=\"steelblue\",\n",
    "        edgecolor=\"k\", capsize=4)\n",
    "ax.set_xlabel(\"CV RMSE (log scale)\")\n",
    "ax.set_ylabel(\"Model\")\n",
    "ax.set_title(\"Model Comparison â€” 5-Fold Cross-Validation RMSE\")\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / \"model_comparison.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "best_name = results_df.iloc[0][\"Model\"]\n",
    "print(f\"\\nðŸ† Best model: {best_name} (CV RMSE = {results_df.iloc[0]['CV RMSE (mean)']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d0b947",
   "metadata": {},
   "source": [
    "- **XGBoost** and **Random Forest** typically outperform linear models because housing prices depend on non-linear interactions (e.g., quality Ã— size Ã— location).\n",
    "- **Ridge/Lasso** still perform respectably thanks to our extensive feature engineering and skewness corrections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07276709",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Preprocessing Pipeline (sklearn Pipeline)\n",
    "\n",
    "Wrapping preprocessing + model into a single `Pipeline` object prevents **data leakage**: the scaler/imputer is fitted only on the training fold during cross-validation, never on the validation fold.  \n",
    "Pipelines also make deployment trivial â€” a single `pipeline.predict(new_data)` call handles everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428d8aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Identify feature types in the engineered data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# After get_dummies, everything is numeric, so we use the raw data pipeline\n",
    "# on the original df (before our manual engineering) to show the concept.\n",
    "\n",
    "# Rebuild from raw data for Pipeline demo\n",
    "raw_train = pd.read_csv(DATA_DIR / \"train.csv\")\n",
    "raw_train = raw_train[raw_train[\"GrLivArea\"] <= 4500].reset_index(drop=True)\n",
    "raw_train[\"MSSubClass\"] = raw_train[\"MSSubClass\"].astype(str)\n",
    "y_pipe = np.log1p(raw_train[TARGET_COL])\n",
    "X_pipe = raw_train.drop(columns=[TARGET_COL, \"Id\"])\n",
    "\n",
    "PIPE_NUM_FEATS = X_pipe.select_dtypes(include=np.number).columns.tolist()\n",
    "PIPE_CAT_FEATS = X_pipe.select_dtypes(include=\"object\").columns.tolist()\n",
    "\n",
    "# â”€â”€ Build ColumnTransformer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "numeric_transformer = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\",  StandardScaler()),\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", numeric_transformer, PIPE_NUM_FEATS),\n",
    "    (\"cat\", categorical_transformer, PIPE_CAT_FEATS),\n",
    "])\n",
    "\n",
    "# â”€â”€ Create pipelines â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "pipeline_ridge = Pipeline([(\"prep\", preprocessor), (\"model\", Ridge(random_state=SEED))])\n",
    "pipeline_xgb   = Pipeline([(\"prep\", preprocessor), (\"model\", XGBRegressor(\n",
    "    n_estimators=300, learning_rate=0.05, random_state=SEED, n_jobs=-1, verbosity=0))])\n",
    "\n",
    "# â”€â”€ Cross-validate pipelines â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "for name, pipe in [(\"Pipeline Ridge\", pipeline_ridge), (\"Pipeline XGBoost\", pipeline_xgb)]:\n",
    "    cv_scores = cross_val_score(pipe, X_pipe, y_pipe, cv=kf,\n",
    "                                scoring=\"neg_root_mean_squared_error\")\n",
    "    rmse = -cv_scores\n",
    "    print(f\"{name:20s}  â†’  CV RMSE: {rmse.mean():.4f} Â± {rmse.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b584077e",
   "metadata": {},
   "source": [
    "- Pipeline scores are slightly different from Section 6 because the manual feature engineering (TotalSF, etc.) is not included here â€” this is intentional to show the Pipeline concept.\n",
    "- **Key benefit:** the `SimpleImputer` and `StandardScaler` are fitted only on each training fold, not the full dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4f5da3",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Hyperparameter Tuning (Best Model)\n",
    "\n",
    "GridSearchCV exhaustively tests every combination in the search space with cross-validation.  \n",
    "Even small RMSE improvements matter on Kaggle, where the leaderboard is dense.  \n",
    "We tune the model that won in Section 6 â€” typically XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8eed5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ GridSearchCV on our best-performing engineered features â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Use the manually-engineered X/y from Section 6\n",
    "\n",
    "param_grid_xgb = {\n",
    "    \"n_estimators\":     [200, 300, 500],\n",
    "    \"max_depth\":        [3, 4, 6],\n",
    "    \"learning_rate\":    [0.01, 0.05, 0.1],\n",
    "    \"subsample\":        [0.8, 1.0],\n",
    "    \"colsample_bytree\": [0.8, 1.0],\n",
    "}\n",
    "\n",
    "param_grid_ridge = {\n",
    "    \"alpha\": [0.1, 1, 10, 50, 100, 500, 1000],\n",
    "}\n",
    "\n",
    "# Choose grid based on winner\n",
    "if \"XGBoost\" in best_name:\n",
    "    base = XGBRegressor(random_state=SEED, n_jobs=-1, verbosity=0)\n",
    "    grid = param_grid_xgb\n",
    "elif \"Ridge\" in best_name:\n",
    "    base = Ridge(random_state=SEED)\n",
    "    grid = param_grid_ridge\n",
    "else:\n",
    "    base = XGBRegressor(random_state=SEED, n_jobs=-1, verbosity=0)\n",
    "    grid = param_grid_xgb\n",
    "    print(f\"Defaulting to XGBoost tuning (winner was {best_name})\")\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    estimator=base,\n",
    "    param_grid=grid,\n",
    "    cv=kf,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    refit=True,\n",
    ")\n",
    "gs.fit(X, y)\n",
    "\n",
    "print(f\"\\nâœ… Best params: {gs.best_params_}\")\n",
    "print(f\"âœ… Best CV RMSE: {-gs.best_score_:.4f}\")\n",
    "\n",
    "best_model = gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb4ec5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Tuned vs untuned comparison â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "untuned_rmse = results_df[results_df[\"Model\"] == best_name][\"CV RMSE (mean)\"].values[0]\n",
    "tuned_rmse   = -gs.best_score_\n",
    "improvement  = (untuned_rmse - tuned_rmse) / untuned_rmse * 100\n",
    "\n",
    "print(f\"Untuned CV RMSE: {untuned_rmse:.4f}\")\n",
    "print(f\"Tuned   CV RMSE: {tuned_rmse:.4f}\")\n",
    "print(f\"Improvement:     {improvement:.1f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d872724",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Model Interpretation & Feature Importance\n",
    "\n",
    "Knowing *which* features drive predictions builds trust and domain understanding.  \n",
    "Residual analysis checks whether our model errors are random (good) or patterned (bad â€” missed signal).  \n",
    "Inspecting the worst predictions reveals edge cases the model struggles with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf8dada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ A. Feature Importance (top 20) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "best_model.fit(X_tr, y_tr)\n",
    "\n",
    "if hasattr(best_model, \"feature_importances_\"):\n",
    "    importances = best_model.feature_importances_\n",
    "elif hasattr(best_model, \"coef_\"):\n",
    "    importances = np.abs(best_model.coef_)\n",
    "else:\n",
    "    importances = np.zeros(X.shape[1])\n",
    "\n",
    "feat_imp = pd.Series(importances, index=X.columns).nlargest(20).sort_values()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "feat_imp.plot.barh(ax=ax, color=\"steelblue\", edgecolor=\"k\")\n",
    "ax.set_title(\"Top 20 Feature Importances\")\n",
    "ax.set_xlabel(\"Importance\")\n",
    "ax.set_ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / \"feature_importance.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c979f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ B. Residuals Analysis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "val_pred = best_model.predict(X_val)\n",
    "residuals = y_val.values - val_pred\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Predicted vs Actual\n",
    "axes[0].scatter(y_val, val_pred, alpha=0.4, c=\"steelblue\", edgecolors=\"k\", linewidths=0.3, s=20)\n",
    "lims = [y_val.min() - 0.1, y_val.max() + 0.1]\n",
    "axes[0].plot(lims, lims, \"r--\", lw=2, label=\"Ideal (y = x)\")\n",
    "axes[0].set_title(\"Predicted vs Actual (log scale)\")\n",
    "axes[0].set_xlabel(\"Actual log(SalePrice)\")\n",
    "axes[0].set_ylabel(\"Predicted log(SalePrice)\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Residuals distribution\n",
    "sns.histplot(residuals, kde=True, ax=axes[1], color=\"steelblue\")\n",
    "axes[1].set_title(\"Residuals Distribution\")\n",
    "axes[1].set_xlabel(\"Residual\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "\n",
    "# Residuals vs Predicted\n",
    "axes[2].scatter(val_pred, residuals, alpha=0.4, c=\"steelblue\", edgecolors=\"k\", linewidths=0.3, s=20)\n",
    "axes[2].axhline(0, color=\"red\", ls=\"--\", lw=1.5)\n",
    "axes[2].set_title(\"Residuals vs Predicted\")\n",
    "axes[2].set_xlabel(\"Predicted log(SalePrice)\")\n",
    "axes[2].set_ylabel(\"Residual\")\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / \"residuals_analysis.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e943a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ C. Top 5 most under- and over-predicted houses â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "error_df = X_val.copy()\n",
    "error_df[\"Actual_log\"]    = y_val.values\n",
    "error_df[\"Predicted_log\"] = val_pred\n",
    "error_df[\"Residual\"]      = residuals\n",
    "error_df[\"Actual_$\"]      = np.expm1(y_val.values)\n",
    "error_df[\"Predicted_$\"]   = np.expm1(val_pred)\n",
    "\n",
    "SHOW_COLS = [\"OverallQual\", \"GrLivArea\", \"Actual_$\", \"Predicted_$\", \"Residual\"]\n",
    "show_cols_available = [c for c in SHOW_COLS if c in error_df.columns]\n",
    "\n",
    "print(\"ðŸ”» Top 5 OVER-predicted (model too high):\")\n",
    "print(error_df.nlargest(5, \"Residual\")[show_cols_available].to_string())\n",
    "\n",
    "print(\"\\nðŸ”º Top 5 UNDER-predicted (model too low):\")\n",
    "print(error_df.nsmallest(5, \"Residual\")[show_cols_available].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd34ca50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ D. Ridge / Lasso Coefficients â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ridge_model = Ridge(alpha=10, random_state=SEED)\n",
    "ridge_model.fit(X_tr, y_tr)\n",
    "coefs = pd.Series(ridge_model.coef_, index=X.columns)\n",
    "\n",
    "top_pos = coefs.nlargest(15)\n",
    "top_neg = coefs.nsmallest(15)\n",
    "top_coefs = pd.concat([top_neg, top_pos]).sort_values()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "colors = [\"#e74c3c\" if v < 0 else \"#2ecc71\" for v in top_coefs.values]\n",
    "top_coefs.plot.barh(ax=ax, color=colors, edgecolor=\"k\")\n",
    "ax.set_title(\"Ridge Regression â€” Top 15 Positive & Negative Coefficients\")\n",
    "ax.set_xlabel(\"Coefficient\")\n",
    "ax.set_ylabel(\"Feature\")\n",
    "ax.axvline(0, color=\"k\", lw=0.8)\n",
    "plt.tight_layout()\n",
    "fig.savefig(FIGURES_DIR / \"ridge_coefficients.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6e2b6b",
   "metadata": {},
   "source": [
    "### ðŸ“Œ Coefficient interpretation\n",
    "- **OverallQual** and **GrLivArea** have the largest positive coefficients â€” higher quality and more living space â†’ higher price.\n",
    "- **HouseAge** has a negative coefficient â€” older houses sell for less, all else equal.\n",
    "- **Neighbourhood dummies** (e.g., NridgHt, StoneBr) capture location premiums that other features can't."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5965e4",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Final Predictions & Submission File\n",
    "\n",
    "For the Kaggle submission, we refit the tuned model on the **entire** training set (no hold-out) to maximise available information.  \n",
    "Since we trained on `log1p(SalePrice)`, we must apply `expm1` to convert predictions back to dollars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff91ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Refit on full training data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "best_model.fit(X, y)\n",
    "\n",
    "# â”€â”€ Predict on test set â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "test_pred_log = best_model.predict(test_eng)\n",
    "test_pred_dollars = np.expm1(test_pred_log)\n",
    "\n",
    "# â”€â”€ Build submission â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "submission = pd.DataFrame({\n",
    "    \"Id\": test_ids,\n",
    "    \"SalePrice\": test_pred_dollars,\n",
    "})\n",
    "\n",
    "submission.to_csv(OUTPUT_DIR / \"submission.csv\", index=False)\n",
    "\n",
    "# â”€â”€ Sanity checks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "assert (submission[\"SalePrice\"] > 0).all(), \"Negative prices detected!\"\n",
    "assert submission.shape[0] == len(test_ids), \"Row count mismatch!\"\n",
    "\n",
    "print(f\"Submission shape: {submission.shape}\")\n",
    "print(f\"Predicted price range: ${submission['SalePrice'].min():,.0f} â€“ ${submission['SalePrice'].max():,.0f}\")\n",
    "print()\n",
    "submission.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d2ff2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4d305d",
   "metadata": {},
   "source": [
    "> ðŸ“„ **`outputs/submission.csv`** has been saved and is ready for [Kaggle upload](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/submit).  \n",
    "> Remember: the model predicts `log1p(SalePrice)` internally â€” `expm1` converts back to real dollars."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3ec21c",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Key Takeaways & Next Steps\n",
    "\n",
    "### Top 5 EDA findings\n",
    "1. **OverallQual** is the single strongest predictor (correlation â‰ˆ 0.79 with SalePrice).\n",
    "2. **GrLivArea** shows a clear linear trend â€” but two outliers (>4500 sq ft, low price) needed removal.\n",
    "3. **Neighbourhood** drives massive price variation: NoRidge/NridgHt/StoneBr vs MeadowV/IDOTRR.\n",
    "4. Houses built after 2000 command significantly higher prices (modern construction premium).\n",
    "5. \"Missing\" data in quality columns (PoolQC, Fence, etc.) actually means *absence of feature*, not unknown data.\n",
    "\n",
    "### Best model\n",
    "- **XGBoost** (tuned via GridSearchCV) achieved the best 5-fold CV RMSE on log(SalePrice).\n",
    "- Residuals are approximately normally distributed with no strong patterns â†’ good model fit.\n",
    "\n",
    "### Top 3 most predictive features\n",
    "1. **OverallQual** â€” Overall material and finish quality (ordinal 1â€“10).\n",
    "2. **TotalSF** (engineered) â€” Combined total square footage (basement + 1st + 2nd floor).\n",
    "3. **GrLivArea** â€” Above-ground living area in sq ft.\n",
    "\n",
    "### Next steps\n",
    "1. **Stacking / Blending** â€” Combine XGBoost + Ridge + LightGBM predictions with a meta-learner.\n",
    "2. **SHAP values** â€” Instance-level feature explanations for model interpretability.\n",
    "3. **Target Encoding** for high-cardinality categoricals (Neighborhood has 25 levels) â€” more efficient than one-hot.\n",
    "4. **Title extraction** from features â€” explore interaction terms (OverallQual Ã— GrLivArea).\n",
    "5. **LightGBM / CatBoost** â€” Alternative gradient boosting implementations that often outperform XGBoost on tabular data."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
