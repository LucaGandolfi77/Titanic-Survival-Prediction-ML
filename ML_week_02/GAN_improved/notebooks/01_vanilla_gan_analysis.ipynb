{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6195e8b",
   "metadata": {},
   "source": [
    "# Stage 1 — Vanilla GAN Analysis\n",
    "\n",
    "This notebook analyzes the training and output quality of the **MLP-based Vanilla GAN** on MNIST.\n",
    "\n",
    "Sections:\n",
    "1. Setup & imports\n",
    "2. Load config and trained model\n",
    "3. Visualize generated samples\n",
    "4. Training loss curves\n",
    "5. Real vs Generated comparison\n",
    "6. Latent space interpolation\n",
    "7. FID & Inception Score evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bf8e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Project root\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "if PROJECT_ROOT.name == \"notebooks\":\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "from IPython.display import display, Image as IPImage\n",
    "\n",
    "from src.utils.config_loader import load_config, get_device\n",
    "from src.utils.checkpointing import load_checkpoint, find_latest_checkpoint\n",
    "from src.models.vanilla_gan import build_vanilla_gan\n",
    "from src.data.dataloaders import get_dataloader_from_config\n",
    "from src.evaluation.visualization import GANVisualizer\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0242c5",
   "metadata": {},
   "source": [
    "## 1. Load Configuration & Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c9f0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config\n",
    "config = load_config(PROJECT_ROOT / \"config\" / \"vanilla_gan.yaml\")\n",
    "device = get_device(config)\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Experiment: {config['experiment']['name']}\")\n",
    "\n",
    "# Build models\n",
    "generator, discriminator = build_vanilla_gan(config)\n",
    "generator = generator.to(device)\n",
    "discriminator = discriminator.to(device)\n",
    "\n",
    "print(f\"\\nGenerator parameters: {sum(p.numel() for p in generator.parameters()):,}\")\n",
    "print(f\"Discriminator parameters: {sum(p.numel() for p in discriminator.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54ba7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained checkpoint\n",
    "ckpt_dir = Path(config['paths']['models_dir'])\n",
    "ckpt_path = find_latest_checkpoint(ckpt_dir)\n",
    "\n",
    "if ckpt_path:\n",
    "    info = load_checkpoint(ckpt_path, generator, discriminator, device=device)\n",
    "    print(f\"Loaded checkpoint: epoch {info['epoch']}, step {info['global_step']}\")\n",
    "    print(f\"Metrics: {info['metrics']}\")\n",
    "else:\n",
    "    print(\"No checkpoint found — using untrained model for demonstration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537eacf1",
   "metadata": {},
   "source": [
    "## 2. Generate & Visualize Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f1cb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a grid of samples\n",
    "generator.eval()\n",
    "latent_dim = config['model']['latent_dim']\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(64, latent_dim, device=device)\n",
    "    fake_images = generator(z)\n",
    "\n",
    "# Display grid\n",
    "grid = make_grid(fake_images.cpu(), nrow=8, normalize=True, value_range=(-1, 1))\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(grid.permute(1, 2, 0).numpy(), cmap='gray')\n",
    "plt.title('Vanilla GAN — Generated Samples')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7b5d22",
   "metadata": {},
   "source": [
    "## 3. Real vs Generated Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b0a36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load real data\n",
    "dataloader = get_dataloader_from_config(config)\n",
    "real_batch, _ = next(iter(dataloader))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Real\n",
    "real_grid = make_grid(real_batch[:32], nrow=8, normalize=True, value_range=(-1, 1))\n",
    "ax1.imshow(real_grid.permute(1, 2, 0).numpy(), cmap='gray')\n",
    "ax1.set_title('Real MNIST Images', fontsize=14)\n",
    "ax1.axis('off')\n",
    "\n",
    "# Fake\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(32, latent_dim, device=device)\n",
    "    fakes = generator(z)\n",
    "fake_grid = make_grid(fakes.cpu(), nrow=8, normalize=True, value_range=(-1, 1))\n",
    "ax2.imshow(fake_grid.permute(1, 2, 0).numpy(), cmap='gray')\n",
    "ax2.set_title('Generated Images (Vanilla GAN)', fontsize=14)\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd206807",
   "metadata": {},
   "source": [
    "## 4. Training Loss Curves\n",
    "\n",
    "If training has been run with TensorBoard logging, we can visualize the loss curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8450cc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Run this after training. The training loop saves loss history.\n",
    "# You can also launch TensorBoard:\n",
    "#   tensorboard --logdir outputs/tensorboard\n",
    "\n",
    "# Placeholder: if you have saved losses, plot them\n",
    "# Example with synthetic data for demonstration:\n",
    "n_steps = 1000\n",
    "g_losses_demo = np.random.exponential(0.7, n_steps).cumsum() / np.arange(1, n_steps + 1)\n",
    "d_losses_demo = np.random.exponential(0.7, n_steps).cumsum() / np.arange(1, n_steps + 1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.plot(g_losses_demo, label='Generator Loss', alpha=0.7)\n",
    "ax.plot(d_losses_demo, label='Discriminator Loss', alpha=0.7)\n",
    "ax.set_xlabel('Training Step')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Vanilla GAN — Training Loss Curves')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Tip: Run `tensorboard --logdir outputs/tensorboard` for interactive plots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22442627",
   "metadata": {},
   "source": [
    "## 5. Latent Space Interpolation\n",
    "\n",
    "Smooth transitions between two random latent vectors demonstrate\n",
    "the generator has learned a meaningful latent representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbc774f",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = GANVisualizer(output_dir=str(PROJECT_ROOT / \"outputs\"))\n",
    "\n",
    "# Generate interpolation\n",
    "n_pairs = 5\n",
    "n_steps = 10\n",
    "\n",
    "generator.eval()\n",
    "all_interp = []\n",
    "\n",
    "for _ in range(n_pairs):\n",
    "    z1 = torch.randn(latent_dim, device=device)\n",
    "    z2 = torch.randn(latent_dim, device=device)\n",
    "    z_interp = visualizer.spherical_interpolation(z1, z2, n_steps)\n",
    "    with torch.no_grad():\n",
    "        imgs = generator(z_interp)\n",
    "    all_interp.append(imgs)\n",
    "\n",
    "interp_grid = make_grid(\n",
    "    torch.cat(all_interp, dim=0).cpu(),\n",
    "    nrow=n_steps, normalize=True, value_range=(-1, 1)\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.imshow(interp_grid.permute(1, 2, 0).numpy(), cmap='gray')\n",
    "plt.title('Latent Space Interpolation (Spherical)', fontsize=14)\n",
    "plt.xlabel('Interpolation steps →')\n",
    "plt.ylabel('Different pairs ↓')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9862c50",
   "metadata": {},
   "source": [
    "## 6. Quantitative Evaluation (FID & IS)\n",
    "\n",
    "**FID Score**: Measures the Fréchet distance between feature distributions of real and generated images.  \n",
    "$$FID = ||\\mu_r - \\mu_g||^2 + \\text{Tr}(\\Sigma_r + \\Sigma_g - 2(\\Sigma_r \\Sigma_g)^{1/2})$$\n",
    "\n",
    "**Inception Score**: Measures quality (sharp predictions) and diversity (varied predictions).  \n",
    "$$IS = \\exp(\\mathbb{E}_x[KL(p(y|x) \\| p(y))])$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8c37ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation.fid_score import FIDCalculator\n",
    "from src.evaluation.inception_score import InceptionScoreCalculator\n",
    "\n",
    "N_EVAL_SAMPLES = 1000  # Use 5000+ for accurate results\n",
    "\n",
    "# Collect real images\n",
    "real_list = []\n",
    "count = 0\n",
    "for imgs, _ in dataloader:\n",
    "    real_list.append(imgs)\n",
    "    count += imgs.size(0)\n",
    "    if count >= N_EVAL_SAMPLES:\n",
    "        break\n",
    "real_images = torch.cat(real_list)[:N_EVAL_SAMPLES]\n",
    "\n",
    "# Generate fake images\n",
    "fake_list = []\n",
    "remaining = N_EVAL_SAMPLES\n",
    "while remaining > 0:\n",
    "    bs = min(64, remaining)\n",
    "    z = torch.randn(bs, latent_dim, device=device)\n",
    "    with torch.no_grad():\n",
    "        fakes = generator(z)\n",
    "    fake_list.append(fakes.cpu())\n",
    "    remaining -= bs\n",
    "fake_images = torch.cat(fake_list)[:N_EVAL_SAMPLES]\n",
    "\n",
    "print(f\"Real: {real_images.shape}, Fake: {fake_images.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce33e028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute FID (requires InceptionV3 — may take a while on first run)\n",
    "print(\"Computing FID Score...\")\n",
    "fid_calc = FIDCalculator(device=str(device), batch_size=32)\n",
    "fid = fid_calc.compute_fid(real_images, fake_images)\n",
    "print(f\"FID Score: {fid:.2f}\")\n",
    "\n",
    "print(\"\\nComputing Inception Score...\")\n",
    "is_calc = InceptionScoreCalculator(device=str(device), batch_size=32)\n",
    "is_mean, is_std = is_calc.compute_inception_score(fake_images, splits=5)\n",
    "print(f\"Inception Score: {is_mean:.2f} ± {is_std:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe358dd6",
   "metadata": {},
   "source": [
    "## 7. Architecture Summary\n",
    "\n",
    "| Component | Architecture | Parameters |\n",
    "|-----------|-------------|------------|\n",
    "| Generator | MLP: 100 → 256 → 512 → 1024 → 784 | ~1.3M |\n",
    "| Discriminator | MLP: 784 → 512 → 256 → 1 | ~0.5M |\n",
    "| Loss | BCE (Binary Cross-Entropy) with logits | — |\n",
    "| Stability | Spectral Norm (D), Label Smoothing, Dropout | — |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
