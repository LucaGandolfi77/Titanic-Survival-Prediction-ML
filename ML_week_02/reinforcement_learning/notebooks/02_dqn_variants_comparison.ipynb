{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44795a70",
   "metadata": {},
   "source": [
    "# Stage 2 — DQN Variants Comparison\n",
    "\n",
    "Comparative analysis of three DQN improvements on CartPole-v1:\n",
    "\n",
    "| Variant | Key Idea |\n",
    "|---------|----------|\n",
    "| **Vanilla DQN** | Experience Replay + Target Network |\n",
    "| **Double DQN** | Decouple selection & evaluation to reduce overestimation |\n",
    "| **Dueling DQN** | Separate Value and Advantage streams |\n",
    "\n",
    "All variants can optionally use **Prioritized Experience Replay**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbcfdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "\n",
    "from src.agents import DQNAgent, DDQNAgent, DuelingDQNAgent\n",
    "from src.environments.wrappers import EpisodeStatsWrapper\n",
    "from src.training.trainer import Trainer\n",
    "from src.training.evaluator import Evaluator\n",
    "from src.utils.config_loader import load_config, get_device, merge_configs\n",
    "from src.utils.logger import RLLogger\n",
    "from src.utils.plotting import plot_comparison\n",
    "\n",
    "print(f\"PyTorch {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2a7c94",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "We use the DDQN config as a base and override the agent type for each variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4862ff2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_config = load_config(PROJECT_ROOT / \"config\" / \"cartpole_ddqn.yaml\")\n",
    "device = get_device(base_config)\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Quick training for comparison (reduce episodes for demo)\n",
    "N_EPISODES = 300\n",
    "\n",
    "variants = {\n",
    "    \"DQN\": {\"agent\": {\"type\": \"dqn\", \"buffer_type\": \"standard\", \"n_episodes\": N_EPISODES}},\n",
    "    \"DDQN\": {\"agent\": {\"type\": \"ddqn\", \"buffer_type\": \"standard\", \"n_episodes\": N_EPISODES}},\n",
    "    \"DDQN + PER\": {\"agent\": {\"type\": \"ddqn\", \"buffer_type\": \"prioritized\", \"n_episodes\": N_EPISODES}},\n",
    "    \"Dueling DQN\": {\"agent\": {\"type\": \"dueling_dqn\", \"buffer_type\": \"standard\", \"n_episodes\": N_EPISODES}},\n",
    "    \"Dueling + PER\": {\"agent\": {\"type\": \"dueling_dqn\", \"buffer_type\": \"prioritized\", \"n_episodes\": N_EPISODES}},\n",
    "}\n",
    "print(f\"Will compare {len(variants)} variants over {N_EPISODES} episodes each.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1340481e",
   "metadata": {},
   "source": [
    "## 2. Train All Variants\n",
    "\n",
    "⚠️ This cell trains 5 agents sequentially — takes a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7d8761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.agents.dqn_agent import DQNAgent\n",
    "from src.agents.ddqn_agent import DDQNAgent\n",
    "from src.agents.dueling_dqn_agent import DuelingDQNAgent\n",
    "\n",
    "AGENT_CLS = {\"dqn\": DQNAgent, \"ddqn\": DDQNAgent, \"dueling_dqn\": DuelingDQNAgent}\n",
    "all_results = {}\n",
    "\n",
    "for name, overrides in variants.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training: {name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "    cfg = merge_configs(base_config, overrides)\n",
    "    cfg[\"experiment\"][\"name\"] = name.lower().replace(\" \", \"_\").replace(\"+\", \"\")\n",
    "\n",
    "    np.random.seed(cfg[\"experiment\"].get(\"seed\", 42))\n",
    "    torch.manual_seed(cfg[\"experiment\"].get(\"seed\", 42))\n",
    "\n",
    "    train_env = EpisodeStatsWrapper(gym.make(\"CartPole-v1\"))\n",
    "    eval_env = EpisodeStatsWrapper(gym.make(\"CartPole-v1\"))\n",
    "\n",
    "    agent_cls = AGENT_CLS[cfg[\"agent\"][\"type\"]]\n",
    "    agent = agent_cls(cfg, device)\n",
    "    logger = RLLogger(cfg[\"experiment\"][\"name\"], use_tensorboard=False)\n",
    "    evaluator = Evaluator(eval_env, cfg)\n",
    "    trainer = Trainer(agent, train_env, cfg, logger, evaluator)\n",
    "    history = trainer.train()\n",
    "\n",
    "    all_results[name] = history[\"rewards\"]\n",
    "    train_env.close()\n",
    "    eval_env.close()\n",
    "    logger.close()\n",
    "\n",
    "print(\"\\nAll variants trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f77af30",
   "metadata": {},
   "source": [
    "## 3. Reward Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4732e170",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_comparison(\n",
    "    results=all_results,\n",
    "    title=\"DQN Variants — CartPole-v1\",\n",
    "    window=20,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d760dce",
   "metadata": {},
   "source": [
    "## 4. Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a84bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{'Variant':<25} {'Mean(last50)':>12} {'Std':>8} {'Max':>8} {'Solved@':>10}\")\n",
    "print(\"─\" * 65)\n",
    "\n",
    "for name, rewards in all_results.items():\n",
    "    last50 = rewards[-50:] if len(rewards) >= 50 else rewards\n",
    "    mean_r = np.mean(last50)\n",
    "    std_r = np.std(last50)\n",
    "    max_r = np.max(rewards)\n",
    "\n",
    "    # Find first episode where running avg(100) >= 475\n",
    "    solved_ep = \"N/A\"\n",
    "    running = []\n",
    "    for i, r in enumerate(rewards):\n",
    "        running.append(r)\n",
    "        if len(running) > 100:\n",
    "            running.pop(0)\n",
    "        if len(running) == 100 and np.mean(running) >= 475:\n",
    "            solved_ep = str(i + 1)\n",
    "            break\n",
    "\n",
    "    print(f\"{name:<25} {mean_r:>12.1f} {std_r:>8.1f} {max_r:>8.0f} {solved_ep:>10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c66e03",
   "metadata": {},
   "source": [
    "## 5. Learning Efficiency — Area Under the Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63ed280",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "names = list(all_results.keys())\n",
    "aucs = [np.trapz(all_results[n]) for n in names]\n",
    "colours = plt.cm.tab10.colors[:len(names)]\n",
    "\n",
    "ax.barh(names, aucs, color=colours)\n",
    "ax.set_xlabel(\"Area Under Reward Curve (higher = faster learning)\")\n",
    "ax.set_title(\"Learning Efficiency Comparison\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c04fb14",
   "metadata": {},
   "source": [
    "## 6. Overestimation Analysis\n",
    "\n",
    "Vanilla DQN tends to overestimate Q-values. Let's compare the max Q-values\n",
    "predicted by DQN vs DDQN on a set of random states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210f0ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick re-train small agents to compare Q-values\n",
    "from src.agents.dqn_agent import DQNAgent\n",
    "from src.agents.ddqn_agent import DDQNAgent\n",
    "\n",
    "small_cfg = merge_configs(base_config, {\"agent\": {\"n_episodes\": 100, \"buffer_type\": \"standard\"}})\n",
    "\n",
    "# Generate random states\n",
    "n_states = 200\n",
    "random_states = np.random.uniform(\n",
    "    low=[-2.4, -3.0, -0.25, -3.0],\n",
    "    high=[2.4, 3.0, 0.25, 3.0],\n",
    "    size=(n_states, 4),\n",
    ").astype(np.float32)\n",
    "states_t = torch.tensor(random_states, device=device)\n",
    "\n",
    "# Check Q-values from already-trained agents (if available)\n",
    "dqn_cfg = merge_configs(small_cfg, {\"agent\": {\"type\": \"dqn\"}})\n",
    "ddqn_cfg = merge_configs(small_cfg, {\"agent\": {\"type\": \"ddqn\"}})\n",
    "\n",
    "dqn_agent = DQNAgent(dqn_cfg, device)\n",
    "ddqn_agent = DDQNAgent(ddqn_cfg, device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    dqn_q = dqn_agent.online_net(states_t).max(dim=1).values.cpu().numpy()\n",
    "    ddqn_q = ddqn_agent.online_net(states_t).max(dim=1).values.cpu().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.hist(dqn_q, bins=30, alpha=0.6, label=\"DQN max Q\", color=\"tomato\")\n",
    "ax.hist(ddqn_q, bins=30, alpha=0.6, label=\"DDQN max Q\", color=\"steelblue\")\n",
    "ax.set_xlabel(\"Max Q-value\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"Q-value Distribution (random states, untrained — for structure demo)\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
