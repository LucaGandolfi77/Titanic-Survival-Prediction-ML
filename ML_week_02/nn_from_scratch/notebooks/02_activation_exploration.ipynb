{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "009b1d4b",
   "metadata": {},
   "source": [
    "# Activation Function Exploration\n",
    "\n",
    "Visual comparison of activation functions: **ReLU, LeakyReLU, Sigmoid, Tanh, Softmax**.\n",
    "\n",
    "For each we plot:\n",
    "1. The function itself\n",
    "2. Its derivative\n",
    "3. Impact on gradient flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e97a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from src.core.activations import ReLU, LeakyReLU, Sigmoid, Tanh, Softmax\n",
    "\n",
    "output_dir = Path('..') / 'outputs' / 'plots'\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877ac1a0",
   "metadata": {},
   "source": [
    "## 1. Compute Activations and Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af168ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.linspace(-5, 5, 500).reshape(-1, 1)\n",
    "\n",
    "activations = {\n",
    "    'ReLU': ReLU(),\n",
    "    'LeakyReLU(0.1)': LeakyReLU(alpha=0.1),\n",
    "    'Sigmoid': Sigmoid(),\n",
    "    'Tanh': Tanh(),\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, act in activations.items():\n",
    "    A = act.forward(Z.copy())\n",
    "    dZ = act.backward(np.ones_like(Z))\n",
    "    results[name] = {'A': A, 'dZ': dZ}\n",
    "    print(f'{name:20s} | output range: [{A.min():.3f}, {A.max():.3f}] | grad range: [{dZ.min():.3f}, {dZ.max():.3f}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8fb96f",
   "metadata": {},
   "source": [
    "## 2. Plot Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7662ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "for ax, (name, data) in zip(axes.ravel(), results.items()):\n",
    "    ax.plot(Z.ravel(), data['A'].ravel(), label=f'{name}(z)', linewidth=2)\n",
    "    ax.plot(Z.ravel(), data['dZ'].ravel(), '--', label=f\"{name}'(z)\", linewidth=2)\n",
    "    ax.axhline(0, color='k', linewidth=0.5)\n",
    "    ax.axvline(0, color='k', linewidth=0.5)\n",
    "    ax.set_title(name, fontsize=14)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlabel('z')\n",
    "\n",
    "fig.suptitle('Activation Functions & Their Derivatives', fontsize=16)\n",
    "fig.tight_layout()\n",
    "fig.savefig(output_dir / 'activation_functions.png', dpi=150)\n",
    "plt.close(fig)\n",
    "print('Saved activation_functions.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ce5db0",
   "metadata": {},
   "source": [
    "## 3. Vanishing Gradient Problem\n",
    "\n",
    "Sigmoid/Tanh saturate for large |z|, causing gradients → 0.\n",
    "ReLU doesn't saturate for z > 0 but \"dies\" for z < 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b88a814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate gradient flow through 10 layers\n",
    "n_layers = 10\n",
    "gradient_norms = {}\n",
    "\n",
    "for name, act_cls in [('Sigmoid', Sigmoid), ('Tanh', Tanh), ('ReLU', ReLU)]:\n",
    "    norms = []\n",
    "    grad = np.ones((1, 64))\n",
    "    rng = np.random.default_rng(42)\n",
    "    \n",
    "    for _ in range(n_layers):\n",
    "        act = act_cls()\n",
    "        Z_layer = rng.standard_normal((1, 64))\n",
    "        act.forward(Z_layer)\n",
    "        grad = act.backward(grad)\n",
    "        norms.append(np.linalg.norm(grad))\n",
    "    \n",
    "    gradient_norms[name] = norms\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "for name, norms in gradient_norms.items():\n",
    "    ax.plot(range(1, n_layers + 1), norms, 'o-', label=name, linewidth=2)\n",
    "ax.set_xlabel('Layer depth')\n",
    "ax.set_ylabel('Gradient norm')\n",
    "ax.set_title('Gradient Flow Through Activations (10 layers)')\n",
    "ax.legend()\n",
    "ax.set_yscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "fig.tight_layout()\n",
    "fig.savefig(output_dir / 'gradient_flow.png', dpi=150)\n",
    "plt.close(fig)\n",
    "print('Saved gradient_flow.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7ae65e",
   "metadata": {},
   "source": [
    "## 4. Softmax Temperature\n",
    "\n",
    "Softmax with temperature T: `softmax(z/T)` — higher T = more uniform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542dba50",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_sm = np.array([[2.0, 1.0, 0.1, -1.0, 3.0]])\n",
    "temperatures = [0.5, 1.0, 2.0, 5.0]\n",
    "sm = Softmax()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "x_ = np.arange(Z_sm.shape[1])\n",
    "width = 0.18\n",
    "\n",
    "for i, T in enumerate(temperatures):\n",
    "    probs = sm.forward(Z_sm / T)\n",
    "    ax.bar(x_ + i * width, probs.ravel(), width, label=f'T={T}')\n",
    "\n",
    "ax.set_xlabel('Class')\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_title('Softmax Temperature Scaling')\n",
    "ax.legend()\n",
    "ax.set_xticks(x_ + 1.5 * width)\n",
    "ax.set_xticklabels([f'Class {i}' for i in range(5)])\n",
    "fig.tight_layout()\n",
    "fig.savefig(output_dir / 'softmax_temperature.png', dpi=150)\n",
    "plt.close(fig)\n",
    "print('Saved softmax_temperature.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9895c1f0",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Activation | Pros | Cons |\n",
    "|-----------|------|------|\n",
    "| **ReLU** | Fast, no saturation for z>0 | Dead neurons (z<0 gives 0 grad) |\n",
    "| **LeakyReLU** | No dead neurons | Extra hyperparameter α |\n",
    "| **Sigmoid** | Output ∈ (0,1), interpretable | Vanishing gradients, not zero-centred |\n",
    "| **Tanh** | Zero-centred, output ∈ (-1,1) | Vanishing gradients |\n",
    "| **Softmax** | Proper probability distribution | Only for output layer |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
