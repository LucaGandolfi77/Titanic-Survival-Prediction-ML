{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7d13e34",
   "metadata": {},
   "source": [
    "# Stage 1 — DQN on CartPole-v1\n",
    "\n",
    "This notebook analyses the Vanilla DQN agent trained on the classic CartPole-v1 environment.\n",
    "\n",
    "**Key concepts:**\n",
    "- Experience Replay for breaking temporal correlations\n",
    "- Target Network for stable TD targets\n",
    "- Epsilon-greedy exploration with multiplicative decay\n",
    "- Huber loss (Smooth L1) for robustness to outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd9f7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure project root is on the path\n",
    "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "\n",
    "from src.agents.dqn_agent import DQNAgent\n",
    "from src.environments.wrappers import EpisodeStatsWrapper\n",
    "from src.training.evaluator import Evaluator\n",
    "from src.utils.config_loader import load_config, get_device\n",
    "from src.utils.plotting import plot_training_curves\n",
    "\n",
    "print(f\"PyTorch {torch.__version__}\")\n",
    "print(f\"Device: {torch.device('mps' if torch.backends.mps.is_available() else 'cpu')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8050a6f",
   "metadata": {},
   "source": [
    "## 1. Load Configuration & Build Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c89b9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(PROJECT_ROOT / \"config\" / \"cartpole_dqn.yaml\")\n",
    "device = get_device(config)\n",
    "print(f\"Experiment: {config['experiment']['name']}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Agent config: hidden_dims={config['agent']['hidden_dims']}, \"\n",
    "      f\"lr={config['agent']['learning_rate']}, gamma={config['agent']['gamma']}\")\n",
    "\n",
    "agent = DQNAgent(config, device)\n",
    "print(f\"\\nOnline network:\\n{agent.online_net}\")\n",
    "n_params = sum(p.numel() for p in agent.online_net.parameters())\n",
    "print(f\"\\nTotal parameters: {n_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98487181",
   "metadata": {},
   "source": [
    "## 2. Load Trained Checkpoint (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafd7ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = PROJECT_ROOT / \"outputs\" / \"models\" / \"cartpole\" / \"checkpoint_best.pt\"\n",
    "if checkpoint_path.exists():\n",
    "    ckpt = agent.load(checkpoint_path)\n",
    "    episode_rewards = ckpt.get(\"episode_rewards\", [])\n",
    "    print(f\"Loaded checkpoint from episode {ckpt.get('episode', '?')}\")\n",
    "    print(f\"Best eval reward: {ckpt.get('best_eval_reward', 'N/A')}\")\n",
    "    print(f\"Training episodes recorded: {len(episode_rewards)}\")\n",
    "else:\n",
    "    episode_rewards = []\n",
    "    print(\"No checkpoint found. Run training first:\")\n",
    "    print(\"  python train.py --config config/cartpole_dqn.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c8cf81",
   "metadata": {},
   "source": [
    "## 3. Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d06ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if episode_rewards:\n",
    "    fig = plot_training_curves(\n",
    "        rewards=episode_rewards,\n",
    "        title=\"DQN — CartPole-v1 Training Progress\",\n",
    "        window=20,\n",
    "    )\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No training data available yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83949828",
   "metadata": {},
   "source": [
    "## 4. Greedy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefd4170",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = EpisodeStatsWrapper(gym.make(\"CartPole-v1\"))\n",
    "evaluator = Evaluator(eval_env, config)\n",
    "agent.epsilon = 0.0  # greedy\n",
    "\n",
    "result = evaluator.evaluate(agent)\n",
    "print(\"Evaluation (greedy policy):\")\n",
    "for k, v in result.items():\n",
    "    print(f\"  {k:<15}: {v:.2f}\")\n",
    "\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17df1613",
   "metadata": {},
   "source": [
    "## 5. Visualise a Single Episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f356abdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "state, _ = env.reset(seed=42)\n",
    "\n",
    "positions, velocities, angles, rewards_ep = [], [], [], []\n",
    "total_reward = 0.0\n",
    "\n",
    "for step in range(500):\n",
    "    action = agent.select_action(state, eval_mode=True)\n",
    "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    positions.append(state[0])\n",
    "    velocities.append(state[1])\n",
    "    angles.append(np.degrees(state[2]))\n",
    "    rewards_ep.append(total_reward)\n",
    "    state = next_state\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 8), sharex=True)\n",
    "axes[0].plot(positions, color=\"steelblue\")\n",
    "axes[0].set_ylabel(\"Cart Position\")\n",
    "axes[0].set_title(f\"DQN CartPole Episode (Total Reward: {total_reward:.0f})\")\n",
    "\n",
    "axes[1].plot(angles, color=\"tomato\")\n",
    "axes[1].set_ylabel(\"Pole Angle (°)\")\n",
    "\n",
    "axes[2].plot(rewards_ep, color=\"green\")\n",
    "axes[2].set_ylabel(\"Cumulative Reward\")\n",
    "axes[2].set_xlabel(\"Step\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3860f65c",
   "metadata": {},
   "source": [
    "## 6. Q-Value Landscape\n",
    "\n",
    "Visualise Q-values as a function of pole angle and angular velocity\n",
    "(with position and velocity fixed at zero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966a9ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "angles_range = np.linspace(-0.25, 0.25, 50)\n",
    "ang_vel_range = np.linspace(-2.0, 2.0, 50)\n",
    "AA, VV = np.meshgrid(angles_range, ang_vel_range)\n",
    "\n",
    "q_left = np.zeros_like(AA)\n",
    "q_right = np.zeros_like(AA)\n",
    "\n",
    "for i in range(AA.shape[0]):\n",
    "    for j in range(AA.shape[1]):\n",
    "        state = np.array([0.0, 0.0, AA[i, j], VV[i, j]], dtype=np.float32)\n",
    "        state_t = torch.tensor(state, device=device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q = agent.online_net(state_t).cpu().numpy()[0]\n",
    "        q_left[i, j] = q[0]\n",
    "        q_right[i, j] = q[1]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "for ax, data, title in [\n",
    "    (axes[0], q_left, \"Q(s, left)\"),\n",
    "    (axes[1], q_right, \"Q(s, right)\"),\n",
    "    (axes[2], q_right - q_left, \"Advantage (right − left)\"),\n",
    "]:\n",
    "    im = ax.contourf(AA, VV, data, levels=30, cmap=\"RdBu_r\")\n",
    "    ax.set_xlabel(\"Pole Angle (rad)\")\n",
    "    ax.set_ylabel(\"Angular Velocity (rad/s)\")\n",
    "    ax.set_title(title)\n",
    "    fig.colorbar(im, ax=ax)\n",
    "\n",
    "fig.suptitle(\"Q-Value Landscape (pos=0, vel=0)\", fontsize=14, y=1.02)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
