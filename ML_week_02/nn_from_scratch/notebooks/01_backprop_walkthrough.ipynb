{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b347735d",
   "metadata": {},
   "source": [
    "# Backpropagation Walkthrough\n",
    "\n",
    "**Step-by-step manual computation** of forward and backward passes\n",
    "through a tiny 2-layer network using our from-scratch framework.\n",
    "\n",
    "We will see every matrix multiply, every activation derivative,\n",
    "and every gradient ‚Äî fully transparent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a422d286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=6, suppress=True)\n",
    "\n",
    "from src.core.activations import ReLU, Sigmoid, Softmax\n",
    "from src.core.layer import DenseLayer\n",
    "from src.core.losses import CrossEntropyLoss, MSELoss\n",
    "from src.core.initializers import he_init\n",
    "from src.validation.gradient_check import gradient_check_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19beb0a",
   "metadata": {},
   "source": [
    "## 1. Setup: Tiny Network\n",
    "\n",
    "Architecture: `Input(2) ‚Üí Dense(2, 3, ReLU) ‚Üí Dense(3, 2, Softmax)`\n",
    "\n",
    "We use a single sample for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24d7f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single input sample (1, 2)\n",
    "X = np.array([[1.0, 0.5]])\n",
    "Y = np.array([[1.0, 0.0]])  # one-hot: class 0\n",
    "\n",
    "# Layer 1: Dense(2‚Üí3) + ReLU\n",
    "layer1 = DenseLayer(2, 3, activation=ReLU(), seed=42)\n",
    "# Layer 2: Dense(3‚Üí2) + Softmax\n",
    "layer2 = DenseLayer(3, 2, activation=Softmax(), seed=43)\n",
    "\n",
    "print('W1 shape:', layer1.W.shape)\n",
    "print('W1:\\n', layer1.W)\n",
    "print('\\nW2 shape:', layer2.W.shape)\n",
    "print('W2:\\n', layer2.W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21c964c",
   "metadata": {},
   "source": [
    "## 2. Forward Pass ‚Äî Step by Step\n",
    "\n",
    "### Layer 1: Z‚ÇÅ = X @ W‚ÇÅ + b‚ÇÅ, then A‚ÇÅ = ReLU(Z‚ÇÅ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e992d2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual forward for layer 1\n",
    "Z1 = X @ layer1.W + layer1.b\n",
    "print('Z1 (pre-activation):', Z1)\n",
    "\n",
    "A1 = np.maximum(0, Z1)  # ReLU\n",
    "print('A1 (post-ReLU):    ', A1)\n",
    "\n",
    "# Verify against our framework\n",
    "A1_fw = layer1.forward(X)\n",
    "print('Framework A1:      ', A1_fw)\n",
    "np.testing.assert_allclose(A1, A1_fw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272eca65",
   "metadata": {},
   "source": [
    "### Layer 2: Z‚ÇÇ = A‚ÇÅ @ W‚ÇÇ + b‚ÇÇ, then ≈∂ = Softmax(Z‚ÇÇ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37e11a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z2 = A1 @ layer2.W + layer2.b\n",
    "print('Z2 (pre-softmax):', Z2)\n",
    "\n",
    "# Softmax\n",
    "exp_Z2 = np.exp(Z2 - Z2.max(axis=1, keepdims=True))\n",
    "Y_hat = exp_Z2 / exp_Z2.sum(axis=1, keepdims=True)\n",
    "print('≈∂  (softmax):   ', Y_hat)\n",
    "print('Sum:', Y_hat.sum())\n",
    "\n",
    "Y_hat_fw = layer2.forward(A1)\n",
    "print('Framework ≈∂:    ', Y_hat_fw)\n",
    "np.testing.assert_allclose(Y_hat, Y_hat_fw, atol=1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e26d3c",
   "metadata": {},
   "source": [
    "## 3. Loss Computation\n",
    "\n",
    "$$L = -\\frac{1}{m}\\sum Y \\cdot \\log(\\hat{Y})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ee1c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = CrossEntropyLoss()\n",
    "loss = loss_fn.forward(Y_hat, Y)\n",
    "print(f'Cross-Entropy Loss: {loss:.6f}')\n",
    "\n",
    "# Manual\n",
    "manual_loss = -np.sum(Y * np.log(Y_hat + 1e-15)) / Y.shape[0]\n",
    "print(f'Manual Loss:        {manual_loss:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4d1d76",
   "metadata": {},
   "source": [
    "## 4. Backward Pass ‚Äî Step by Step\n",
    "\n",
    "### Combined softmax + cross-entropy gradient: dZ‚ÇÇ = ≈∂ ‚àí Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fc6f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dZ2 = Y_hat - Y (combined gradient)\n",
    "dZ2 = loss_fn.backward()\n",
    "print('dZ2 (≈∂ - Y):', dZ2)\n",
    "print('Manual:      ', Y_hat - Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d836216",
   "metadata": {},
   "source": [
    "### Layer 2 backward:\n",
    "\n",
    "- $dW_2 = \\frac{1}{m} A_1^T \\cdot dZ_2$\n",
    "- $db_2 = \\frac{1}{m} \\sum dZ_2$\n",
    "- $dA_1 = dZ_2 \\cdot W_2^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f77b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dA1 = layer2.backward(dZ2)\n",
    "print('dW2:', layer2.dW)\n",
    "print('db2:', layer2.db)\n",
    "print('dA1 (to pass back):', dA1)\n",
    "\n",
    "# Manual verification\n",
    "m = X.shape[0]\n",
    "dW2_manual = A1.T @ dZ2 / m\n",
    "print('\\nManual dW2:', dW2_manual)\n",
    "np.testing.assert_allclose(layer2.dW, dW2_manual, atol=1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c551273f",
   "metadata": {},
   "source": [
    "### Layer 1 backward (with ReLU derivative):\n",
    "\n",
    "- $dZ_1 = dA_1 \\odot \\mathbb{1}(Z_1 > 0)$\n",
    "- $dW_1 = \\frac{1}{m} X^T \\cdot dZ_1$\n",
    "- $db_1 = \\frac{1}{m} \\sum dZ_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e384fb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "dX = layer1.backward(dA1)\n",
    "print('dW1:', layer1.dW)\n",
    "print('db1:', layer1.db)\n",
    "print('dX (input grad):', dX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4be57de",
   "metadata": {},
   "source": [
    "## 5. Gradient Checking\n",
    "\n",
    "Verify our analytical gradients vs. numerical approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fba8976",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Layer 1 gradient check:')\n",
    "errors1 = gradient_check_layer(layer1, X, dA1, verbose=True)\n",
    "\n",
    "print('\\nLayer 2 gradient check:')\n",
    "errors2 = gradient_check_layer(layer2, A1_fw, dZ2, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae73739",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We traced every single computation:\n",
    "\n",
    "1. **Forward**: `X ‚Üí Z‚ÇÅ = XW‚ÇÅ + b‚ÇÅ ‚Üí A‚ÇÅ = ReLU(Z‚ÇÅ) ‚Üí Z‚ÇÇ = A‚ÇÅW‚ÇÇ + b‚ÇÇ ‚Üí ≈∂ = Softmax(Z‚ÇÇ)`\n",
    "2. **Loss**: `L = -Œ£ Y¬∑log(≈∂) / m`\n",
    "3. **Backward**: `dZ‚ÇÇ = ≈∂ - Y ‚Üí dW‚ÇÇ, db‚ÇÇ, dA‚ÇÅ ‚Üí dZ‚ÇÅ = dA‚ÇÅ ‚äô ùüô(Z‚ÇÅ>0) ‚Üí dW‚ÇÅ, db‚ÇÅ`\n",
    "\n",
    "Every gradient was verified against numerical differentiation. ‚úÖ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
