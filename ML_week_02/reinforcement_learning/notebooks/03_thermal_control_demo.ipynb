{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c4b687e",
   "metadata": {},
   "source": [
    "# Stage 3 — Thermal Control: DQN vs PID\n",
    "\n",
    "This notebook demonstrates the custom **ThermalControlEnv** — a Gymnasium\n",
    "environment simulating avionics thermal management.\n",
    "\n",
    "**Scenario:** An electronic box in an aircraft generates heat from a CPU.\n",
    "The agent controls a fan (5 discrete speed levels) to keep the temperature\n",
    "in a safe band (50–60°C) while minimising energy consumption.\n",
    "\n",
    "We compare:\n",
    "1. **Classical PID controller** (hand-tuned baseline)\n",
    "2. **Trained DQN agent** (learned optimal policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df739be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.environments.thermal_control_env import ThermalControlEnv\n",
    "from src.control.pid_controller import PIDController\n",
    "from src.control.control_utils import (\n",
    "    compute_settling_time, compute_overshoot,\n",
    "    compute_steady_state_error, compute_integral_absolute_error,\n",
    "    compute_energy_cost,\n",
    ")\n",
    "from src.agents.ddqn_agent import DDQNAgent\n",
    "from src.utils.config_loader import load_config, get_device\n",
    "from src.utils.plotting import plot_thermal_trajectory, plot_controller_comparison\n",
    "\n",
    "print(f\"PyTorch {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7438755",
   "metadata": {},
   "source": [
    "## 1. Explore the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dd31c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(PROJECT_ROOT / \"config\" / \"thermal_control.yaml\")\n",
    "device = get_device(config)\n",
    "\n",
    "env = ThermalControlEnv(config=config)\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "print(f\"Action space:      {env.action_space} (fan levels)\")\n",
    "print(f\"\\nThermal parameters:\")\n",
    "print(f\"  Target temp:    {env.target_temp}°C ± {env.temp_tolerance}°C\")\n",
    "print(f\"  Critical temp:  {env.critical_temp}°C\")\n",
    "print(f\"  Thermal mass:   {env.thermal_mass} J/°C\")\n",
    "print(f\"  Thermal R:      {env.thermal_resistance} °C/W\")\n",
    "print(f\"  Fan cooling:    {env.fan_cooling} W\")\n",
    "print(f\"  Fan cost:       {env.fan_cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d087e6",
   "metadata": {},
   "source": [
    "## 2. Random Agent Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c477acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "state, info = env.reset(seed=42)\n",
    "random_temps, random_fans, random_rewards = [], [], []\n",
    "\n",
    "for _ in range(500):\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "    random_temps.append(info[\"temperature\"])\n",
    "    random_fans.append(info[\"fan_level\"])\n",
    "    random_rewards.append(reward)\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "print(f\"Random agent: {len(random_temps)} steps, \"\n",
    "      f\"total reward = {sum(random_rewards):.1f}, \"\n",
    "      f\"temp range = [{min(random_temps):.1f}, {max(random_temps):.1f}]°C\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 6), sharex=True)\n",
    "ax1.plot(random_temps, color=\"tomato\")\n",
    "ax1.axhline(55, color=\"green\", linestyle=\"--\", alpha=0.6)\n",
    "ax1.axhspan(50, 60, alpha=0.1, color=\"green\")\n",
    "ax1.set_ylabel(\"Temperature (°C)\")\n",
    "ax1.set_title(\"Random Agent\")\n",
    "ax2.step(range(len(random_fans)), random_fans, color=\"steelblue\", where=\"post\")\n",
    "ax2.set_ylabel(\"Fan Level\")\n",
    "ax2.set_xlabel(\"Step\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed1af96",
   "metadata": {},
   "source": [
    "## 3. PID Controller Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31466ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pid = PIDController.from_config(config)\n",
    "state, info = env.reset(seed=42)\n",
    "pid.reset()\n",
    "\n",
    "pid_traj = {\"states\": [state.copy()], \"actions\": [], \"rewards\": [], \"infos\": [info]}\n",
    "\n",
    "for _ in range(500):\n",
    "    temperature = state[0]\n",
    "    action = pid.compute_action(temperature)\n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "    pid_traj[\"actions\"].append(action)\n",
    "    pid_traj[\"rewards\"].append(reward)\n",
    "    pid_traj[\"states\"].append(state.copy())\n",
    "    pid_traj[\"infos\"].append(info)\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "pid_temps = np.array([i[\"temperature\"] for i in pid_traj[\"infos\"]])\n",
    "pid_total = sum(pid_traj[\"rewards\"])\n",
    "print(f\"PID: {len(pid_traj['actions'])} steps, total reward = {pid_total:.1f}\")\n",
    "print(f\"  Temp range: [{pid_temps.min():.1f}, {pid_temps.max():.1f}]°C\")\n",
    "\n",
    "fig = plot_thermal_trajectory(pid_traj, config, title=\"PID Controller\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8528dc8b",
   "metadata": {},
   "source": [
    "## 4. Load Trained DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64711e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DDQNAgent(config, device)\n",
    "\n",
    "ckpt_path = PROJECT_ROOT / \"outputs\" / \"models\" / \"thermal_control\" / \"checkpoint_best.pt\"\n",
    "if ckpt_path.exists():\n",
    "    ckpt = agent.load(ckpt_path)\n",
    "    agent.epsilon = 0.0\n",
    "    print(f\"Loaded checkpoint from episode {ckpt.get('episode', '?')}\")\n",
    "    print(f\"Best eval reward: {ckpt.get('best_eval_reward', 'N/A')}\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Train first:\")\n",
    "    print(\"  python train.py --config config/thermal_control.yaml\")\n",
    "\n",
    "print(f\"\\nNetwork:\\n{agent.online_net}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98436039",
   "metadata": {},
   "source": [
    "## 5. DQN Agent Rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1804fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "state, info = env.reset(seed=42)\n",
    "dqn_traj = {\"states\": [state.copy()], \"actions\": [], \"rewards\": [], \"infos\": [info]}\n",
    "\n",
    "for _ in range(500):\n",
    "    action = agent.select_action(state, eval_mode=True)\n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "    dqn_traj[\"actions\"].append(action)\n",
    "    dqn_traj[\"rewards\"].append(reward)\n",
    "    dqn_traj[\"states\"].append(state.copy())\n",
    "    dqn_traj[\"infos\"].append(info)\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "dqn_temps = np.array([i[\"temperature\"] for i in dqn_traj[\"infos\"]])\n",
    "dqn_total = sum(dqn_traj[\"rewards\"])\n",
    "print(f\"DQN: {len(dqn_traj['actions'])} steps, total reward = {dqn_total:.1f}\")\n",
    "print(f\"  Temp range: [{dqn_temps.min():.1f}, {dqn_temps.max():.1f}]°C\")\n",
    "\n",
    "fig = plot_thermal_trajectory(dqn_traj, config, title=\"DQN Agent\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc2fb8c",
   "metadata": {},
   "source": [
    "## 6. Head-to-Head Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e926af44",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_controller_comparison(dqn_traj, pid_traj, config)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f6a686",
   "metadata": {},
   "source": [
    "## 7. Quantitative Control Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55be4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "thermal_cfg = config.get(\"environment\", {}).get(\"thermal\", {})\n",
    "target = thermal_cfg.get(\"target_temp\", 55.0)\n",
    "dt = thermal_cfg.get(\"dt\", 1.0)\n",
    "fan_cost_table = thermal_cfg.get(\"fan_energy_cost\", [0, 0.5, 1.5, 3.5, 7.0])\n",
    "\n",
    "print(f\"{'Metric':<30} {'DQN':>12} {'PID':>12}\")\n",
    "print(\"─\" * 55)\n",
    "\n",
    "for label, traj in [(\"DQN\", dqn_traj), (\"PID\", pid_traj)]:\n",
    "    temps = np.array([i[\"temperature\"] for i in traj[\"infos\"]])\n",
    "    actions = np.array(traj[\"actions\"])\n",
    "    pass  # just for structure\n",
    "\n",
    "# Compute and display\n",
    "metrics = {}\n",
    "for label, traj in [(\"DQN\", dqn_traj), (\"PID\", pid_traj)]:\n",
    "    temps = np.array([i[\"temperature\"] for i in traj[\"infos\"]])\n",
    "    actions = np.array(traj[\"actions\"])\n",
    "    metrics[label] = {\n",
    "        \"Total Reward\": sum(traj[\"rewards\"]),\n",
    "        \"Settling Time (s)\": compute_settling_time(temps, target, 0.05, dt) or float(\"nan\"),\n",
    "        \"Overshoot (%)\": compute_overshoot(temps, target),\n",
    "        \"Steady-State Error (°C)\": compute_steady_state_error(temps, target),\n",
    "        \"IAE\": compute_integral_absolute_error(temps, target, dt),\n",
    "        \"Energy Cost\": compute_energy_cost(actions, fan_cost_table),\n",
    "        \"In-Band %\": 100 * np.mean(np.abs(temps - target) <= 5.0),\n",
    "    }\n",
    "\n",
    "for metric_name in metrics[\"DQN\"]:\n",
    "    dqn_val = metrics[\"DQN\"][metric_name]\n",
    "    pid_val = metrics[\"PID\"][metric_name]\n",
    "    print(f\"{metric_name:<30} {dqn_val:>12.2f} {pid_val:>12.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a87958",
   "metadata": {},
   "source": [
    "## 8. Environment Dynamics Exploration\n",
    "\n",
    "Visualise how the thermal system responds to fixed fan levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c802449",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for fan_level in range(5):\n",
    "    state, _ = env.reset(seed=0)\n",
    "    temps = [state[0]]\n",
    "    for _ in range(200):\n",
    "        state, _, terminated, truncated, _ = env.step(fan_level)\n",
    "        temps.append(state[0])\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    axes[0].plot(temps, label=f\"Fan {fan_level}\")\n",
    "\n",
    "axes[0].axhline(55, color=\"green\", linestyle=\"--\", alpha=0.5)\n",
    "axes[0].axhspan(50, 60, alpha=0.08, color=\"green\")\n",
    "axes[0].set_xlabel(\"Step\")\n",
    "axes[0].set_ylabel(\"Temperature (°C)\")\n",
    "axes[0].set_title(\"Step Response per Fan Level\")\n",
    "axes[0].legend(fontsize=9)\n",
    "\n",
    "# Workload profile\n",
    "steps = np.arange(500)\n",
    "freq = thermal_cfg.get(\"workload_frequency\", 0.02)\n",
    "base = thermal_cfg.get(\"heat_generation_base\", 30.0)\n",
    "var = thermal_cfg.get(\"heat_generation_var\", 15.0)\n",
    "workload = base + var * np.sin(2 * np.pi * freq * steps)\n",
    "axes[1].plot(steps, workload, color=\"orange\")\n",
    "axes[1].set_xlabel(\"Step\")\n",
    "axes[1].set_ylabel(\"Heat Generation (W)\")\n",
    "axes[1].set_title(\"Workload Profile (sinusoidal)\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
