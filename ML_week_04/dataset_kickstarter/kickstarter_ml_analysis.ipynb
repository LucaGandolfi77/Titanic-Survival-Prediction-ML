{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ Kickstarter Projects â€” ML Analysis\n",
    "\n",
    "**Dataset**: 374,853 crowdfunding projects (2009â€“2018), 15 categories, 22 countries\n",
    "\n",
    "## ML Tasks\n",
    "| # | Task | Type | Target |\n",
    "|---|------|------|--------|\n",
    "| 1 | Project Success Prediction | Binary Classification | Successful vs Failed |\n",
    "| 2 | Pledged Amount Regression | Regression | Log(USD pledged + 1) |\n",
    "| 3 | Backer Count Regression | Regression | Log(backers + 1) |\n",
    "| 4 | Project Category Clustering | Unsupervised | K-Means / DBSCAN |"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import warnings, os, base64, io, pathlib\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from jinja2 import Template\n",
    "\n",
    "from sklearn.model_selection import (train_test_split, cross_val_score,\n",
    "                                     GridSearchCV, RandomizedSearchCV,\n",
    "                                     learning_curve, StratifiedKFold)\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import (RandomForestClassifier, RandomForestRegressor,\n",
    "                              GradientBoostingClassifier, GradientBoostingRegressor,\n",
    "                              AdaBoostClassifier, VotingClassifier, StackingClassifier)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, confusion_matrix, classification_report,\n",
    "                             mean_absolute_error, mean_squared_error, r2_score)\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams.update({'figure.max_open_warning': 0, 'figure.dpi': 120})\n",
    "\n",
    "PLOT_DIR = pathlib.Path('outputs/plots')\n",
    "PLOT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "all_results = {}\n",
    "saved_plots = []\n",
    "\n",
    "def save(fig, name, title=None):\n",
    "    p = PLOT_DIR / name\n",
    "    fig.savefig(p, bbox_inches='tight', facecolor='white')\n",
    "    plt.close(fig)\n",
    "    saved_plots.append((title or name.replace('.png','').replace('_',' ').title(), str(p)))\n",
    "    print(f'  âœ“ {name}')\n",
    "\n",
    "import sklearn\n",
    "print(f'All imports successful')\n",
    "print(f'  scikit-learn {sklearn.__version__}')\n",
    "print(f'  pandas {pd.__version__}, numpy {np.__version__}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Â· Data Loading & Exploration"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "DATA = pathlib.Path('Kickstarter+Projects')\n",
    "raw = pd.read_csv(DATA / 'kickstarter_projects.csv')\n",
    "\n",
    "print(f'Raw dataset: {raw.shape[0]:,} rows Ã— {raw.shape[1]} columns')\n",
    "print(f'Columns: {list(raw.columns)}')\n",
    "print(f'\\nState distribution:')\n",
    "print(raw['State'].value_counts().to_string())\n",
    "print(f'\\nNumeric summary:')\n",
    "print(raw[['Goal', 'Pledged', 'Backers']].describe().round(1).to_string())\n",
    "print(f'\\nCategories: {raw[\"Category\"].nunique()}, Subcategories: {raw[\"Subcategory\"].nunique()}')\n",
    "print(f'Countries: {raw[\"Country\"].nunique()}')\n",
    "print(f'Missing values: {raw.isnull().sum().sum()}')\n",
    "raw.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Â· Data Cleaning & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df = raw.copy()\n",
    "\n",
    "# â”€â”€ Parse dates â”€â”€\n",
    "df['Launched'] = pd.to_datetime(df['Launched'])\n",
    "df['Deadline'] = pd.to_datetime(df['Deadline'])\n",
    "\n",
    "# â”€â”€ Campaign duration in days â”€â”€\n",
    "df['campaign_days'] = (df['Deadline'] - df['Launched']).dt.total_seconds() / 86400\n",
    "df['campaign_days'] = df['campaign_days'].clip(lower=1)  # minimum 1 day\n",
    "\n",
    "# â”€â”€ Time features â”€â”€\n",
    "df['launch_year'] = df['Launched'].dt.year\n",
    "df['launch_month'] = df['Launched'].dt.month\n",
    "df['launch_dow'] = df['Launched'].dt.dayofweek   # 0=Mon, 6=Sun\n",
    "df['launch_hour'] = df['Launched'].dt.hour\n",
    "\n",
    "# â”€â”€ Log transforms (highly skewed numerics) â”€â”€\n",
    "df['log_goal'] = np.log1p(df['Goal'])\n",
    "df['log_pledged'] = np.log1p(df['Pledged'])\n",
    "df['log_backers'] = np.log1p(df['Backers'])\n",
    "\n",
    "# â”€â”€ Name length as proxy for project description detail â”€â”€\n",
    "df['name_length'] = df['Name'].fillna('').str.len()\n",
    "df['name_word_count'] = df['Name'].fillna('').str.split().str.len()\n",
    "\n",
    "# â”€â”€ Category encoding â”€â”€\n",
    "le_cat = LabelEncoder()\n",
    "df['category_enc'] = le_cat.fit_transform(df['Category'])\n",
    "cat_classes = list(le_cat.classes_)\n",
    "\n",
    "le_country = LabelEncoder()\n",
    "df['country_enc'] = le_country.fit_transform(df['Country'])\n",
    "\n",
    "# â”€â”€ Binary target: focus on Successful vs Failed (drop others) â”€â”€\n",
    "df_binary = df[df['State'].isin(['Successful', 'Failed'])].copy()\n",
    "df_binary['success'] = (df_binary['State'] == 'Successful').astype(int)\n",
    "\n",
    "print(f'Full dataset with features: {df.shape}')\n",
    "print(f'Binary (Successful/Failed): {len(df_binary):,} rows')\n",
    "print(f'  Successful: {df_binary[\"success\"].sum():,} ({df_binary[\"success\"].mean():.1%})')\n",
    "print(f'  Failed:     {(1-df_binary[\"success\"]).sum():,} ({1-df_binary[\"success\"].mean():.1%})')\n",
    "print(f'\\nEngineered features: campaign_days, launch_year/month/dow/hour, '\n",
    "      f'log_goal, name_length, name_word_count, category_enc, country_enc')\n",
    "print(f'\\nCampaign days: mean={df_binary[\"campaign_days\"].mean():.1f}, '\n",
    "      f'median={df_binary[\"campaign_days\"].median():.1f}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Â· Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# â”€â”€ Plot 1: Success rate by category â”€â”€\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "cat_success = df_binary.groupby('Category')['success'].agg(['mean', 'count']).sort_values('mean', ascending=True)\n",
    "colors = plt.cm.RdYlGn(cat_success['mean'])\n",
    "bars = ax.barh(cat_success.index, cat_success['mean'] * 100, color=colors, edgecolor='white')\n",
    "ax.set_xlabel('Success Rate (%)')\n",
    "ax.set_title('Kickstarter Success Rate by Category')\n",
    "for i, (rate, count) in enumerate(zip(cat_success['mean'], cat_success['count'])):\n",
    "    ax.text(rate * 100 + 0.5, i, f'{rate:.0%} ({count:,})', va='center', fontsize=8)\n",
    "ax.axvline(x=df_binary['success'].mean() * 100, color='red', linestyle='--', alpha=0.7, label='Overall avg')\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "save(fig, 'success_by_category.png', 'Success Rate by Category')\n",
    "\n",
    "# â”€â”€ Plot 2: Goal distribution â€” success vs failed â”€â”€\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "for i, state in enumerate(['Successful', 'Failed']):\n",
    "    subset = df_binary[df_binary['State'] == state]\n",
    "    axes[i].hist(subset['log_goal'], bins=50, color='#2E86AB' if i == 0 else '#E74C3C',\n",
    "                 alpha=0.8, edgecolor='white')\n",
    "    axes[i].set_xlabel('Log(Goal + 1)')\n",
    "    axes[i].set_ylabel('Count')\n",
    "    axes[i].set_title(f'{state} Projects â€” Goal Distribution')\n",
    "    axes[i].axvline(subset['log_goal'].median(), color='black', linestyle='--',\n",
    "                     label=f'Median: ${np.expm1(subset[\"log_goal\"].median()):,.0f}')\n",
    "    axes[i].legend()\n",
    "fig.tight_layout()\n",
    "save(fig, 'goal_distribution.png', 'Goal Distribution â€” Success vs Failed')\n",
    "\n",
    "# â”€â”€ Plot 3: Projects over time â”€â”€\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "monthly = df_binary.groupby([df_binary['Launched'].dt.to_period('Q')])['success'].agg(['count', 'mean'])\n",
    "ax.bar(range(len(monthly)), monthly['count'], color='#2E86AB', alpha=0.6, label='Total Projects')\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(range(len(monthly)), monthly['mean'] * 100, 'o-', color='#F18F01', linewidth=2, label='Success Rate %')\n",
    "ax.set_xlabel('Quarter')\n",
    "ax.set_ylabel('Number of Projects', color='#2E86AB')\n",
    "ax2.set_ylabel('Success Rate %', color='#F18F01')\n",
    "xtick_positions = range(0, len(monthly), max(1, len(monthly)//10))\n",
    "ax.set_xticks(list(xtick_positions))\n",
    "ax.set_xticklabels([str(monthly.index[i]) for i in xtick_positions], rotation=45, fontsize=8)\n",
    "lines1, labels1 = ax.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "ax.set_title('Kickstarter Projects Over Time')\n",
    "fig.tight_layout()\n",
    "save(fig, 'projects_over_time.png', 'Projects Over Time')\n",
    "\n",
    "# â”€â”€ Plot 4: Success rate by campaign duration â”€â”€\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "dur_bins = pd.cut(df_binary['campaign_days'], bins=[0, 7, 15, 30, 45, 60, 90, 365])\n",
    "dur_success = df_binary.groupby(dur_bins, observed=True)['success'].agg(['mean', 'count'])\n",
    "dur_success.plot.bar(y='mean', ax=ax, color='#2E86AB', edgecolor='white', legend=False)\n",
    "ax.set_xlabel('Campaign Duration (days)')\n",
    "ax.set_ylabel('Success Rate')\n",
    "ax.set_title('Success Rate by Campaign Duration')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "for i, (rate, count) in enumerate(zip(dur_success['mean'], dur_success['count'])):\n",
    "    ax.text(i, rate + 0.01, f'{rate:.0%}\\n({count:,})', ha='center', fontsize=7)\n",
    "fig.tight_layout()\n",
    "save(fig, 'success_by_duration.png', 'Success Rate by Campaign Duration')\n",
    "\n",
    "# â”€â”€ Plot 5: Top 10 countries â”€â”€\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "top_countries = df_binary.groupby('Country')['success'].agg(['mean', 'count']).nlargest(15, 'count')\n",
    "top_countries = top_countries.sort_values('mean', ascending=True)\n",
    "ax.barh(top_countries.index, top_countries['mean'] * 100,\n",
    "        color=plt.cm.viridis(np.linspace(0.2, 0.9, len(top_countries))), edgecolor='white')\n",
    "ax.set_xlabel('Success Rate (%)')\n",
    "ax.set_title('Success Rate by Country (top 15 by volume)')\n",
    "for i, (rate, count) in enumerate(zip(top_countries['mean'], top_countries['count'])):\n",
    "    ax.text(rate * 100 + 0.3, i, f'{rate:.0%} ({count:,})', va='center', fontsize=8)\n",
    "fig.tight_layout()\n",
    "save(fig, 'success_by_country.png', 'Success Rate by Country')\n",
    "\n",
    "# â”€â”€ Plot 6: Correlation heatmap â”€â”€\n",
    "num_cols = ['Goal', 'log_goal', 'campaign_days', 'name_length', 'name_word_count',\n",
    "            'launch_year', 'launch_month', 'launch_dow', 'launch_hour',\n",
    "            'category_enc', 'country_enc', 'success']\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "corr = df_binary[num_cols].corr()\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "sns.heatmap(corr, mask=mask, annot=True, fmt='.2f', cmap='RdBu_r',\n",
    "            center=0, square=True, linewidths=0.5, ax=ax, annot_kws={'size': 7})\n",
    "ax.set_title('Feature Correlation Heatmap')\n",
    "fig.tight_layout()\n",
    "save(fig, 'correlation_heatmap.png', 'Feature Correlation Heatmap')\n",
    "\n",
    "print('All EDA plots saved!')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Â· Task 1 â€” Project Success Prediction (Binary Classification)\n",
    "Predict whether a Kickstarter project will be **Successful** or **Failed** using only pre-launch features\n",
    "(goal, category, country, campaign duration, name length, launch timing)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# â”€â”€ Pre-launch features only (no pledged/backers â€” those are post-launch!) â”€â”€\n",
    "clf_features = ['log_goal', 'campaign_days', 'category_enc', 'country_enc',\n",
    "                'launch_year', 'launch_month', 'launch_dow', 'launch_hour',\n",
    "                'name_length', 'name_word_count']\n",
    "\n",
    "X_clf = df_binary[clf_features].astype(float)\n",
    "y_clf = df_binary['success']\n",
    "\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n",
    "    X_clf, y_clf, test_size=0.2, random_state=SEED, stratify=y_clf)\n",
    "\n",
    "scaler_c = StandardScaler()\n",
    "X_train_cs = scaler_c.fit_transform(X_train_c)\n",
    "X_test_cs = scaler_c.transform(X_test_c)\n",
    "\n",
    "print(f'Classification dataset: {len(X_clf):,} rows')\n",
    "print(f'Train: {len(X_train_c):,}  |  Test: {len(X_test_c):,}')\n",
    "print(f'Positive rate: {y_clf.mean():.1%}')\n",
    "print(f'Features: {clf_features}')\n",
    "\n",
    "# â”€â”€ Use a sample for slow models (SVM, MLP) â”€â”€\n",
    "SAMPLE_SIZE = 50000\n",
    "if len(X_train_c) > SAMPLE_SIZE:\n",
    "    idx_sample = np.random.choice(len(X_train_c), SAMPLE_SIZE, replace=False)\n",
    "    X_train_sample = X_train_c.iloc[idx_sample]\n",
    "    y_train_sample = y_train_c.iloc[idx_sample]\n",
    "    X_train_sample_s = scaler_c.transform(X_train_sample)\n",
    "    print(f'Sampled {SAMPLE_SIZE:,} rows for slow models (SVM, MLP)')\n",
    "else:\n",
    "    X_train_sample = X_train_c\n",
    "    y_train_sample = y_train_c\n",
    "    X_train_sample_s = X_train_cs\n",
    "\n",
    "def eval_clf(name, model, Xtr, ytr, Xte, yte):\n",
    "    model.fit(Xtr, ytr)\n",
    "    pred = model.predict(Xte)\n",
    "    acc = accuracy_score(yte, pred)\n",
    "    f1 = f1_score(yte, pred, average='weighted')\n",
    "    prec = precision_score(yte, pred, average='weighted')\n",
    "    rec = recall_score(yte, pred, average='weighted')\n",
    "    return {'Accuracy': acc, 'Precision': prec, 'Recall': rec, 'F1 Score': f1,\n",
    "            'model': model, 'predictions': pred}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "clf_results = {}\n",
    "\n",
    "# â”€â”€ Fast models on full training data â”€â”€\n",
    "fast_classifiers = [\n",
    "    ('Logistic Regression', LogisticRegression(max_iter=1000, random_state=SEED), True, False),\n",
    "    ('Decision Tree', DecisionTreeClassifier(max_depth=12, random_state=SEED), False, False),\n",
    "    ('Random Forest', RandomForestClassifier(n_estimators=200, max_depth=15, random_state=SEED, n_jobs=-1), False, False),\n",
    "    ('Gradient Boosting', GradientBoostingClassifier(n_estimators=200, max_depth=5, random_state=SEED), False, False),\n",
    "    ('AdaBoost', AdaBoostClassifier(n_estimators=100, random_state=SEED), False, False),\n",
    "    ('Naive Bayes', GaussianNB(), True, False),\n",
    "]\n",
    "\n",
    "# â”€â”€ Slow models on sampled data â”€â”€\n",
    "slow_classifiers = [\n",
    "    ('SVM (linear)', SVC(kernel='linear', random_state=SEED), True, True),\n",
    "    ('SVM (rbf)', SVC(kernel='rbf', random_state=SEED), True, True),\n",
    "    ('MLP Neural Network', MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=300,\n",
    "                                          random_state=SEED, early_stopping=True), True, True),\n",
    "]\n",
    "\n",
    "print('Project Success Classification')\n",
    "print('=' * 65)\n",
    "\n",
    "for name, model, needs_scale, use_sample in fast_classifiers + slow_classifiers:\n",
    "    if use_sample:\n",
    "        Xtr = X_train_sample_s if needs_scale else X_train_sample\n",
    "        ytr = y_train_sample\n",
    "    else:\n",
    "        Xtr = X_train_cs if needs_scale else X_train_c\n",
    "        ytr = y_train_c\n",
    "    Xte = X_test_cs if needs_scale else X_test_c\n",
    "    r = eval_clf(name, model, Xtr, ytr, Xte, y_test_c)\n",
    "    clf_results[name] = r\n",
    "    print(f'  {name:30s} Acc={r[\"Accuracy\"]:.4f}  F1={r[\"F1 Score\"]:.4f}')\n",
    "\n",
    "# KNN with k search (on sample)\n",
    "k_scores = {}\n",
    "for k in [3, 5, 7, 9, 11]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, n_jobs=-1)\n",
    "    knn.fit(X_train_sample_s, y_train_sample)\n",
    "    k_scores[k] = accuracy_score(y_test_c, knn.predict(X_test_cs))\n",
    "best_k = max(k_scores, key=k_scores.get)\n",
    "print(f'  KNN by k: {k_scores} -> best k={best_k}')\n",
    "\n",
    "r = eval_clf(f'KNN (k={best_k})', KNeighborsClassifier(n_neighbors=best_k, n_jobs=-1),\n",
    "             X_train_sample_s, y_train_sample, X_test_cs, y_test_c)\n",
    "clf_results[f'KNN (k={best_k})'] = r\n",
    "print(f'  {\"KNN (k=\"+str(best_k)+\")\":30s} Acc={r[\"Accuracy\"]:.4f}  F1={r[\"F1 Score\"]:.4f}')\n",
    "\n",
    "print(f'\\nAll {len(clf_results)} classifiers trained!')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Â· Task 2 â€” Pledged Amount Regression\n",
    "Predict the **log-transformed pledged amount** for successful projects using goal, category, country, and campaign features."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# â”€â”€ Regression: predict log(pledged) for successful projects â”€â”€\n",
    "succ = df_binary[df_binary['success'] == 1].copy()\n",
    "\n",
    "reg_features = ['log_goal', 'campaign_days', 'category_enc', 'country_enc',\n",
    "                'launch_year', 'launch_month', 'launch_dow', 'launch_hour',\n",
    "                'name_length', 'name_word_count']\n",
    "\n",
    "X_reg = succ[reg_features].astype(float)\n",
    "y_reg = succ['log_pledged']\n",
    "\n",
    "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=SEED)\n",
    "\n",
    "scaler_r = StandardScaler()\n",
    "X_train_rs = scaler_r.fit_transform(X_train_r)\n",
    "X_test_rs = scaler_r.transform(X_test_r)\n",
    "\n",
    "print(f'Regression dataset (successful projects): {len(X_reg):,} rows')\n",
    "print(f'Train: {len(X_train_r):,}  |  Test: {len(X_test_r):,}')\n",
    "print(f'Target: log(pledged+1), range [{y_reg.min():.2f}, {y_reg.max():.2f}]')\n",
    "\n",
    "regressors = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(alpha=1.0),\n",
    "    'Lasso Regression': Lasso(alpha=0.01),\n",
    "    'Decision Tree': DecisionTreeRegressor(max_depth=10, random_state=SEED),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=12, random_state=SEED, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=200, max_depth=5, random_state=SEED),\n",
    "}\n",
    "\n",
    "reg_results = {}\n",
    "print(f'\\nPledged Amount Regression:')\n",
    "print('=' * 70)\n",
    "for name, model in regressors.items():\n",
    "    use_scaled = name in ['Ridge Regression', 'Lasso Regression']\n",
    "    Xtr = X_train_rs if use_scaled else X_train_r\n",
    "    Xte = X_test_rs if use_scaled else X_test_r\n",
    "    model.fit(Xtr, y_train_r)\n",
    "    pred = model.predict(Xte)\n",
    "    r2 = r2_score(y_test_r, pred)\n",
    "    mae = mean_absolute_error(y_test_r, pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_r, pred))\n",
    "    reg_results[name] = {'RÂ²': r2, 'MAE': mae, 'RMSE': rmse, 'model': model}\n",
    "    print(f'  {name:30s} RÂ²={r2:.4f}  MAE={mae:.2f}  RMSE={rmse:.2f}')\n",
    "\n",
    "# â”€â”€ Plots â”€â”€\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "names = list(reg_results.keys())\n",
    "r2s = [reg_results[n]['RÂ²'] for n in names]\n",
    "maes = [reg_results[n]['MAE'] for n in names]\n",
    "\n",
    "axes[0].barh(names, r2s, color='#2E86AB', edgecolor='white')\n",
    "axes[0].set_xlabel('RÂ² Score'); axes[0].set_title('Regression â€” RÂ² Comparison')\n",
    "for i, v in enumerate(r2s):\n",
    "    axes[0].text(max(v + 0.01, 0.01), i, f'{v:.4f}', va='center', fontsize=8)\n",
    "\n",
    "axes[1].barh(names, maes, color='#F18F01', edgecolor='white')\n",
    "axes[1].set_xlabel('MAE'); axes[1].set_title('Regression â€” MAE Comparison')\n",
    "for i, v in enumerate(maes):\n",
    "    axes[1].text(v + 0.02, i, f'{v:.2f}', va='center', fontsize=8)\n",
    "fig.tight_layout()\n",
    "save(fig, 'regression_results.png', 'Pledged Amount Regression Results')\n",
    "\n",
    "# Feature importance\n",
    "best_tree_name = max(['Decision Tree', 'Random Forest', 'Gradient Boosting'],\n",
    "                      key=lambda n: reg_results[n]['RÂ²'])\n",
    "best_tree = reg_results[best_tree_name]['model']\n",
    "imp = pd.Series(best_tree.feature_importances_, index=reg_features).sort_values()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "imp.plot.barh(ax=ax, color='#2E86AB', edgecolor='white')\n",
    "ax.set_xlabel('Feature Importance'); ax.set_title(f'Regression Feature Importance ({best_tree_name})')\n",
    "fig.tight_layout()\n",
    "save(fig, 'regression_feature_importance.png', 'Regression Feature Importance')\n",
    "\n",
    "all_results['regression'] = {k: {m: v for m, v in v.items() if m != 'model'}\n",
    "                              for k, v in reg_results.items()}\n",
    "best_reg = max(reg_results, key=lambda n: reg_results[n]['RÂ²'])\n",
    "print(f'\\nBest: {best_reg} (RÂ² = {reg_results[best_reg][\"RÂ²\"]:.4f})')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Â· Task 3 â€” Backer Count Regression\n",
    "Predict the **log-transformed number of backers** using pre-launch features."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# â”€â”€ Backer count regression (all projects) â”€â”€\n",
    "backer_features = ['log_goal', 'campaign_days', 'category_enc', 'country_enc',\n",
    "                   'launch_year', 'launch_month', 'launch_dow', 'launch_hour',\n",
    "                   'name_length', 'name_word_count', 'success']\n",
    "\n",
    "X_bk = df_binary[backer_features].astype(float)\n",
    "y_bk = df_binary['log_backers']\n",
    "\n",
    "X_train_b, X_test_b, y_train_b, y_test_b = train_test_split(\n",
    "    X_bk, y_bk, test_size=0.2, random_state=SEED)\n",
    "\n",
    "print(f'Backer regression dataset: {len(X_bk):,} rows')\n",
    "print(f'Train: {len(X_train_b):,}  |  Test: {len(X_test_b):,}')\n",
    "\n",
    "backer_regressors = {\n",
    "    'Linear': LinearRegression(),\n",
    "    'Ridge': Ridge(alpha=1.0),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=12, random_state=SEED, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=200, max_depth=5, random_state=SEED),\n",
    "}\n",
    "\n",
    "backer_results = {}\n",
    "print(f'\\nBacker Count Regression:')\n",
    "print('=' * 70)\n",
    "for name, model in backer_regressors.items():\n",
    "    model.fit(X_train_b, y_train_b)\n",
    "    pred = model.predict(X_test_b)\n",
    "    r2 = r2_score(y_test_b, pred)\n",
    "    mae = mean_absolute_error(y_test_b, pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_b, pred))\n",
    "    backer_results[name] = {'RÂ²': r2, 'MAE': mae, 'RMSE': rmse}\n",
    "    print(f'  {name:30s} RÂ²={r2:.4f}  MAE={mae:.2f}  RMSE={rmse:.2f}')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "names_b = list(backer_results.keys())\n",
    "r2s_b = [backer_results[n]['RÂ²'] for n in names_b]\n",
    "ax.barh(names_b, r2s_b, color='#2E86AB', edgecolor='white')\n",
    "ax.set_xlabel('RÂ² Score'); ax.set_title('Backer Count Regression â€” RÂ² Comparison')\n",
    "for i, v in enumerate(r2s_b):\n",
    "    ax.text(max(v + 0.01, 0.01), i, f'{v:.4f}', va='center', fontsize=8)\n",
    "fig.tight_layout()\n",
    "save(fig, 'backer_regression.png', 'Backer Count Regression')\n",
    "\n",
    "all_results['backer_regression'] = backer_results\n",
    "print(f'\\nBest: {max(backer_results, key=lambda n: backer_results[n][\"RÂ²\"])}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Â· Task 4 â€” Project Category Clustering\n",
    "Cluster project categories by their success rate, average goal, average pledged, average backers, and campaign duration."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# â”€â”€ Category-level profiles â”€â”€\n",
    "cat_profiles = df_binary.groupby('Category').agg(\n",
    "    success_rate=('success', 'mean'),\n",
    "    avg_goal=('log_goal', 'mean'),\n",
    "    avg_pledged=('log_pledged', 'mean'),\n",
    "    avg_backers=('log_backers', 'mean'),\n",
    "    avg_duration=('campaign_days', 'mean'),\n",
    "    n_projects=('success', 'count'),\n",
    ").sort_values('n_projects', ascending=False)\n",
    "\n",
    "print(f'Category profiles: {len(cat_profiles)} categories')\n",
    "print(cat_profiles.round(3).to_string())\n",
    "\n",
    "cluster_feats = ['success_rate', 'avg_goal', 'avg_pledged', 'avg_backers', 'avg_duration']\n",
    "X_clust = cat_profiles[cluster_feats].values\n",
    "scaler_cl = StandardScaler()\n",
    "X_clust_s = scaler_cl.fit_transform(X_clust)\n",
    "\n",
    "# â”€â”€ Elbow + Silhouette â”€â”€\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "K_range = range(2, min(11, len(cat_profiles)))\n",
    "inertias, sils = [], []\n",
    "for k in K_range:\n",
    "    km = KMeans(n_clusters=k, random_state=SEED, n_init=10)\n",
    "    km.fit(X_clust_s)\n",
    "    inertias.append(km.inertia_)\n",
    "    sils.append(silhouette_score(X_clust_s, km.labels_))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "axes[0].plot(list(K_range), inertias, 'o-', color='#2E86AB', linewidth=2)\n",
    "axes[0].set_xlabel('k'); axes[0].set_ylabel('Inertia'); axes[0].set_title('Elbow Method')\n",
    "axes[1].plot(list(K_range), sils, 'o-', color='#F18F01', linewidth=2)\n",
    "axes[1].set_xlabel('k'); axes[1].set_ylabel('Silhouette Score'); axes[1].set_title('Silhouette Scores')\n",
    "fig.tight_layout()\n",
    "save(fig, 'elbow_silhouette.png', 'Elbow & Silhouette Analysis')\n",
    "\n",
    "best_k = list(K_range)[np.argmax(sils)]\n",
    "print(f'Best k = {best_k} (silhouette = {max(sils):.4f})')\n",
    "\n",
    "# â”€â”€ Final clustering â”€â”€\n",
    "km_final = KMeans(n_clusters=best_k, random_state=SEED, n_init=10)\n",
    "cat_profiles['cluster'] = km_final.fit_predict(X_clust_s)\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=2, random_state=SEED)\n",
    "X_pca = pca.fit_transform(X_clust_s)\n",
    "\n",
    "# DBSCAN\n",
    "db = DBSCAN(eps=1.5, min_samples=2)\n",
    "cat_profiles['dbscan_cluster'] = db.fit_predict(X_clust_s)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "for cl in range(best_k):\n",
    "    mask = cat_profiles['cluster'] == cl\n",
    "    axes[0].scatter(X_pca[mask, 0], X_pca[mask, 1], label=f'Cluster {cl}',\n",
    "                     alpha=0.8, s=100, edgecolors='white', linewidths=0.5)\n",
    "for i, cat in enumerate(cat_profiles.index):\n",
    "    axes[0].annotate(cat, (X_pca[i, 0], X_pca[i, 1]), fontsize=7, alpha=0.8,\n",
    "                      xytext=(5, 5), textcoords='offset points')\n",
    "axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n",
    "axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n",
    "axes[0].set_title(f'K-Means Clustering (k={best_k})')\n",
    "axes[0].legend(fontsize=8)\n",
    "\n",
    "unique_db = sorted(cat_profiles['dbscan_cluster'].unique())\n",
    "for cl in unique_db:\n",
    "    mask = cat_profiles['dbscan_cluster'] == cl\n",
    "    label = f'Cluster {cl}' if cl >= 0 else 'Noise'\n",
    "    axes[1].scatter(X_pca[mask, 0], X_pca[mask, 1], label=label, alpha=0.8, s=100,\n",
    "                     edgecolors='white', linewidths=0.5)\n",
    "for i, cat in enumerate(cat_profiles.index):\n",
    "    axes[1].annotate(cat, (X_pca[i, 0], X_pca[i, 1]), fontsize=7, alpha=0.8,\n",
    "                      xytext=(5, 5), textcoords='offset points')\n",
    "axes[1].set_xlabel('PC1'); axes[1].set_ylabel('PC2')\n",
    "axes[1].set_title('DBSCAN Clustering')\n",
    "axes[1].legend(fontsize=8)\n",
    "fig.tight_layout()\n",
    "save(fig, 'clustering_results.png', 'Category Clustering Results')\n",
    "\n",
    "print(f'\\nCluster Profiles:')\n",
    "print(cat_profiles.groupby('cluster')[cluster_feats].mean().round(3).to_string())\n",
    "\n",
    "all_results['clustering'] = {\n",
    "    'k': best_k, 'silhouette': max(sils),\n",
    "    'profiles': cat_profiles.groupby('cluster')[cluster_feats].mean().round(3).to_dict()\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 Â· Hyperparameter Tuning\n",
    "GridSearchCV and RandomizedSearchCV on the best classifiers for project success prediction."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# â”€â”€ Use a subsample for tuning (full dataset is 300K+ rows) â”€â”€\n",
    "TUNE_SIZE = 80000\n",
    "idx_tune = np.random.choice(len(X_train_c), TUNE_SIZE, replace=False)\n",
    "X_tune = X_train_c.iloc[idx_tune]\n",
    "y_tune = y_train_c.iloc[idx_tune]\n",
    "\n",
    "# â”€â”€ GridSearchCV â€” Random Forest â”€â”€\n",
    "print('GridSearchCV: Random Forest ...')\n",
    "rf_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 15, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "}\n",
    "gs_rf = GridSearchCV(RandomForestClassifier(random_state=SEED, n_jobs=-1),\n",
    "                      rf_grid, cv=3, scoring='f1_weighted', n_jobs=-1, verbose=0)\n",
    "gs_rf.fit(X_tune, y_tune)\n",
    "pred_gs = gs_rf.predict(X_test_c)\n",
    "acc_gs = accuracy_score(y_test_c, pred_gs)\n",
    "f1_gs = f1_score(y_test_c, pred_gs, average='weighted')\n",
    "print(f'  Best params: {gs_rf.best_params_}')\n",
    "print(f'  Best CV F1:  {gs_rf.best_score_:.4f}')\n",
    "print(f'  Test Acc:    {acc_gs:.4f}  F1: {f1_gs:.4f}')\n",
    "\n",
    "clf_results['RF (Tuned)'] = {\n",
    "    'Accuracy': acc_gs, 'F1 Score': f1_gs,\n",
    "    'Precision': precision_score(y_test_c, pred_gs, average='weighted'),\n",
    "    'Recall': recall_score(y_test_c, pred_gs, average='weighted'),\n",
    "    'model': gs_rf.best_estimator_, 'predictions': pred_gs}\n",
    "\n",
    "# â”€â”€ RandomizedSearchCV â€” Gradient Boosting â”€â”€\n",
    "print(f'\\nRandomizedSearchCV: Gradient Boosting ...')\n",
    "gb_dist = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "}\n",
    "rs_gb = RandomizedSearchCV(GradientBoostingClassifier(random_state=SEED),\n",
    "                            gb_dist, n_iter=15, cv=3, scoring='f1_weighted',\n",
    "                            random_state=SEED, n_jobs=-1, verbose=0)\n",
    "rs_gb.fit(X_tune, y_tune)\n",
    "pred_rs = rs_gb.predict(X_test_c)\n",
    "acc_rs = accuracy_score(y_test_c, pred_rs)\n",
    "f1_rs = f1_score(y_test_c, pred_rs, average='weighted')\n",
    "print(f'  Best params: {rs_gb.best_params_}')\n",
    "print(f'  Test Acc:    {acc_rs:.4f}  F1: {f1_rs:.4f}')\n",
    "\n",
    "clf_results['GB (Tuned)'] = {\n",
    "    'Accuracy': acc_rs, 'F1 Score': f1_rs,\n",
    "    'Precision': precision_score(y_test_c, pred_rs, average='weighted'),\n",
    "    'Recall': recall_score(y_test_c, pred_rs, average='weighted'),\n",
    "    'model': rs_gb.best_estimator_, 'predictions': pred_rs}\n",
    "\n",
    "print(f'\\nHyperparameter tuning complete!')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 Â· Cross-Validation, Confusion Matrices & Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# â”€â”€ 5-Fold Cross-Validation (on subsample for speed) â”€â”€\n",
    "CV_SIZE = 60000\n",
    "idx_cv = np.random.choice(len(X_train_c), CV_SIZE, replace=False)\n",
    "X_cv_data = X_train_c.iloc[idx_cv]\n",
    "y_cv_data = y_train_c.iloc[idx_cv]\n",
    "X_cv_data_s = scaler_c.transform(X_cv_data)\n",
    "\n",
    "cv_models = {\n",
    "    'Logistic Regression': (LogisticRegression(max_iter=1000, random_state=SEED), True),\n",
    "    'Random Forest': (RandomForestClassifier(n_estimators=200, max_depth=15, random_state=SEED, n_jobs=-1), False),\n",
    "    'Gradient Boosting': (GradientBoostingClassifier(n_estimators=200, max_depth=5, random_state=SEED), False),\n",
    "}\n",
    "\n",
    "cv_scores = {}\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "for name, (model, needs_scale) in cv_models.items():\n",
    "    X_cv = X_cv_data_s if needs_scale else X_cv_data\n",
    "    scores = cross_val_score(model, X_cv, y_cv_data, cv=skf, scoring='f1_weighted', n_jobs=-1)\n",
    "    cv_scores[name] = scores\n",
    "    print(f'{name:30s} CV F1: {scores.mean():.4f} Â± {scores.std():.4f}')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.boxplot(cv_scores.values(), labels=cv_scores.keys(), patch_artist=True,\n",
    "           boxprops=dict(facecolor='#2E86AB', alpha=0.7))\n",
    "ax.set_ylabel('F1 Score'); ax.set_title('5-Fold Cross-Validation â€” F1 Scores')\n",
    "ax.tick_params(axis='x', rotation=15)\n",
    "fig.tight_layout()\n",
    "save(fig, 'cv_comparison.png', 'Cross-Validation Comparison')\n",
    "\n",
    "# â”€â”€ Feature Importance â€” RF â”€â”€\n",
    "rf_model = clf_results.get('RF (Tuned)', clf_results.get('Random Forest'))['model']\n",
    "if hasattr(rf_model, 'feature_importances_'):\n",
    "    imp_clf = pd.Series(rf_model.feature_importances_, index=clf_features).sort_values()\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    imp_clf.plot.barh(ax=ax, color='#2E86AB', edgecolor='white')\n",
    "    ax.set_xlabel('Feature Importance')\n",
    "    ax.set_title('Success Prediction â€” Feature Importance (Random Forest)')\n",
    "    fig.tight_layout()\n",
    "    save(fig, 'feature_importance.png', 'Success Prediction Feature Importance')\n",
    "\n",
    "# â”€â”€ Confusion Matrices (top 4) â”€â”€\n",
    "top4 = sorted(clf_results, key=lambda n: clf_results[n]['F1 Score'], reverse=True)[:4]\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "class_names = ['Failed', 'Successful']\n",
    "for ax, name in zip(axes.ravel(), top4):\n",
    "    cm = confusion_matrix(y_test_c, clf_results[name]['predictions'])\n",
    "    sns.heatmap(cm, annot=True, fmt=',d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names, ax=ax)\n",
    "    ax.set_title(f'{name}\\n(F1={clf_results[name][\"F1 Score\"]:.4f})')\n",
    "    ax.set_xlabel('Predicted'); ax.set_ylabel('Actual')\n",
    "fig.suptitle('Confusion Matrices â€” Top 4 Classifiers', fontsize=14, y=1.02)\n",
    "fig.tight_layout()\n",
    "save(fig, 'confusion_matrices.png', 'Confusion Matrices â€” Top 4')\n",
    "\n",
    "# â”€â”€ Learning Curves â”€â”€\n",
    "best_clf_name = top4[0]\n",
    "# Use GB or RF for learning curve\n",
    "if 'Random Forest' in best_clf_name or 'RF' in best_clf_name:\n",
    "    lc_model = RandomForestClassifier(n_estimators=100, max_depth=15, random_state=SEED, n_jobs=-1)\n",
    "elif 'Gradient Boosting' in best_clf_name or 'GB' in best_clf_name:\n",
    "    lc_model = GradientBoostingClassifier(n_estimators=100, max_depth=5, random_state=SEED)\n",
    "else:\n",
    "    lc_model = RandomForestClassifier(n_estimators=100, max_depth=15, random_state=SEED, n_jobs=-1)\n",
    "\n",
    "# Subsample for learning curve speed\n",
    "LC_SIZE = 40000\n",
    "idx_lc = np.random.choice(len(X_cv_data), LC_SIZE, replace=False)\n",
    "X_lc = X_cv_data.iloc[idx_lc]\n",
    "y_lc = y_cv_data.iloc[idx_lc]\n",
    "\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    lc_model, X_lc, y_lc, cv=5, scoring='f1_weighted',\n",
    "    train_sizes=np.linspace(0.1, 1.0, 8), n_jobs=-1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.plot(train_sizes, train_scores.mean(axis=1), 'o-', color='#2E86AB', label='Train')\n",
    "ax.fill_between(train_sizes, train_scores.mean(axis=1) - train_scores.std(axis=1),\n",
    "                train_scores.mean(axis=1) + train_scores.std(axis=1), alpha=0.1, color='#2E86AB')\n",
    "ax.plot(train_sizes, val_scores.mean(axis=1), 'o-', color='#F18F01', label='Validation')\n",
    "ax.fill_between(train_sizes, val_scores.mean(axis=1) - val_scores.std(axis=1),\n",
    "                val_scores.mean(axis=1) + val_scores.std(axis=1), alpha=0.1, color='#F18F01')\n",
    "ax.set_xlabel('Training Size'); ax.set_ylabel('F1 Score')\n",
    "ax.set_title(f'Learning Curve â€” {best_clf_name}')\n",
    "ax.legend(); ax.grid(True, alpha=0.3)\n",
    "fig.tight_layout()\n",
    "save(fig, 'learning_curves.png', f'Learning Curve â€” {best_clf_name}')\n",
    "\n",
    "print('CV, confusion matrices, and learning curves complete!')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 Â· Ensemble Methods & Final Comparison"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# â”€â”€ Voting Classifier â”€â”€\n",
    "print('Training Voting Classifier ...')\n",
    "voting = VotingClassifier(estimators=[\n",
    "    ('rf', RandomForestClassifier(n_estimators=200, max_depth=15, random_state=SEED, n_jobs=-1)),\n",
    "    ('gb', GradientBoostingClassifier(n_estimators=200, max_depth=5, random_state=SEED)),\n",
    "    ('lr', LogisticRegression(max_iter=1000, random_state=SEED)),\n",
    "], voting='hard', n_jobs=-1)\n",
    "voting.fit(X_train_c, y_train_c)\n",
    "pred_v = voting.predict(X_test_c)\n",
    "clf_results['Voting Ensemble'] = {\n",
    "    'Accuracy': accuracy_score(y_test_c, pred_v),\n",
    "    'F1 Score': f1_score(y_test_c, pred_v, average='weighted'),\n",
    "    'Precision': precision_score(y_test_c, pred_v, average='weighted'),\n",
    "    'Recall': recall_score(y_test_c, pred_v, average='weighted'),\n",
    "    'model': voting, 'predictions': pred_v}\n",
    "print(f'  Voting Ensemble               Acc={clf_results[\"Voting Ensemble\"][\"Accuracy\"]:.4f}  '\n",
    "      f'F1={clf_results[\"Voting Ensemble\"][\"F1 Score\"]:.4f}')\n",
    "\n",
    "# â”€â”€ Stacking Classifier â”€â”€\n",
    "print('Training Stacking Classifier ...')\n",
    "stacking = StackingClassifier(estimators=[\n",
    "    ('rf', RandomForestClassifier(n_estimators=200, max_depth=15, random_state=SEED, n_jobs=-1)),\n",
    "    ('gb', GradientBoostingClassifier(n_estimators=200, max_depth=5, random_state=SEED)),\n",
    "    ('dt', DecisionTreeClassifier(max_depth=12, random_state=SEED)),\n",
    "], final_estimator=LogisticRegression(max_iter=1000, random_state=SEED),\n",
    "    cv=3, n_jobs=-1)\n",
    "stacking.fit(X_train_c, y_train_c)\n",
    "pred_s = stacking.predict(X_test_c)\n",
    "clf_results['Stacking Ensemble'] = {\n",
    "    'Accuracy': accuracy_score(y_test_c, pred_s),\n",
    "    'F1 Score': f1_score(y_test_c, pred_s, average='weighted'),\n",
    "    'Precision': precision_score(y_test_c, pred_s, average='weighted'),\n",
    "    'Recall': recall_score(y_test_c, pred_s, average='weighted'),\n",
    "    'model': stacking, 'predictions': pred_s}\n",
    "print(f'  Stacking Ensemble             Acc={clf_results[\"Stacking Ensemble\"][\"Accuracy\"]:.4f}  '\n",
    "      f'F1={clf_results[\"Stacking Ensemble\"][\"F1 Score\"]:.4f}')\n",
    "\n",
    "# â”€â”€ Final Comparison â”€â”€\n",
    "comp = pd.DataFrame({\n",
    "    name: {k: v for k, v in metrics.items() if k not in ('model', 'predictions')}\n",
    "    for name, metrics in clf_results.items()\n",
    "}).T.sort_values('F1 Score', ascending=False)\n",
    "comp.index.name = 'Model'\n",
    "\n",
    "print(f'\\nFinal Model Comparison:')\n",
    "print(comp.to_string(float_format=lambda x: f'{x:.6f}'))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "colors_bar = plt.cm.viridis(np.linspace(0.2, 0.9, len(comp)))\n",
    "bars = ax.barh(range(len(comp)), comp['F1 Score'], color=colors_bar, edgecolor='white')\n",
    "ax.set_yticks(range(len(comp)))\n",
    "ax.set_yticklabels(comp.index, fontsize=9)\n",
    "ax.set_xlabel('F1 Score (weighted)')\n",
    "ax.set_title('Project Success Classification â€” All Models')\n",
    "for i, v in enumerate(comp['F1 Score']):\n",
    "    ax.text(v + 0.001, i, f'{v:.4f}', va='center', fontsize=8)\n",
    "ax.invert_yaxis()\n",
    "fig.tight_layout()\n",
    "save(fig, 'model_comparison.png', 'Model Comparison â€” Success Prediction')\n",
    "\n",
    "all_results['classification'] = comp.to_dict('index')\n",
    "best_overall = comp.index[0]\n",
    "print(f'\\nBest model: {best_overall} (F1 = {comp.loc[best_overall, \"F1 Score\"]:.4f})')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11 Â· HTML Report Generation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def img_to_b64(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return base64.b64encode(f.read()).decode()\n",
    "\n",
    "HTML_TEMPLATE = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "<meta charset=\"UTF-8\"><meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "<title>Kickstarter ML Analysis Report</title>\n",
    "<style>\n",
    "  :root { --primary: #0B3D91; --accent: #1E88E5; --light: #E3F2FD; --bg: #F5F7FA; --success: #43A047; }\n",
    "  * { margin: 0; padding: 0; box-sizing: border-box; }\n",
    "  body { font-family: 'Segoe UI', system-ui, sans-serif; background: var(--bg); color: #333; line-height: 1.6; }\n",
    "  .header { background: linear-gradient(135deg, var(--primary), var(--accent)); color: white;\n",
    "             padding: 3rem 2rem; text-align: center; }\n",
    "  .header h1 { font-size: 2.2rem; margin-bottom: 0.5rem; }\n",
    "  .header p { opacity: 0.9; font-size: 1.1rem; }\n",
    "  .container { max-width: 1200px; margin: 0 auto; padding: 2rem; }\n",
    "  .section { background: white; border-radius: 12px; padding: 2rem; margin-bottom: 2rem;\n",
    "             box-shadow: 0 2px 8px rgba(0,0,0,0.08); }\n",
    "  .section h2 { color: var(--primary); border-bottom: 3px solid var(--accent);\n",
    "                 padding-bottom: 0.5rem; margin-bottom: 1.5rem; font-size: 1.5rem; }\n",
    "  .section h3 { color: var(--accent); margin: 1rem 0 0.5rem; }\n",
    "  table { width: 100%%; border-collapse: collapse; margin: 1rem 0; font-size: 0.9rem; }\n",
    "  th { background: var(--accent); color: white; padding: 10px 12px; text-align: left; }\n",
    "  td { padding: 8px 12px; border-bottom: 1px solid #e0e0e0; }\n",
    "  tr:nth-child(even) { background: var(--light); }\n",
    "  tr:hover { background: #BBDEFB; }\n",
    "  .best { background: #C8E6C9 !important; font-weight: bold; }\n",
    "  img { max-width: 100%%; border-radius: 8px; margin: 1rem 0; }\n",
    "  .grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(400px, 1fr)); gap: 1.5rem; }\n",
    "  .metric-card { background: var(--light); border-radius: 8px; padding: 1.5rem; text-align: center; }\n",
    "  .metric-card .value { font-size: 2rem; font-weight: bold; color: var(--primary); }\n",
    "  .metric-card .label { color: #666; font-size: 0.9rem; }\n",
    "  .footer { text-align: center; padding: 2rem; color: #888; font-size: 0.85rem; }\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "<div class=\"header\">\n",
    "  <h1>ðŸš€ Kickstarter Projects â€” ML Report</h1>\n",
    "  <p>Comprehensive Machine Learning Analysis â€¢ {{ n_projects }} projects â€¢ {{ n_categories }} categories â€¢ {{ year_range }}</p>\n",
    "</div>\n",
    "<div class=\"container\">\n",
    "\n",
    "  <div class=\"grid\" style=\"margin-bottom:2rem;\">\n",
    "    <div class=\"metric-card\"><div class=\"value\">{{ n_projects }}</div><div class=\"label\">Total Projects</div></div>\n",
    "    <div class=\"metric-card\"><div class=\"value\">{{ success_rate }}</div><div class=\"label\">Overall Success Rate</div></div>\n",
    "    <div class=\"metric-card\"><div class=\"value\">{{ n_categories }}</div><div class=\"label\">Categories</div></div>\n",
    "    <div class=\"metric-card\"><div class=\"value\">{{ n_countries }}</div><div class=\"label\">Countries</div></div>\n",
    "  </div>\n",
    "\n",
    "  <div class=\"section\">\n",
    "    <h2>1 Â· Exploratory Data Analysis</h2>\n",
    "    <div class=\"grid\">\n",
    "    {% for title, b64 in eda_plots %}\n",
    "      <div><h3>{{ title }}</h3><img src=\"data:image/png;base64,{{ b64 }}\" alt=\"{{ title }}\"></div>\n",
    "    {% endfor %}\n",
    "    </div>\n",
    "  </div>\n",
    "\n",
    "  <div class=\"section\">\n",
    "    <h2>2 Â· Project Success Classification</h2>\n",
    "    <p>Binary classification: predict Successful vs Failed using only pre-launch features.</p>\n",
    "    <table>\n",
    "      <tr><th>Model</th><th>Accuracy</th><th>Precision</th><th>Recall</th><th>F1 Score</th></tr>\n",
    "      {% for name, m in clf_table.items() %}\n",
    "      <tr{% if loop.index == 1 %} class=\"best\"{% endif %}>\n",
    "        <td>{{ name }}</td><td>{{ \"%.4f\"|format(m['Accuracy']) }}</td>\n",
    "        <td>{{ \"%.4f\"|format(m['Precision']) }}</td><td>{{ \"%.4f\"|format(m['Recall']) }}</td>\n",
    "        <td>{{ \"%.4f\"|format(m['F1 Score']) }}</td></tr>\n",
    "      {% endfor %}\n",
    "    </table>\n",
    "    <div class=\"grid\">\n",
    "    {% for title, b64 in clf_plots %}\n",
    "      <div><h3>{{ title }}</h3><img src=\"data:image/png;base64,{{ b64 }}\" alt=\"{{ title }}\"></div>\n",
    "    {% endfor %}\n",
    "    </div>\n",
    "  </div>\n",
    "\n",
    "  <div class=\"section\">\n",
    "    <h2>3 Â· Pledged Amount Regression</h2>\n",
    "    <p>Predict log(pledged+1) for successful projects.</p>\n",
    "    <table>\n",
    "      <tr><th>Model</th><th>RÂ²</th><th>MAE</th><th>RMSE</th></tr>\n",
    "      {% for name, m in reg_results.items() %}\n",
    "      <tr{% if loop.index == 1 %} class=\"best\"{% endif %}>\n",
    "        <td>{{ name }}</td><td>{{ \"%.4f\"|format(m['RÂ²']) }}</td>\n",
    "        <td>{{ \"%.2f\"|format(m['MAE']) }}</td><td>{{ \"%.2f\"|format(m['RMSE']) }}</td></tr>\n",
    "      {% endfor %}\n",
    "    </table>\n",
    "    <div class=\"grid\">\n",
    "    {% for title, b64 in reg_plots %}\n",
    "      <div><h3>{{ title }}</h3><img src=\"data:image/png;base64,{{ b64 }}\" alt=\"{{ title }}\"></div>\n",
    "    {% endfor %}\n",
    "    </div>\n",
    "  </div>\n",
    "\n",
    "  <div class=\"section\">\n",
    "    <h2>4 Â· Backer Count Regression</h2>\n",
    "    <table>\n",
    "      <tr><th>Model</th><th>RÂ²</th><th>MAE</th><th>RMSE</th></tr>\n",
    "      {% for name, m in backer_results.items() %}\n",
    "      <tr{% if loop.index == 1 %} class=\"best\"{% endif %}>\n",
    "        <td>{{ name }}</td><td>{{ \"%.4f\"|format(m['RÂ²']) }}</td>\n",
    "        <td>{{ \"%.2f\"|format(m['MAE']) }}</td><td>{{ \"%.2f\"|format(m['RMSE']) }}</td></tr>\n",
    "      {% endfor %}\n",
    "    </table>\n",
    "    <div class=\"grid\">\n",
    "    {% for title, b64 in backer_plots %}\n",
    "      <div><h3>{{ title }}</h3><img src=\"data:image/png;base64,{{ b64 }}\" alt=\"{{ title }}\"></div>\n",
    "    {% endfor %}\n",
    "    </div>\n",
    "  </div>\n",
    "\n",
    "  <div class=\"section\">\n",
    "    <h2>5 Â· Category Clustering</h2>\n",
    "    <div class=\"grid\" style=\"margin-bottom:1rem;\">\n",
    "      <div class=\"metric-card\"><div class=\"value\">{{ cluster_k }}</div><div class=\"label\">Optimal Clusters</div></div>\n",
    "      <div class=\"metric-card\"><div class=\"value\">{{ \"%.4f\"|format(cluster_sil) }}</div><div class=\"label\">Silhouette Score</div></div>\n",
    "    </div>\n",
    "    <div class=\"grid\">\n",
    "    {% for title, b64 in cluster_plots %}\n",
    "      <div><h3>{{ title }}</h3><img src=\"data:image/png;base64,{{ b64 }}\" alt=\"{{ title }}\"></div>\n",
    "    {% endfor %}\n",
    "    </div>\n",
    "  </div>\n",
    "\n",
    "  <div class=\"section\">\n",
    "    <h2>6 Â· Cross-Validation & Analysis</h2>\n",
    "    <div class=\"grid\">\n",
    "    {% for title, b64 in analysis_plots %}\n",
    "      <div><h3>{{ title }}</h3><img src=\"data:image/png;base64,{{ b64 }}\" alt=\"{{ title }}\"></div>\n",
    "    {% endfor %}\n",
    "    </div>\n",
    "  </div>\n",
    "\n",
    "  <div class=\"section\">\n",
    "    <h2>7 Â· Final Model Comparison</h2>\n",
    "    {% for title, b64 in final_plots %}\n",
    "      <h3>{{ title }}</h3><img src=\"data:image/png;base64,{{ b64 }}\" alt=\"{{ title }}\">\n",
    "    {% endfor %}\n",
    "  </div>\n",
    "\n",
    "</div>\n",
    "<div class=\"footer\">\n",
    "  Generated automatically Â· Kickstarter ML Analysis Â· scikit-learn {{ sklearn_ver }}\n",
    "</div>\n",
    "</body></html>\n",
    "\"\"\"\n",
    "\n",
    "plot_categories = {\n",
    "    'eda': ['success_by_category', 'goal_distribution', 'projects_over_time',\n",
    "            'success_by_duration', 'success_by_country', 'correlation_heatmap'],\n",
    "    'reg': ['regression_results', 'regression_feature_importance'],\n",
    "    'backer': ['backer_regression'],\n",
    "    'clf': ['confusion_matrices', 'feature_importance'],\n",
    "    'cluster': ['elbow_silhouette', 'clustering_results'],\n",
    "    'analysis': ['cv_comparison', 'learning_curves'],\n",
    "    'final': ['model_comparison'],\n",
    "}\n",
    "\n",
    "def get_plots(category):\n",
    "    keys = plot_categories[category]\n",
    "    result = []\n",
    "    for title, path in saved_plots:\n",
    "        fname = pathlib.Path(path).stem\n",
    "        if fname in keys:\n",
    "            result.append((title, img_to_b64(path)))\n",
    "    return result\n",
    "\n",
    "# Sort regression results\n",
    "reg_sorted = dict(sorted(all_results['regression'].items(),\n",
    "                          key=lambda x: x[1]['RÂ²'], reverse=True))\n",
    "backer_sorted = dict(sorted(all_results['backer_regression'].items(),\n",
    "                              key=lambda x: x[1]['RÂ²'], reverse=True))\n",
    "\n",
    "# Sort classification\n",
    "clf_sorted = dict(sorted(\n",
    "    {k: {m: v for m, v in met.items() if m not in ('model', 'predictions')}\n",
    "     for k, met in clf_results.items()}.items(),\n",
    "    key=lambda x: x[1]['F1 Score'], reverse=True))\n",
    "\n",
    "import sklearn\n",
    "tmpl = Template(HTML_TEMPLATE)\n",
    "html = tmpl.render(\n",
    "    n_projects=f'{len(df_binary):,}',\n",
    "    success_rate=f'{df_binary[\"success\"].mean():.1%}',\n",
    "    n_categories=df_binary['Category'].nunique(),\n",
    "    n_countries=df_binary['Country'].nunique(),\n",
    "    year_range=f'{df_binary[\"launch_year\"].min()}â€“{df_binary[\"launch_year\"].max()}',\n",
    "    eda_plots=get_plots('eda'),\n",
    "    clf_table=clf_sorted,\n",
    "    clf_plots=get_plots('clf'),\n",
    "    reg_results=reg_sorted,\n",
    "    reg_plots=get_plots('reg'),\n",
    "    backer_results=backer_sorted,\n",
    "    backer_plots=get_plots('backer'),\n",
    "    cluster_k=all_results['clustering']['k'],\n",
    "    cluster_sil=all_results['clustering']['silhouette'],\n",
    "    cluster_plots=get_plots('cluster'),\n",
    "    analysis_plots=get_plots('analysis'),\n",
    "    final_plots=get_plots('final'),\n",
    "    sklearn_ver=sklearn.__version__,\n",
    ")\n",
    "\n",
    "report_path = pathlib.Path('outputs/kickstarter_ml_report.html')\n",
    "report_path.write_text(html)\n",
    "print(f'âœ… HTML Report generated: {report_path}')\n",
    "print(f'   File size: {report_path.stat().st_size / 1024:.1f} KB')\n",
    "print(f'   Embedded images: {html.count(\"data:image/png;base64,\")}')\n",
    "print(f'\\nðŸŽ‰ Analysis complete! Open the HTML file to view the full report.')"
   ],
   "outputs": [],
   "execution_count": null
  }
 ]
}