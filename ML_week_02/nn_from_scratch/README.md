# Neural Network From Scratch ðŸ§ 

> **Complete neural network framework built with NumPy only** â€” no PyTorch, TensorFlow, or JAX.  
> Educational mastery of backpropagation, gradient descent, and network architecture at the matrix operation level.

## Highlights

- **Pure NumPy** â€” every forward/backward pass is explicit matrix math
- **Mathematical rigour** â€” every function includes equations, shape annotations, chain-rule derivations
- **Gradient checking** â€” numerical verification of all backpropagation gradients
- **Multiple examples** â€” XOR, spiral, sine regression, MNIST digit classification
- **Production-quality code** â€” type hints, docstrings, tests, CI-ready

## Project Structure

```
nn_from_scratch/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ activations.py      # ReLU, LeakyReLU, Sigmoid, Tanh, Softmax
â”‚   â”‚   â”œâ”€â”€ initializers.py     # He, Xavier, LeCun weight init
â”‚   â”‚   â”œâ”€â”€ layer.py            # Layer base class + DenseLayer
â”‚   â”‚   â”œâ”€â”€ losses.py           # CrossEntropy, MSE, BinaryCrossEntropy
â”‚   â”‚   â””â”€â”€ optimizers.py       # SGD, Momentum, Adam
â”‚   â”œâ”€â”€ network/
â”‚   â”‚   â”œâ”€â”€ sequential.py       # Sequential container
â”‚   â”‚   â””â”€â”€ model.py            # High-level Model (fit/evaluate/predict)
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ data_utils.py       # Batching, shuffling, splitting, one-hot
â”‚   â”‚   â”œâ”€â”€ metrics.py          # Accuracy, Precision, Recall, F1, CM
â”‚   â”‚   â””â”€â”€ visualization.py    # Training curves, decision boundaries
â”‚   â””â”€â”€ validation/
â”‚       â””â”€â”€ gradient_check.py   # Numerical gradient verification
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ xor_example.py          # XOR (classic non-linear test)
â”‚   â”œâ”€â”€ spiral_example.py       # 2D spiral classification
â”‚   â”œâ”€â”€ regression_example.py   # sin(x) function approximation
â”‚   â””â”€â”€ mnist_example.py        # MNIST digit classification
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_backprop_walkthrough.ipynb
â”‚   â”œâ”€â”€ 02_activation_exploration.ipynb
â”‚   â””â”€â”€ 03_mnist_full_pipeline.ipynb
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_activations.py
â”‚   â”œâ”€â”€ test_layers.py
â”‚   â”œâ”€â”€ test_losses.py
â”‚   â”œâ”€â”€ test_network.py
â”‚   â””â”€â”€ test_gradient_check.py
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture.md
â”‚   â”œâ”€â”€ backpropagation.md
â”‚   â””â”€â”€ api_reference.md
â”œâ”€â”€ train_mnist.py              # CLI entry point
â”œâ”€â”€ Makefile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â””â”€â”€ README.md
```

## Quick Start

```bash
# Install dependencies
make install

# Run tests
make test

# Run examples
make xor          # XOR problem
make spiral       # Spiral classification
make regression   # sin(x) regression
make mnist        # MNIST digit classification (downloads data)

# Or use the CLI directly
python train_mnist.py --epochs 20 --lr 0.001 --batch-size 128
```

## Architecture

```python
from src.core import DenseLayer, ReLU, Softmax, CrossEntropyLoss, Adam
from src.network import Sequential, Model

network = Sequential(
    DenseLayer(784, 256, activation=ReLU()),
    DenseLayer(256, 128, activation=ReLU()),
    DenseLayer(128, 10,  activation=Softmax()),
)

model = Model(network, CrossEntropyLoss(), Adam(lr=0.001))
history = model.fit(X_train, Y_train, epochs=20, batch_size=128)
```

## Key Concepts Demonstrated

### Backpropagation
Every layer computes gradients step by step:
```
dZ = activation.backward(dA)      # through activation
dW = (1/m) Â· Xáµ€ @ dZ             # weight gradient
db = (1/m) Â· sum(dZ, axis=0)     # bias gradient
dX = dZ @ Wáµ€                     # pass to previous layer
```

### Gradient Checking
Verify correctness with numerical differentiation:
```python
from src.validation import gradient_check_layer
errors = gradient_check_layer(dense_layer, X, dY, verbose=True)
#   W  rel_error = 1.23e-08  âœ…
#   b  rel_error = 4.56e-09  âœ…
```

### Weight Initialization
Choose initialization based on activation:
- **He** (ReLU): `W ~ N(0, âˆš(2/n_in))`
- **Xavier** (Sigmoid/Tanh): `W ~ U[-âˆš(6/(n_in+n_out)), âˆš(6/(n_in+n_out))]`
- **LeCun** (SELU): `W ~ N(0, âˆš(1/n_in))`

## Tests

```bash
$ make test
# 50+ tests covering activations, layers, losses, network, gradient checking
```

## Requirements

- Python â‰¥ 3.10
- NumPy â‰¥ 1.24
- Matplotlib â‰¥ 3.8 (visualization only)
- Scikit-learn â‰¥ 1.3 (validation/comparison only â€” NOT used in NN)

## Documentation

- [Architecture Overview](docs/architecture.md)
- [Backpropagation Math](docs/backpropagation.md)
- [API Reference](docs/api_reference.md)

## License

Educational project â€” MIT License.
